{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 필요한 모듈 설치 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.autograd as autograd\n",
    "import numpy as np\n",
    "\n",
    "import random\n",
    "import os\n",
    "import pickle\n",
    "import time\n",
    "from collections import deque\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "sns.set()\n",
    "\n",
    "from wrappers import wrap, wrap_cover, SubprocVecEnv\n",
    "from runner import Runner"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 하이퍼 파라미터 정의하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sungyubkim/anaconda3/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.uint8'>. Please provide explicit dtype.\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n",
      "/home/sungyubkim/anaconda3/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n",
      "/home/sungyubkim/anaconda3/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.uint8'>. Please provide explicit dtype.\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n",
      "/home/sungyubkim/anaconda3/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.uint8'>. Please provide explicit dtype.\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N_ACTIONS :  4\n",
      "N_STATES :  (4, 84, 84)\n",
      "USE GPU: True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sungyubkim/anaconda3/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n",
      "/home/sungyubkim/anaconda3/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.uint8'>. Please provide explicit dtype.\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n",
      "/home/sungyubkim/anaconda3/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.uint8'>. Please provide explicit dtype.\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n",
      "/home/sungyubkim/anaconda3/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n",
      "/home/sungyubkim/anaconda3/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.uint8'>. Please provide explicit dtype.\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n",
      "/home/sungyubkim/anaconda3/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n",
      "/home/sungyubkim/anaconda3/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.uint8'>. Please provide explicit dtype.\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n",
      "/home/sungyubkim/anaconda3/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.uint8'>. Please provide explicit dtype.\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n",
      "/home/sungyubkim/anaconda3/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: <class 'wrappers.FrameStack'> doesn't implement 'reset' method, but it implements deprecated '_reset' method.\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n",
      "/home/sungyubkim/anaconda3/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: <class 'wrappers.FrameStack'> doesn't implement 'reset' method, but it implements deprecated '_reset' method.\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n",
      "/home/sungyubkim/anaconda3/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: <class 'wrappers.FrameStack'> doesn't implement 'reset' method, but it implements deprecated '_reset' method.\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n",
      "/home/sungyubkim/anaconda3/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: <class 'wrappers.FrameStack'> doesn't implement 'reset' method, but it implements deprecated '_reset' method.\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n",
      "Process Process-2:\n",
      "Process Process-3:\n",
      "Process Process-4:\n",
      "Process Process-1:\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/sungyubkim/anaconda3/lib/python3.7/multiprocessing/process.py\", line 297, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/sungyubkim/anaconda3/lib/python3.7/multiprocessing/process.py\", line 297, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/sungyubkim/anaconda3/lib/python3.7/multiprocessing/process.py\", line 297, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/sungyubkim/anaconda3/lib/python3.7/multiprocessing/process.py\", line 99, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/sungyubkim/anaconda3/lib/python3.7/multiprocessing/process.py\", line 297, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/sungyubkim/anaconda3/lib/python3.7/multiprocessing/process.py\", line 99, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/sungyubkim/Dropbox/Deep_RL_with_pytorch/7_Imitation_Learning/wrappers.py\", line 270, in worker\n",
      "    cmd, data = remote.recv()\n",
      "  File \"/home/sungyubkim/anaconda3/lib/python3.7/multiprocessing/process.py\", line 99, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/sungyubkim/anaconda3/lib/python3.7/multiprocessing/process.py\", line 99, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/sungyubkim/Dropbox/Deep_RL_with_pytorch/7_Imitation_Learning/wrappers.py\", line 270, in worker\n",
      "    cmd, data = remote.recv()\n",
      "  File \"/home/sungyubkim/anaconda3/lib/python3.7/multiprocessing/connection.py\", line 250, in recv\n",
      "    buf = self._recv_bytes()\n",
      "  File \"/home/sungyubkim/Dropbox/Deep_RL_with_pytorch/7_Imitation_Learning/wrappers.py\", line 270, in worker\n",
      "    cmd, data = remote.recv()\n",
      "  File \"/home/sungyubkim/Dropbox/Deep_RL_with_pytorch/7_Imitation_Learning/wrappers.py\", line 270, in worker\n",
      "    cmd, data = remote.recv()\n",
      "  File \"/home/sungyubkim/anaconda3/lib/python3.7/multiprocessing/connection.py\", line 250, in recv\n",
      "    buf = self._recv_bytes()\n",
      "  File \"/home/sungyubkim/anaconda3/lib/python3.7/multiprocessing/connection.py\", line 407, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/home/sungyubkim/anaconda3/lib/python3.7/multiprocessing/connection.py\", line 250, in recv\n",
      "    buf = self._recv_bytes()\n",
      "  File \"/home/sungyubkim/anaconda3/lib/python3.7/multiprocessing/connection.py\", line 250, in recv\n",
      "    buf = self._recv_bytes()\n",
      "  File \"/home/sungyubkim/anaconda3/lib/python3.7/multiprocessing/connection.py\", line 407, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/home/sungyubkim/anaconda3/lib/python3.7/multiprocessing/connection.py\", line 379, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "  File \"/home/sungyubkim/anaconda3/lib/python3.7/multiprocessing/connection.py\", line 407, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/home/sungyubkim/anaconda3/lib/python3.7/multiprocessing/connection.py\", line 407, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/home/sungyubkim/anaconda3/lib/python3.7/multiprocessing/connection.py\", line 379, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "  File \"/home/sungyubkim/anaconda3/lib/python3.7/multiprocessing/connection.py\", line 379, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "KeyboardInterrupt\n",
      "  File \"/home/sungyubkim/anaconda3/lib/python3.7/multiprocessing/connection.py\", line 379, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "'''PPO Settings'''\n",
    "TRAJ_LEN = 1000\n",
    "N_OPT_EPOCHS = 4\n",
    "ENT_COEF = 1e-2\n",
    "CLIP_RANGE = 0.1\n",
    "LAMBDA = 0.95\n",
    "\n",
    "'''Environment Settings'''\n",
    "# sequential images to define state\n",
    "STATE_LEN = 4\n",
    "# openai gym env name\n",
    "ENV_NAME = 'BreakoutNoFrameskip-v4'\n",
    "# number of environments for A2C\n",
    "N_ENVS = 4\n",
    "# define gym \n",
    "env = SubprocVecEnv([wrap_cover(ENV_NAME) for i in range(N_ENVS)])\n",
    "# check gym setting\n",
    "N_ACTIONS = env.action_space.n;print('N_ACTIONS : ',N_ACTIONS) #  6\n",
    "N_STATES = env.observation_space.shape;print('N_STATES : ',N_STATES) # (4, 84, 84)\n",
    "# Total simulation step\n",
    "N_STEP = 10**7\n",
    "# gamma for MDP\n",
    "GAMMA = 0.99\n",
    "# visualize for agent playing\n",
    "RENDERING = False\n",
    "\n",
    "'''Training settings'''\n",
    "# check GPU usage\n",
    "USE_GPU = torch.cuda.is_available()\n",
    "print('USE GPU: '+str(USE_GPU))\n",
    "# mini-batch size\n",
    "BATCH_SIZE = 32\n",
    "# learning rage\n",
    "LR = 1e-4\n",
    "# clip gradient\n",
    "MAX_GRAD_NORM = 0.1\n",
    "ZERO_GP = True\n",
    "# log optimization\n",
    "LOG_OPT = False\n",
    "\n",
    "'''Save&Load Settings'''\n",
    "# log frequency\n",
    "LOG_FREQ = 1\n",
    "# check save/load\n",
    "SAVE = True\n",
    "LOAD = False\n",
    "# paths for predction net, target net, result log\n",
    "NET_PATH = './data/model/ppo_net.pkl'\n",
    "DIS_PATH = './data/model/gail_dis_net.pkl'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 네트워크 구조 정의하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ConvNet, self).__init__()\n",
    "        # architecture def\n",
    "        self.feature_extraction = nn.Sequential(\n",
    "            nn.Conv2d(STATE_LEN, 32, kernel_size=8, stride=4),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, kernel_size=4, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, stride=1),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.fc = nn.Linear(7 * 7 * 64, 256)\n",
    "        # actor\n",
    "        self.actor = nn.Linear(256, N_ACTIONS)\n",
    "        # critic\n",
    "        self.critic = nn.Linear(256, 1)\n",
    "            \n",
    "        # parameter initialization\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d) or isinstance(m, nn.Linear):\n",
    "                nn.init.orthogonal_(m.weight, gain = np.sqrt(2))\n",
    "                nn.init.constant_(m.bias, 0.0)\n",
    "            \n",
    "\n",
    "    def forward(self, x):\n",
    "        # x is a tensor of (m, 4, 84, 84)\n",
    "        x = self.feature_extraction(x / 255.0)\n",
    "        # x.size(0) : mini-batch size\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = F.relu(self.fc(x))\n",
    "        # use log_softmax for numerical stability\n",
    "        action_log_prob = F.log_softmax(self.actor(x), dim=1)\n",
    "        state_value = self.critic(x)\n",
    "\n",
    "        return action_log_prob, state_value\n",
    "\n",
    "    def save(self, PATH):\n",
    "        torch.save(self.state_dict(),PATH)\n",
    "\n",
    "    def load(self, PATH):\n",
    "        self.load_state_dict(torch.load(PATH))\n",
    "        \n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Discriminator, self).__init__()\n",
    "        # architecture def\n",
    "        self.feature_extraction = nn.Sequential(\n",
    "            nn.Conv2d(STATE_LEN, 32, kernel_size=8, stride=4),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, kernel_size=4, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, stride=1),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.fc1 = nn.Linear(7 * 7 * 64, 256)\n",
    "        self.action_feature = nn.Linear(N_ACTIONS, 256)\n",
    "        # actor\n",
    "        self.fc2 = nn.Linear(256, 1)\n",
    "        \n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.orthogonal_(m.weight, gain = np.sqrt(2))\n",
    "                nn.init.constant_(m.bias, 0.0)\n",
    "            elif isinstance(m, nn.Linear):\n",
    "                nn.init.kaiming_normal_(m.weight)\n",
    "                nn.init.constant_(m.bias, 0.0)\n",
    "            \n",
    "\n",
    "    def forward(self, x, a):\n",
    "        # x is a tensor of (m, 4, 84, 84)\n",
    "        x = self.feature_extraction(x / 255.0)\n",
    "        # x.size(0) : mini-batch size\n",
    "        x = x.view(x.size(0), -1)\n",
    "        a_onehot = torch.zeros(x.size(0), N_ACTIONS)\n",
    "        if USE_GPU:\n",
    "            a_onehot = a_onehot.cuda(device=0)\n",
    "        a_onehot.scatter_(1, a.unsqueeze(1), 1)\n",
    "        x = F.relu(self.fc1(x) * F.leaky_relu(self.action_feature(a_onehot), negative_slope=2e-1))\n",
    "        action_value = self.fc2(x)\n",
    "\n",
    "        return action_value\n",
    "\n",
    "    def save(self, PATH):\n",
    "        torch.save(self.state_dict(),PATH)\n",
    "\n",
    "    def load(self, PATH):\n",
    "        self.load_state_dict(torch.load(PATH))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PPO 정의하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PPO:\n",
    "    def __init__(self):\n",
    "        self.net = ConvNet()\n",
    "        self.dis_net = Discriminator()\n",
    "        # use gpu\n",
    "        if USE_GPU:\n",
    "            self.net = self.net.cuda()\n",
    "            self.dis_net = self.dis_net.cuda()\n",
    "            \n",
    "        # simulator step conter\n",
    "        self.memory_counter = 0\n",
    "        \n",
    "        # create the replay buffer for expert\n",
    "        with open( \"replay.pkl\", \"rb\" ) as f:\n",
    "            self.expert_replay_buffer = pickle.load(f)\n",
    "        \n",
    "        # define optimizer\n",
    "        self.optimizer = torch.optim.Adam(self.net.parameters(), lr=LR)\n",
    "        self.dis_opt = torch.optim.RMSprop(self.dis_net.parameters(), lr=LR, alpha=0.9)\n",
    "        \n",
    "        # ppo clip range\n",
    "        self.clip_range = CLIP_RANGE\n",
    "        \n",
    "    def save_model(self):\n",
    "        self.net.cpu()\n",
    "        self.dis_net.cpu()\n",
    "        self.net.save(NET_PATH)\n",
    "        self.dis_net.save(DIS_PATH)\n",
    "        if USE_GPU:\n",
    "            self.net.cuda()\n",
    "            self.dis_net.cuda()\n",
    "            \n",
    "    def load_model(self):\n",
    "        self.net.cpu()\n",
    "        self.dis_net.cpu()\n",
    "        self.net.load(NET_PATH)\n",
    "        self.dis_net.load(DIS_PATH)\n",
    "        if USE_GPU:\n",
    "            self.net.cuda()\n",
    "            self.dis_net.cuda()\n",
    "        \n",
    "    def choose_action(self, x):\n",
    "        self.memory_counter += 1\n",
    "        # Assume that x is a np.array of shape (nenvs, 4, 84, 84)\n",
    "        x = torch.FloatTensor(x)\n",
    "        if USE_GPU:\n",
    "            x = x.cuda()\n",
    "        # get action log probs and state values\n",
    "        action_log_probs, state_values = self.net(x) # (nenvs, N_ACTIONS)\n",
    "        probs = F.softmax(action_log_probs, dim=1).data.cpu().numpy()\n",
    "        probs = (probs+1e-8)/np.sum((probs+1e-8), axis=1, keepdims=True)\n",
    "        # sample actions\n",
    "        actions = np.array([np.random.choice(N_ACTIONS,p=probs[i]) for i in range(len(probs))])\n",
    "        # convert tensor to np.array\n",
    "        action_log_probs , state_values = action_log_probs.data.cpu().numpy() , state_values.squeeze(1).data.cpu().numpy()\n",
    "        # calc selected logprob\n",
    "        selected_log_probs = np.array([action_log_probs[i][actions[i]] for i in range(len(probs))])\n",
    "        return actions, state_values, selected_log_probs\n",
    "    \n",
    "    def reward_dis(self, s, a):\n",
    "        s = torch.FloatTensor(s)\n",
    "        a = torch.LongTensor(a)\n",
    "        \n",
    "        if USE_GPU:\n",
    "            s , a = s.cuda(), a.cuda()\n",
    "        \n",
    "        d_reward = -torch.log(torch.sigmoid(self.dis_net(s, a)))+torch.log(1-torch.sigmoid(self.dis_net(s, a)))\n",
    "        \n",
    "        return d_reward.squeeze(1).data.cpu().numpy()\n",
    "    \n",
    "    def learn_dis(self, obs, returns, masks, actions, values, selected_log_probs):\n",
    "        # optimize discriminator\n",
    "        b_s = torch.FloatTensor(obs)\n",
    "        b_s.requires_grad = True\n",
    "        b_a = torch.LongTensor(actions)\n",
    "        \n",
    "        # data sample from expert experience replay\n",
    "        e_state_memory, e_action_memory, e_reward_memory, \\\n",
    "        e_next_state_memory, e_done = self.expert_replay_buffer.sample(len(b_s))\n",
    "        \n",
    "        e_s = torch.FloatTensor(e_state_memory)\n",
    "        e_s.requires_grad = True\n",
    "        e_a = torch.LongTensor(e_action_memory)\n",
    "        \n",
    "        if USE_GPU:\n",
    "            b_s, b_a, e_s, e_a = b_s.cuda(device=0), b_a.cuda(device=0), e_s.cuda(device=0), e_a.cuda(device=0)\n",
    "        \n",
    "        d_policy = self.dis_net(b_s, b_a).squeeze(1) # (m)\n",
    "        d_expert = self.dis_net(e_s, e_a).squeeze(1) # (m)\n",
    "        \n",
    "        d_loss = -torch.log(torch.sigmoid(d_policy)).mean() -torch.log(1-torch.sigmoid(d_expert)).mean()\n",
    "        loss = d_loss\n",
    "        \n",
    "        # calc gradient penalty\n",
    "        if ZERO_GP:\n",
    "            b_grad = autograd.grad(d_policy, b_s, create_graph=True,\n",
    "                            grad_outputs=torch.ones_like(d_policy),\n",
    "                            retain_graph=True, only_inputs=True)[0].view(BATCH_SIZE, -1)\n",
    "            e_grad = autograd.grad(d_expert, e_s, create_graph=True,\n",
    "                            grad_outputs=torch.ones_like(d_expert),\n",
    "                            retain_graph=True, only_inputs=True)[0].view(BATCH_SIZE, -1)\n",
    "            b_grad = b_grad.norm(dim=1)\n",
    "            e_grad = e_grad.norm(dim=1)\n",
    "            gp_loss = 1e+5 * ((b_grad)**2 + (e_grad)**2).mean()\n",
    "            loss += gp_loss\n",
    "        \n",
    "        self.dis_opt.zero_grad()\n",
    "        loss.backward()\n",
    "        self.dis_opt.step()\n",
    "        \n",
    "        return round(float(d_loss.item()), 4), round(float(gp_loss.item()), 4)\n",
    "\n",
    "    def learn(self, obs, returns, masks, actions, values, selected_log_probs):\n",
    "        \n",
    "        # calculate the advantages\n",
    "        advs = returns - values\n",
    "        advs = (advs - advs.mean())/(advs.std() + 1e-8)\n",
    "        \n",
    "        # np.array -> torch.Tensor\n",
    "        obs = torch.FloatTensor(obs) # (m, 4, 84, 84)\n",
    "        returns = torch.FloatTensor(returns) # (m)\n",
    "        advs = torch.FloatTensor(advs) # (m)\n",
    "        actions = torch.LongTensor(actions) # (m)\n",
    "        selected_log_probs = torch.FloatTensor(selected_log_probs) # (m)\n",
    "        values = torch.FloatTensor(values) # (m)\n",
    "        if USE_GPU:\n",
    "            obs = obs.cuda()\n",
    "            returns = returns.cuda()\n",
    "            advs = advs.cuda()\n",
    "            actions = actions.cuda()\n",
    "            selected_log_probs = selected_log_probs.cuda()\n",
    "            values = values.cuda()\n",
    "        \n",
    "        # get action log probs and state values\n",
    "        action_log_probs, state_values = self.net(obs)\n",
    "        # (m, N_ACTIONS), (m, 1)\n",
    "        \n",
    "        # calc probs\n",
    "        probs = F.softmax(action_log_probs, dim=1)\n",
    "        # (m, N_ACTIONS)\n",
    "        \n",
    "        # calc entropy loss\n",
    "        ent_loss = ENT_COEF *((action_log_probs * probs).sum(dim=1)).mean()\n",
    "        # (1)\n",
    "        \n",
    "        # calc log probs\n",
    "        cur_log_probs = action_log_probs.gather(1,actions.unsqueeze(1))\n",
    "        # cur : (m, 1)\n",
    "        ratio = torch.exp(cur_log_probs.squeeze(1)-selected_log_probs)\n",
    "        # (m)\n",
    "        \n",
    "        # actor loss\n",
    "        surr1 = ratio * advs # (m)\n",
    "        surr2 = torch.clamp(ratio, 1.0 - self.clip_range, 1.0 + self.clip_range)*advs # (m)\n",
    "        actor_loss = -torch.min(surr1, surr2).mean() # (1)\n",
    "        # critic loss\n",
    "        critic_loss = F.smooth_l1_loss(state_values.squeeze(1), returns) # (1)\n",
    "\n",
    "        loss = actor_loss + critic_loss + ent_loss # (1)\n",
    "        \n",
    "        actor_loss, critic_loss, ent_loss, total_loss = actor_loss.data.cpu().numpy(), \\\n",
    "        critic_loss.data.cpu().numpy(), ent_loss.data.cpu().numpy(), loss.data.cpu().numpy()\n",
    "\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        nn.utils.clip_grad_norm_(self.net.parameters(), MAX_GRAD_NORM)\n",
    "        self.optimizer.step()\n",
    "        \n",
    "        return round(float(actor_loss), 4), round(float(critic_loss), 4),\\\n",
    "    round(float(ent_loss), 4), round(float(total_loss), 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialize results!\n",
      "Collecting experience...\n",
      "N update:  1 | Mean ep 100 return:  1.79 /Used Time: 9.8 /Used Step: 4004 | Dis loss :  1.1856 | GP loss :  0.3294\n",
      "N update:  2 | Mean ep 100 return:  1.82 /Used Time: 19.57 /Used Step: 8008 | Dis loss :  0.9765 | GP loss :  0.2363\n",
      "N update:  3 | Mean ep 100 return:  1.69 /Used Time: 29.41 /Used Step: 12012 | Dis loss :  0.783 | GP loss :  0.2175\n",
      "N update:  4 | Mean ep 100 return:  1.51 /Used Time: 39.19 /Used Step: 16016 | Dis loss :  0.6666 | GP loss :  0.1836\n",
      "N update:  5 | Mean ep 100 return:  1.5 /Used Time: 49.11 /Used Step: 20020 | Dis loss :  0.72 | GP loss :  0.2073\n",
      "N update:  6 | Mean ep 100 return:  1.4 /Used Time: 59.13 /Used Step: 24024 | Dis loss :  0.7069 | GP loss :  0.1763\n",
      "N update:  7 | Mean ep 100 return:  1.28 /Used Time: 69.02 /Used Step: 28028 | Dis loss :  0.6624 | GP loss :  0.1751\n",
      "N update:  8 | Mean ep 100 return:  1.21 /Used Time: 78.88 /Used Step: 32032 | Dis loss :  0.6206 | GP loss :  0.1813\n",
      "N update:  9 | Mean ep 100 return:  1.03 /Used Time: 88.7 /Used Step: 36036 | Dis loss :  0.7936 | GP loss :  0.1884\n",
      "N update:  10 | Mean ep 100 return:  0.99 /Used Time: 98.51 /Used Step: 40040 | Dis loss :  0.7376 | GP loss :  0.1826\n",
      "N update:  11 | Mean ep 100 return:  1.02 /Used Time: 108.59 /Used Step: 44044 | Dis loss :  0.6792 | GP loss :  0.1813\n",
      "N update:  12 | Mean ep 100 return:  1.07 /Used Time: 118.5 /Used Step: 48048 | Dis loss :  0.6188 | GP loss :  0.1431\n",
      "N update:  13 | Mean ep 100 return:  0.94 /Used Time: 128.7 /Used Step: 52052 | Dis loss :  0.6617 | GP loss :  0.2014\n",
      "N update:  14 | Mean ep 100 return:  0.7 /Used Time: 138.62 /Used Step: 56056 | Dis loss :  0.5751 | GP loss :  0.1609\n",
      "N update:  15 | Mean ep 100 return:  0.55 /Used Time: 148.47 /Used Step: 60060 | Dis loss :  0.6511 | GP loss :  0.1645\n",
      "N update:  16 | Mean ep 100 return:  0.54 /Used Time: 158.37 /Used Step: 64064 | Dis loss :  0.6187 | GP loss :  0.1465\n",
      "N update:  17 | Mean ep 100 return:  0.54 /Used Time: 168.74 /Used Step: 68068 | Dis loss :  0.5902 | GP loss :  0.1678\n",
      "N update:  18 | Mean ep 100 return:  0.54 /Used Time: 178.85 /Used Step: 72072 | Dis loss :  0.6338 | GP loss :  0.1843\n",
      "N update:  19 | Mean ep 100 return:  0.54 /Used Time: 188.97 /Used Step: 76076 | Dis loss :  0.6783 | GP loss :  0.1968\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-383f51feec7a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mmbinds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mend\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m             \u001b[0mslices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0marr\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmbinds\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0marr\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmasks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mneglogpacs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m             \u001b[0mactor_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcritic_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ment_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mppo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mslices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m             \u001b[0;31m# save opt log\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m             \u001b[0ma_losses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mactor_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-c964eeb36391>\u001b[0m in \u001b[0;36mlearn\u001b[0;34m(self, obs, returns, masks, actions, values, selected_log_probs)\u001b[0m\n\u001b[1;32m    163\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    164\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 165\u001b[0;31m         \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip_grad_norm_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mMAX_GRAD_NORM\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    166\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    167\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py\u001b[0m in \u001b[0;36mclip_grad_norm_\u001b[0;34m(parameters, max_norm, norm_type)\u001b[0m\n\u001b[1;32m     36\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mclip_coef\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mparameters\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m             \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclip_coef\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtotal_norm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "ppo = PPO()\n",
    "runner = Runner(env=env, model=ppo, nsteps=TRAJ_LEN, gamma=GAMMA, lam=LAMBDA)\n",
    "\n",
    "# model load with check\n",
    "if LOAD and os.path.isfile(PRED_PATH) and os.path.isfile(TARGET_PATH):\n",
    "    ppo.load_model()\n",
    "    pkl_file = open(RESULT_PATH,'rb')\n",
    "    result = pickle.load(pkl_file)\n",
    "    pkl_file.close()\n",
    "    print('Load complete!')\n",
    "else:\n",
    "    result = []\n",
    "    print('Initialize results!')\n",
    "\n",
    "print('Collecting experience...')\n",
    "\n",
    "# episode step for accumulate reward \n",
    "epinfobuf = deque(maxlen=100)\n",
    "# in PPO, we iterate over optimization step\n",
    "nbatch = N_ENVS * TRAJ_LEN\n",
    "nupdates = N_STEP// nbatch\n",
    "# check learning time\n",
    "start_time = time.time()\n",
    "\n",
    "for update in range(1, nupdates+1):\n",
    "    # get minibatch\n",
    "    obs, returns, masks, actions, values, neglogpacs, epinfos = runner.run()\n",
    "    epinfobuf.extend(epinfos)\n",
    "    \n",
    "    inds = np.arange(nbatch)\n",
    "    for start in range(0, nbatch, BATCH_SIZE):\n",
    "        end = start + BATCH_SIZE\n",
    "        mbinds = inds[start:end]\n",
    "        slices = (arr[mbinds] for arr in (obs, returns, masks, actions, values, neglogpacs))\n",
    "        d_loss, gp_loss = ppo.learn_dis(*slices)\n",
    "    \n",
    "    # calculate loss\n",
    "    inds = np.arange(nbatch)\n",
    "    for _ in range(N_OPT_EPOCHS):\n",
    "        a_losses, c_losses, e_losses, t_losses = list(), list(), list(), list()\n",
    "        # shuffle indices for i.i.d.\n",
    "        np.random.shuffle(inds)\n",
    "        # 0 to batch_size with batch_train_size step\n",
    "        for start in range(0, nbatch, BATCH_SIZE):\n",
    "            end = start + BATCH_SIZE\n",
    "            mbinds = inds[start:end]\n",
    "            slices = (arr[mbinds] for arr in (obs, returns, masks, actions, values, neglogpacs))\n",
    "            actor_loss, critic_loss, ent_loss, total_loss = ppo.learn(*slices)\n",
    "            # save opt log\n",
    "            a_losses.append(actor_loss)\n",
    "            c_losses.append(critic_loss)\n",
    "            e_losses.append(ent_loss)\n",
    "            t_losses.append(total_loss)\n",
    "        # print opt log\n",
    "        if LOG_OPT:\n",
    "            print('Iter ',_,\n",
    "                 'actor loss : ',round(float(np.mean(a_losses)), 3),\n",
    "                 'critic loss : ', round(float(np.mean(c_losses)), 3),\n",
    "                 'ent loss : ', round(float(np.mean(e_losses)), 3),\n",
    "                 'total loss : ', round(float(np.mean(t_losses)), 3))\n",
    "            \n",
    "    if update % LOG_FREQ == 0:\n",
    "        # print log and save\n",
    "        # check time interval\n",
    "        time_interval = round(time.time() - start_time, 2)\n",
    "        # calc mean return\n",
    "        mean_100_ep_return = round(np.mean([epinfo['r'] for epinfo in epinfobuf]),2)\n",
    "        result.append(mean_100_ep_return)\n",
    "        # print epi log\n",
    "        print('N update: ',update,\n",
    "              '| Mean ep 100 return: ', mean_100_ep_return,\n",
    "              '/Used Time:',time_interval,\n",
    "              '/Used Step:',ppo.memory_counter*N_ENVS,\n",
    "             '| Dis loss : ',d_loss,\n",
    "             '| GP loss : ', gp_loss)\n",
    "        # save model\n",
    "        if SAVE:\n",
    "            ppo.save_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 결과 시각화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(range(len(result)), result)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import animation\n",
    "\n",
    "def display_frames_as_gif(frames):\n",
    "    patch = plt.imshow(frames[0])\n",
    "    plt.axis('off')\n",
    "    def animate(i):\n",
    "        patch.set_data(frames[i])\n",
    "        \n",
    "    anim = animation.FuncAnimation(plt.gcf(), animate, frames = len(frames), interval=5)\n",
    "    anim.save('./gail_breakout_result.gif', writer='imagemagick', fps=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = wrap(gym.make('BreakoutNoFrameskip-v4'))\n",
    "s = np.array(env.reset())\n",
    "total_reward = 0\n",
    "frames = []\n",
    "\n",
    "for t in range(10000):\n",
    "    # Render into buffer. \n",
    "    frames.append(env.render(mode = 'rgb_array'))\n",
    "    a, v, l = ppo.choose_action(np.expand_dims(s,axis=0))\n",
    "    # take action and get next state\n",
    "    s_, r, done, info = env.step(a)\n",
    "    s_ = np.array(s_)\n",
    "    total_reward += r\n",
    "    if done:\n",
    "        break\n",
    "    s = s_\n",
    "env.close()\n",
    "print('Total Reward : %.2f'%total_reward)\n",
    "display_frames_as_gif(frames)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](./ppo_pong_result.gif \"segment\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
