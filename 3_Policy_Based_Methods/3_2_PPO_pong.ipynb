{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 필요한 모듈 설치 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "\n",
    "import random\n",
    "import os\n",
    "import pickle\n",
    "import time\n",
    "from collections import deque\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "sns.set()\n",
    "\n",
    "from wrappers import wrap_cover, SubprocVecEnv\n",
    "from runner import Runner"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 하이퍼 파라미터 정의하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sungyubkim/anaconda3/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.uint8'>. Please provide explicit dtype.\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n",
      "/home/sungyubkim/anaconda3/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.uint8'>. Please provide explicit dtype.\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n",
      "/home/sungyubkim/anaconda3/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n",
      "/home/sungyubkim/anaconda3/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n",
      "/home/sungyubkim/anaconda3/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.uint8'>. Please provide explicit dtype.\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n",
      "/home/sungyubkim/anaconda3/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.uint8'>. Please provide explicit dtype.\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n",
      "/home/sungyubkim/anaconda3/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n",
      "/home/sungyubkim/anaconda3/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.uint8'>. Please provide explicit dtype.\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N_ACTIONS :  6\n",
      "N_STATES :  (4, 84, 84)\n",
      "USE GPU: True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sungyubkim/anaconda3/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.uint8'>. Please provide explicit dtype.\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n",
      "/home/sungyubkim/anaconda3/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n",
      "/home/sungyubkim/anaconda3/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.uint8'>. Please provide explicit dtype.\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n",
      "/home/sungyubkim/anaconda3/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.uint8'>. Please provide explicit dtype.\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n",
      "/home/sungyubkim/anaconda3/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: <class 'wrappers.FrameStack'> doesn't implement 'reset' method, but it implements deprecated '_reset' method.\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n",
      "/home/sungyubkim/anaconda3/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: <class 'wrappers.FrameStack'> doesn't implement 'reset' method, but it implements deprecated '_reset' method.\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n",
      "/home/sungyubkim/anaconda3/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: <class 'wrappers.FrameStack'> doesn't implement 'reset' method, but it implements deprecated '_reset' method.\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n",
      "/home/sungyubkim/anaconda3/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: <class 'wrappers.FrameStack'> doesn't implement 'reset' method, but it implements deprecated '_reset' method.\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    }
   ],
   "source": [
    "'''PPO Settings'''\n",
    "TRAJ_LEN = 1000\n",
    "N_OPT_EPOCHS = 4\n",
    "ENT_COEF = 1e-2\n",
    "CLIP_RANGE = 0.1\n",
    "LAMBDA = 0.95\n",
    "\n",
    "'''Environment Settings'''\n",
    "# sequential images to define state\n",
    "STATE_LEN = 4\n",
    "# openai gym env name\n",
    "ENV_NAME = 'PongNoFrameskip-v4'\n",
    "# number of environments for A2C\n",
    "N_ENVS = 4\n",
    "# define gym \n",
    "env = SubprocVecEnv([wrap_cover(ENV_NAME) for i in range(N_ENVS)])\n",
    "# check gym setting\n",
    "N_ACTIONS = env.action_space.n;print('N_ACTIONS : ',N_ACTIONS) #  6\n",
    "N_STATES = env.observation_space.shape;print('N_STATES : ',N_STATES) # (4, 84, 84)\n",
    "# Total simulation step\n",
    "N_STEP = 10**7\n",
    "# gamma for MDP\n",
    "GAMMA = 0.99\n",
    "# visualize for agent playing\n",
    "RENDERING = False\n",
    "\n",
    "'''Training settings'''\n",
    "# check GPU usage\n",
    "USE_GPU = torch.cuda.is_available()\n",
    "print('USE GPU: '+str(USE_GPU))\n",
    "# mini-batch size\n",
    "BATCH_SIZE = 32\n",
    "# learning rage\n",
    "LR = 1e-4\n",
    "# clip gradient\n",
    "MAX_GRAD_NORM = 0.1\n",
    "# log optimization\n",
    "LOG_OPT = False\n",
    "\n",
    "'''Save&Load Settings'''\n",
    "# log frequency\n",
    "LOG_FREQ = 10\n",
    "# check save/load\n",
    "SAVE = True\n",
    "LOAD = False\n",
    "# paths for predction net, target net, result log\n",
    "NET_PATH = './data/model/ppo_net.pkl'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 네트워크 구조 정의하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ConvNet, self).__init__()\n",
    "        # architecture def\n",
    "        self.feature_extraction = nn.Sequential(\n",
    "            nn.Conv2d(STATE_LEN, 32, kernel_size=8, stride=4),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, kernel_size=4, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, stride=1),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.fc = nn.Linear(7 * 7 * 64, 256)\n",
    "        # actor\n",
    "        self.actor = nn.Linear(256, N_ACTIONS)\n",
    "        # critic\n",
    "        self.critic = nn.Linear(256, 1)\n",
    "            \n",
    "        # parameter initialization\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d) or isinstance(m, nn.Linear):\n",
    "                nn.init.orthogonal_(m.weight, gain = np.sqrt(2))\n",
    "                nn.init.constant_(m.bias, 0.0)\n",
    "            \n",
    "\n",
    "    def forward(self, x):\n",
    "        # x is a tensor of (m, 4, 84, 84)\n",
    "        x = self.feature_extraction(x / 255.0)\n",
    "        # x.size(0) : mini-batch size\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = F.relu(self.fc(x))\n",
    "        # use log_softmax for numerical stability\n",
    "        action_log_prob = F.log_softmax(self.actor(x), dim=1)\n",
    "        state_value = self.critic(x)\n",
    "\n",
    "        return action_log_prob, state_value\n",
    "\n",
    "    def save(self, PATH):\n",
    "        torch.save(self.state_dict(),PATH)\n",
    "\n",
    "    def load(self, PATH):\n",
    "        self.load_state_dict(torch.load(PATH))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PPO 정의하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PPO:\n",
    "    def __init__(self):\n",
    "        self.net = ConvNet()\n",
    "        # use gpu\n",
    "        if USE_GPU:\n",
    "            self.net = self.net.cuda()\n",
    "            \n",
    "        # simulator step conter\n",
    "        self.memory_counter = 0\n",
    "        \n",
    "        # define optimizer\n",
    "        self.optimizer = torch.optim.Adam(self.net.parameters(), lr=LR)\n",
    "        \n",
    "        # ppo clip range\n",
    "        self.clip_range = CLIP_RANGE\n",
    "        \n",
    "    def save_model(self):\n",
    "        self.net.cpu()\n",
    "        self.net.save(NET_PATH)\n",
    "        if USE_GPU:\n",
    "            self.net.cuda()\n",
    "            \n",
    "    def load_model(self):\n",
    "        self.net.cpu()\n",
    "        self.net.load(NET_PATH)\n",
    "        if USE_GPU:\n",
    "            self.net.cuda()\n",
    "        \n",
    "    def choose_action(self, x):\n",
    "        self.memory_counter += 1\n",
    "        # Assume that x is a np.array of shape (nenvs, 4, 84, 84)\n",
    "        x = torch.FloatTensor(x)\n",
    "        if USE_GPU:\n",
    "            x = x.cuda()\n",
    "        # get action log probs and state values\n",
    "        action_log_probs, state_values = self.net(x) # (nenvs, N_ACTIONS)\n",
    "        probs = F.softmax(action_log_probs, dim=1).data.cpu().numpy()\n",
    "        probs = (probs+1e-8)/np.sum((probs+1e-8), axis=1, keepdims=True)\n",
    "        # sample actions\n",
    "        actions = np.array([np.random.choice(N_ACTIONS,p=probs[i]) for i in range(len(probs))])\n",
    "        # convert tensor to np.array\n",
    "        action_log_probs , state_values = action_log_probs.data.cpu().numpy() , state_values.squeeze(1).data.cpu().numpy()\n",
    "        # calc selected logprob\n",
    "        selected_log_probs = np.array([action_log_probs[i][actions[i]] for i in range(len(probs))])\n",
    "        return actions, state_values, selected_log_probs\n",
    "\n",
    "    def learn(self, obs, returns, masks, actions, values, selected_log_probs):\n",
    "        \n",
    "        # calculate the advantages\n",
    "        advs = returns - values\n",
    "        # Normalize the advantages for numerical stability\n",
    "        advs = (advs - advs.mean()) / (advs.std() + 1e-8)\n",
    "        \n",
    "        # np.array -> torch.Tensor\n",
    "        obs = torch.FloatTensor(obs) # (m, 4, 84, 84)\n",
    "        advs = torch.FloatTensor(advs) # (m)\n",
    "        actions = torch.LongTensor(actions) # (m)\n",
    "        selected_log_probs = torch.FloatTensor(selected_log_probs) # (m)\n",
    "        values = torch.FloatTensor(values) # (m)\n",
    "        if USE_GPU:\n",
    "            obs = obs.cuda()\n",
    "            advs = advs.cuda()\n",
    "            actions = actions.cuda()\n",
    "            selected_log_probs = selected_log_probs.cuda()\n",
    "            values = values.cuda()\n",
    "        \n",
    "        # get action log probs and state values\n",
    "        action_log_probs, state_values = self.net(obs)\n",
    "        # (m, N_ACTIONS), (m, 1)\n",
    "        \n",
    "        # calc probs\n",
    "        probs = F.softmax(action_log_probs, dim=1)\n",
    "        # (m, N_ACTIONS)\n",
    "        \n",
    "        # calc entropy loss\n",
    "        ent_loss = ENT_COEF *((action_log_probs * probs).sum(dim=1)).mean()\n",
    "        # (1)\n",
    "        \n",
    "        # calc log probs\n",
    "        cur_log_probs = action_log_probs.gather(1,actions.unsqueeze(1))\n",
    "        # cur : (m, 1)\n",
    "        ratio = torch.exp(cur_log_probs.squeeze(1)-selected_log_probs)\n",
    "        # (m)\n",
    "        \n",
    "        # actor loss\n",
    "        surr1 = ratio * advs # (m)\n",
    "        surr2 = torch.clamp(ratio, 1.0 - self.clip_range, 1.0 + self.clip_range)*advs # (m)\n",
    "        actor_loss = -torch.min(surr1, surr2).mean() # (1)\n",
    "        # critic loss\n",
    "        critic_loss = 0.5 * torch.mean( (state_values.squeeze(1) - values)**2 ) # (1)\n",
    "\n",
    "        loss = actor_loss + critic_loss + ent_loss # (1)\n",
    "        \n",
    "        actor_loss, critic_loss, ent_loss, total_loss = actor_loss.data.cpu().numpy(), \\\n",
    "        critic_loss.data.cpu().numpy(), ent_loss.data.cpu().numpy(), loss.data.cpu().numpy()\n",
    "\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        nn.utils.clip_grad_norm_(self.net.parameters(), MAX_GRAD_NORM)\n",
    "        self.optimizer.step()\n",
    "        \n",
    "        return round(float(actor_loss), 4), round(float(critic_loss), 4),\\\n",
    "    round(float(ent_loss), 4), round(float(total_loss), 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialize results!\n",
      "Collecting experience...\n",
      "N update:  10 | Mean ep 100 return:  -20.23 /Used Time: 66.1 /Used Step: 40040\n",
      "N update:  20 | Mean ep 100 return:  -20.26 /Used Time: 133.16 /Used Step: 80080\n",
      "N update:  30 | Mean ep 100 return:  -20.03 /Used Time: 199.95 /Used Step: 120120\n",
      "N update:  40 | Mean ep 100 return:  -19.86 /Used Time: 266.74 /Used Step: 160160\n",
      "N update:  50 | Mean ep 100 return:  -19.47 /Used Time: 332.46 /Used Step: 200200\n",
      "N update:  60 | Mean ep 100 return:  -19.27 /Used Time: 398.73 /Used Step: 240240\n",
      "N update:  70 | Mean ep 100 return:  -18.66 /Used Time: 464.97 /Used Step: 280280\n",
      "N update:  80 | Mean ep 100 return:  -18.12 /Used Time: 531.95 /Used Step: 320320\n",
      "N update:  90 | Mean ep 100 return:  -17.32 /Used Time: 598.32 /Used Step: 360360\n",
      "N update:  100 | Mean ep 100 return:  -16.7 /Used Time: 665.58 /Used Step: 400400\n",
      "N update:  110 | Mean ep 100 return:  -15.97 /Used Time: 732.04 /Used Step: 440440\n",
      "N update:  120 | Mean ep 100 return:  -14.85 /Used Time: 798.18 /Used Step: 480480\n",
      "N update:  130 | Mean ep 100 return:  -13.25 /Used Time: 863.15 /Used Step: 520520\n",
      "N update:  140 | Mean ep 100 return:  -11.4 /Used Time: 928.94 /Used Step: 560560\n",
      "N update:  150 | Mean ep 100 return:  -9.62 /Used Time: 994.71 /Used Step: 600600\n",
      "N update:  160 | Mean ep 100 return:  -7.68 /Used Time: 1059.92 /Used Step: 640640\n",
      "N update:  170 | Mean ep 100 return:  -6.06 /Used Time: 1125.82 /Used Step: 680680\n",
      "N update:  180 | Mean ep 100 return:  -4.83 /Used Time: 1191.27 /Used Step: 720720\n",
      "N update:  190 | Mean ep 100 return:  -4.49 /Used Time: 1256.45 /Used Step: 760760\n",
      "N update:  200 | Mean ep 100 return:  -4.5 /Used Time: 1321.89 /Used Step: 800800\n",
      "N update:  210 | Mean ep 100 return:  -4.51 /Used Time: 1387.26 /Used Step: 840840\n",
      "N update:  220 | Mean ep 100 return:  -4.2 /Used Time: 1452.45 /Used Step: 880880\n",
      "N update:  230 | Mean ep 100 return:  -3.57 /Used Time: 1517.7 /Used Step: 920920\n",
      "N update:  240 | Mean ep 100 return:  -3.54 /Used Time: 1582.9 /Used Step: 960960\n",
      "N update:  250 | Mean ep 100 return:  -3.24 /Used Time: 1648.08 /Used Step: 1001000\n",
      "N update:  260 | Mean ep 100 return:  -3.62 /Used Time: 1713.8 /Used Step: 1041040\n",
      "N update:  270 | Mean ep 100 return:  -3.66 /Used Time: 1778.74 /Used Step: 1081080\n",
      "N update:  280 | Mean ep 100 return:  -3.8 /Used Time: 1844.39 /Used Step: 1121120\n",
      "N update:  290 | Mean ep 100 return:  -3.97 /Used Time: 1909.98 /Used Step: 1161160\n",
      "N update:  300 | Mean ep 100 return:  -4.18 /Used Time: 1975.73 /Used Step: 1201200\n",
      "N update:  310 | Mean ep 100 return:  -4.49 /Used Time: 2040.85 /Used Step: 1241240\n",
      "N update:  320 | Mean ep 100 return:  -4.37 /Used Time: 2106.17 /Used Step: 1281280\n",
      "N update:  330 | Mean ep 100 return:  -4.27 /Used Time: 2171.71 /Used Step: 1321320\n",
      "N update:  340 | Mean ep 100 return:  -4.46 /Used Time: 2237.27 /Used Step: 1361360\n",
      "N update:  350 | Mean ep 100 return:  -4.71 /Used Time: 2302.61 /Used Step: 1401400\n",
      "N update:  360 | Mean ep 100 return:  -4.53 /Used Time: 2368.25 /Used Step: 1441440\n",
      "N update:  370 | Mean ep 100 return:  -4.69 /Used Time: 2433.87 /Used Step: 1481480\n",
      "N update:  380 | Mean ep 100 return:  -4.61 /Used Time: 2499.28 /Used Step: 1521520\n",
      "N update:  390 | Mean ep 100 return:  -4.56 /Used Time: 2564.61 /Used Step: 1561560\n",
      "N update:  400 | Mean ep 100 return:  -4.4 /Used Time: 2630.51 /Used Step: 1601600\n",
      "N update:  410 | Mean ep 100 return:  -4.35 /Used Time: 2696.04 /Used Step: 1641640\n",
      "N update:  420 | Mean ep 100 return:  -4.32 /Used Time: 2761.56 /Used Step: 1681680\n",
      "N update:  430 | Mean ep 100 return:  -4.45 /Used Time: 2827.17 /Used Step: 1721720\n",
      "N update:  440 | Mean ep 100 return:  -4.19 /Used Time: 2892.74 /Used Step: 1761760\n",
      "N update:  450 | Mean ep 100 return:  -4.41 /Used Time: 2957.56 /Used Step: 1801800\n",
      "N update:  460 | Mean ep 100 return:  -4.43 /Used Time: 3022.42 /Used Step: 1841840\n",
      "N update:  470 | Mean ep 100 return:  -4.35 /Used Time: 3088.02 /Used Step: 1881880\n",
      "N update:  480 | Mean ep 100 return:  -4.21 /Used Time: 3153.59 /Used Step: 1921920\n",
      "N update:  490 | Mean ep 100 return:  -4.13 /Used Time: 3218.97 /Used Step: 1961960\n",
      "N update:  500 | Mean ep 100 return:  -3.98 /Used Time: 3283.81 /Used Step: 2002000\n",
      "N update:  510 | Mean ep 100 return:  -3.81 /Used Time: 3348.99 /Used Step: 2042040\n",
      "N update:  520 | Mean ep 100 return:  -3.65 /Used Time: 3414.51 /Used Step: 2082080\n",
      "N update:  530 | Mean ep 100 return:  -3.45 /Used Time: 3480.2 /Used Step: 2122120\n",
      "N update:  540 | Mean ep 100 return:  -3.08 /Used Time: 3545.27 /Used Step: 2162160\n",
      "N update:  550 | Mean ep 100 return:  -2.86 /Used Time: 3610.52 /Used Step: 2202200\n",
      "N update:  560 | Mean ep 100 return:  -2.66 /Used Time: 3676.08 /Used Step: 2242240\n",
      "N update:  570 | Mean ep 100 return:  -2.17 /Used Time: 3741.27 /Used Step: 2282280\n",
      "N update:  580 | Mean ep 100 return:  -1.81 /Used Time: 3806.4 /Used Step: 2322320\n",
      "N update:  590 | Mean ep 100 return:  -1.32 /Used Time: 3871.41 /Used Step: 2362360\n",
      "N update:  600 | Mean ep 100 return:  -0.26 /Used Time: 3936.56 /Used Step: 2402400\n",
      "N update:  610 | Mean ep 100 return:  2.37 /Used Time: 4001.96 /Used Step: 2442440\n",
      "N update:  620 | Mean ep 100 return:  4.59 /Used Time: 4067.52 /Used Step: 2482480\n",
      "N update:  630 | Mean ep 100 return:  6.96 /Used Time: 4132.91 /Used Step: 2522520\n",
      "N update:  640 | Mean ep 100 return:  9.4 /Used Time: 4198.85 /Used Step: 2562560\n",
      "N update:  650 | Mean ep 100 return:  11.21 /Used Time: 4263.67 /Used Step: 2602600\n",
      "N update:  660 | Mean ep 100 return:  10.88 /Used Time: 4328.87 /Used Step: 2642640\n",
      "N update:  670 | Mean ep 100 return:  12.03 /Used Time: 4394.61 /Used Step: 2682680\n",
      "N update:  680 | Mean ep 100 return:  12.93 /Used Time: 4460.13 /Used Step: 2722720\n",
      "N update:  690 | Mean ep 100 return:  12.52 /Used Time: 4525.01 /Used Step: 2762760\n",
      "N update:  700 | Mean ep 100 return:  12.89 /Used Time: 4590.18 /Used Step: 2802800\n",
      "N update:  710 | Mean ep 100 return:  12.52 /Used Time: 4656.03 /Used Step: 2842840\n",
      "N update:  720 | Mean ep 100 return:  12.34 /Used Time: 4721.09 /Used Step: 2882880\n",
      "N update:  730 | Mean ep 100 return:  12.72 /Used Time: 4786.49 /Used Step: 2922920\n",
      "N update:  740 | Mean ep 100 return:  12.24 /Used Time: 4852.05 /Used Step: 2962960\n",
      "N update:  750 | Mean ep 100 return:  13.52 /Used Time: 4917.73 /Used Step: 3003000\n",
      "N update:  760 | Mean ep 100 return:  13.73 /Used Time: 4983.41 /Used Step: 3043040\n",
      "N update:  770 | Mean ep 100 return:  14.56 /Used Time: 5048.88 /Used Step: 3083080\n",
      "N update:  780 | Mean ep 100 return:  15.24 /Used Time: 5114.18 /Used Step: 3123120\n"
     ]
    }
   ],
   "source": [
    "ppo = PPO()\n",
    "runner = Runner(env=env, model=ppo, nsteps=TRAJ_LEN, gamma=GAMMA, lam=LAMBDA)\n",
    "\n",
    "# model load with check\n",
    "if LOAD and os.path.isfile(PRED_PATH) and os.path.isfile(TARGET_PATH):\n",
    "    ppo.load_model()\n",
    "    pkl_file = open(RESULT_PATH,'rb')\n",
    "    result = pickle.load(pkl_file)\n",
    "    pkl_file.close()\n",
    "    print('Load complete!')\n",
    "else:\n",
    "    result = []\n",
    "    print('Initialize results!')\n",
    "\n",
    "print('Collecting experience...')\n",
    "\n",
    "# episode step for accumulate reward \n",
    "epinfobuf = deque(maxlen=100)\n",
    "# in PPO, we iterate over optimization step\n",
    "nbatch = N_ENVS * TRAJ_LEN\n",
    "nupdates = N_STEP// nbatch\n",
    "# check learning time\n",
    "start_time = time.time()\n",
    "\n",
    "for update in range(1, nupdates+1):\n",
    "    # get minibatch\n",
    "    obs, returns, masks, actions, values, neglogpacs, epinfos = runner.run()\n",
    "    epinfobuf.extend(epinfos)\n",
    "    \n",
    "    # calculate loss\n",
    "    inds = np.arange(nbatch)\n",
    "    for _ in range(N_OPT_EPOCHS):\n",
    "        a_losses, c_losses, e_losses, t_losses = list(), list(), list(), list()\n",
    "        # shuffle indices for i.i.d.\n",
    "        np.random.shuffle(inds)\n",
    "        # 0 to batch_size with batch_train_size step\n",
    "        for start in range(0, nbatch, BATCH_SIZE):\n",
    "            end = start + BATCH_SIZE\n",
    "            mbinds = inds[start:end]\n",
    "            slices = (arr[mbinds] for arr in (obs, returns, masks, actions, values, neglogpacs))\n",
    "            actor_loss, critic_loss, ent_loss, total_loss = ppo.learn(*slices)\n",
    "            # save opt log\n",
    "            a_losses.append(actor_loss)\n",
    "            c_losses.append(critic_loss)\n",
    "            e_losses.append(ent_loss)\n",
    "            t_losses.append(total_loss)\n",
    "        # print opt log\n",
    "        if LOG_OPT:\n",
    "            print('Iter ',_,\n",
    "                 'actor loss : ',round(float(np.mean(a_losses)), 3),\n",
    "                 'critic loss : ', round(float(np.mean(c_losses)), 3),\n",
    "                 'ent loss : ', round(float(np.mean(e_losses)), 3),\n",
    "                 'total loss : ', round(float(np.mean(t_losses)), 3))\n",
    "            \n",
    "    if update % LOG_FREQ == 0:\n",
    "        # print log and save\n",
    "        # check time interval\n",
    "        time_interval = round(time.time() - start_time, 2)\n",
    "        # calc mean return\n",
    "        mean_100_ep_return = round(np.mean([epinfo['r'] for epinfo in epinfobuf]),2)\n",
    "        result.append(mean_100_ep_return)\n",
    "        # print epi log\n",
    "        print('N update: ',update,\n",
    "              '| Mean ep 100 return: ', mean_100_ep_return,\n",
    "              '/Used Time:',time_interval,\n",
    "              '/Used Step:',ppo.memory_counter*N_ENVS)\n",
    "        # save model\n",
    "        if SAVE:\n",
    "            ppo.save_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 결과 시각화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(range(len(result)), result)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import animation\n",
    "\n",
    "def display_frames_as_gif(frames):\n",
    "    patch = plt.imshow(frames[0])\n",
    "    plt.axis('off')\n",
    "    def animate(i):\n",
    "        patch.set_data(frames[i])\n",
    "        \n",
    "    anim = animation.FuncAnimation(plt.gcf(), animate, frames = len(frames), interval=5)\n",
    "    anim.save('./ppo_pong_result.gif', writer='imagemagick', fps=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = wrap(gym.make('PongNoFrameskip-v4'))\n",
    "s = np.array(env.reset())\n",
    "total_reward = 0\n",
    "frames = []\n",
    "\n",
    "dqn = DQN()\n",
    "dqn.load_model()\n",
    "\n",
    "for t in range(10000):\n",
    "    # Render into buffer. \n",
    "    frames.append(env.render(mode = 'rgb_array'))\n",
    "    a = dqn.choose_action(s, 1.0)\n",
    "    # take action and get next state\n",
    "    s_, r, done, info = env.step(a)\n",
    "    s_ = np.array(s_)\n",
    "    total_reward += r\n",
    "    if done:\n",
    "        break\n",
    "    s = s_\n",
    "env.close()\n",
    "print('Total Reward : %.2f'%total_reward)\n",
    "display_frames_as_gif(frames)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](./ppo_pong_result.gif \"segment\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
