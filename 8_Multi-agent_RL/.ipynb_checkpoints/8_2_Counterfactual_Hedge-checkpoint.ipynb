{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 필요한 모듈 설치 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "\n",
    "import random\n",
    "import os\n",
    "import pickle\n",
    "import time\n",
    "from collections import deque\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "sns.set()\n",
    "\n",
    "from wrappers import wrap, wrap_cover, SubprocVecEnv\n",
    "from runner import Runner"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 하이퍼 파라미터 정의하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sungyubkim/anaconda3/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.uint8'>. Please provide explicit dtype.\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n",
      "/home/sungyubkim/anaconda3/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.uint8'>. Please provide explicit dtype.\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n",
      "/home/sungyubkim/anaconda3/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n",
      "/home/sungyubkim/anaconda3/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.uint8'>. Please provide explicit dtype.\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n",
      "/home/sungyubkim/anaconda3/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n",
      "/home/sungyubkim/anaconda3/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.uint8'>. Please provide explicit dtype.\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n",
      "/home/sungyubkim/anaconda3/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.uint8'>. Please provide explicit dtype.\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N_ACTIONS :  4\n",
      "N_STATES :  (4, 84, 84)\n",
      "USE GPU: True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sungyubkim/anaconda3/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n",
      "/home/sungyubkim/anaconda3/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.uint8'>. Please provide explicit dtype.\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n",
      "/home/sungyubkim/anaconda3/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.uint8'>. Please provide explicit dtype.\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n",
      "/home/sungyubkim/anaconda3/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n",
      "/home/sungyubkim/anaconda3/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.uint8'>. Please provide explicit dtype.\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n",
      "/home/sungyubkim/anaconda3/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: <class 'wrappers.FrameStack'> doesn't implement 'reset' method, but it implements deprecated '_reset' method.\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n",
      "/home/sungyubkim/anaconda3/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: <class 'wrappers.FrameStack'> doesn't implement 'reset' method, but it implements deprecated '_reset' method.\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n",
      "/home/sungyubkim/anaconda3/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: <class 'wrappers.FrameStack'> doesn't implement 'reset' method, but it implements deprecated '_reset' method.\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n",
      "/home/sungyubkim/anaconda3/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: <class 'wrappers.FrameStack'> doesn't implement 'reset' method, but it implements deprecated '_reset' method.\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    }
   ],
   "source": [
    "'''PPO Settings'''\n",
    "TRAJ_LEN = 1000\n",
    "N_OPT_EPOCHS = 4\n",
    "ENT_COEF = 1e-2\n",
    "LAMBDA = 0.95\n",
    "\n",
    "'''Environment Settings'''\n",
    "# sequential images to define state\n",
    "STATE_LEN = 4\n",
    "# openai gym env name\n",
    "ENV_NAME = 'BreakoutNoFrameskip-v4'\n",
    "# number of environments for A2C\n",
    "N_ENVS = 4\n",
    "# define gym \n",
    "env = SubprocVecEnv([wrap_cover(ENV_NAME) for i in range(N_ENVS)])\n",
    "# check gym setting\n",
    "N_ACTIONS = env.action_space.n;print('N_ACTIONS : ',N_ACTIONS) #  6\n",
    "N_STATES = env.observation_space.shape;print('N_STATES : ',N_STATES) # (4, 84, 84)\n",
    "# Total simulation step\n",
    "N_STEP = 10**7\n",
    "# gamma for MDP\n",
    "GAMMA = 0.99\n",
    "# visualize for agent playing\n",
    "RENDERING = False\n",
    "\n",
    "'''Training settings'''\n",
    "# check GPU usage\n",
    "USE_GPU = torch.cuda.is_available()\n",
    "print('USE GPU: '+str(USE_GPU))\n",
    "# mini-batch size\n",
    "BATCH_SIZE = 32\n",
    "# learning rage\n",
    "LR = 1e-4\n",
    "# clip gradient\n",
    "MAX_GRAD_NORM = 0.1\n",
    "# log optimization\n",
    "LOG_OPT = False\n",
    "\n",
    "'''Save&Load Settings'''\n",
    "# log frequency\n",
    "LOG_FREQ = 1\n",
    "# check save/load\n",
    "SAVE = True\n",
    "LOAD = False\n",
    "# paths for predction net, target net, result log\n",
    "NET_PATH = './data/model/ppo_net.pkl'\n",
    "REGRET_PATH = './data/model/regret_net.pkl'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 네트워크 구조 정의하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ConvNet, self).__init__()\n",
    "        # architecture def\n",
    "        self.feature_extraction = nn.Sequential(\n",
    "            nn.Conv2d(STATE_LEN, 32, kernel_size=8, stride=4),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, kernel_size=4, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, stride=1),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.fc = nn.Linear(7 * 7 * 64, 256)\n",
    "        # actor\n",
    "        self.actor = nn.Linear(256, N_ACTIONS)\n",
    "        # critic\n",
    "        self.critic = nn.Linear(256, 1)\n",
    "            \n",
    "        # parameter initialization\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d) or isinstance(m, nn.Linear):\n",
    "                nn.init.orthogonal_(m.weight, gain = np.sqrt(2))\n",
    "                nn.init.constant_(m.bias, 0.0)\n",
    "            \n",
    "\n",
    "    def forward(self, x):\n",
    "        # x is a tensor of (m, 4, 84, 84)\n",
    "        x = self.feature_extraction(x / 255.0)\n",
    "        # x.size(0) : mini-batch size\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = F.relu(self.fc(x))\n",
    "        # use log_softmax for numerical stability\n",
    "        action_log_prob = F.log_softmax(self.actor(x), dim=1)\n",
    "        state_value = self.critic(x)\n",
    "\n",
    "        return action_log_prob, state_value\n",
    "\n",
    "    def save(self, PATH):\n",
    "        torch.save(self.state_dict(),PATH)\n",
    "\n",
    "    def load(self, PATH):\n",
    "        self.load_state_dict(torch.load(PATH))\n",
    "        \n",
    "class RegretNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(RegretNet, self).__init__()\n",
    "        # architecture def\n",
    "        self.feature_extraction = nn.Sequential(\n",
    "            nn.Conv2d(STATE_LEN, 32, kernel_size=8, stride=4),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, kernel_size=4, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, stride=1),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.fc = nn.Linear(7 * 7 * 64, 256)\n",
    "        # actor\n",
    "        self.action_critic = nn.Linear(256, N_ACTIONS)\n",
    "        \n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.orthogonal_(m.weight, gain = np.sqrt(2))\n",
    "                nn.init.constant_(m.bias, 0.0)\n",
    "            elif isinstance(m, nn.Linear):\n",
    "                nn.init.kaiming_normal_(m.weight)\n",
    "                nn.init.constant_(m.bias, 0.0)\n",
    "            \n",
    "\n",
    "    def forward(self, x):\n",
    "        # x is a tensor of (m, 4, 84, 84)\n",
    "        x = self.feature_extraction(x / 255.0)\n",
    "        # x.size(0) : mini-batch size\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = F.relu(self.fc(x))\n",
    "        action_value = self.action_critic(x)\n",
    "\n",
    "        return action_value\n",
    "\n",
    "    def save(self, PATH):\n",
    "        torch.save(self.state_dict(),REGRET_PATH)\n",
    "\n",
    "    def load(self, PATH):\n",
    "        self.load_state_dict(torch.load(REGRET_PATH))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PPO 정의하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PPO:\n",
    "    def __init__(self):\n",
    "        self.net = ConvNet()\n",
    "        self.regret_net = RegretNet()\n",
    "        self.regret_net_old = RegretNet()\n",
    "        # use gpu\n",
    "        if USE_GPU:\n",
    "            self.net = self.net.cuda()\n",
    "            self.regret_net = self.regret_net.cuda()\n",
    "            self.regret_net_old = self.regret_net_old.cuda()\n",
    "            \n",
    "        # simulator step conter\n",
    "        self.memory_counter = 0\n",
    "        \n",
    "        # define optimizer\n",
    "        self.optimizer = torch.optim.Adam(self.net.parameters(), lr=LR)\n",
    "        self.regret_opt = torch.optim.Adam(self.regret_net.parameters(), lr=LR)\n",
    "        \n",
    "    def save_model(self):\n",
    "        self.net.cpu()\n",
    "        self.net.save(NET_PATH)\n",
    "        if USE_GPU:\n",
    "            self.net.cuda()\n",
    "            \n",
    "    def load_model(self):\n",
    "        self.net.cpu()\n",
    "        self.net.load(NET_PATH)\n",
    "        if USE_GPU:\n",
    "            self.net.cuda()\n",
    "            \n",
    "    def update_target(self, target, pred, update_rate):\n",
    "        # update target network parameters using predcition network\n",
    "        for target_param, pred_param in zip(target.parameters(), pred.parameters()):\n",
    "            target_param.data.copy_((1.0 - update_rate) \\\n",
    "                                    * target_param.data + update_rate*pred_param.data)\n",
    "        \n",
    "    def choose_action(self, x):\n",
    "        self.memory_counter += 1\n",
    "        # Assume that x is a np.array of shape (nenvs, 4, 84, 84)\n",
    "        x = torch.FloatTensor(x)\n",
    "        if USE_GPU:\n",
    "            x = x.cuda()\n",
    "        # get action log probs and state values\n",
    "        action_log_probs, state_values = self.net(x) # (nenvs, N_ACTIONS)\n",
    "        probs = F.softmax(action_log_probs, dim=1).data.cpu().numpy()\n",
    "        probs = (probs+1e-8)/np.sum((probs+1e-8), axis=1, keepdims=True)\n",
    "        # sample actions\n",
    "        actions = np.array([np.random.choice(N_ACTIONS,p=probs[i]) for i in range(len(probs))])\n",
    "        # convert tensor to np.array\n",
    "        action_log_probs , state_values = action_log_probs.data.cpu().numpy() , state_values.squeeze(1).data.cpu().numpy()\n",
    "        # calc selected logprob\n",
    "        selected_log_probs = np.array([action_log_probs[i][actions[i]] for i in range(len(probs))])\n",
    "        return actions, state_values, selected_log_probs\n",
    "\n",
    "    def learn_value(self, obs, returns, masks, actions, values, selected_log_probs):\n",
    "        # np.array -> torch.Tensor\n",
    "        obs = torch.FloatTensor(obs) # (m, 4, 84, 84)\n",
    "        returns = torch.FloatTensor(returns) # (m)\n",
    "        actions = torch.LongTensor(actions) # (m)\n",
    "        if USE_GPU:\n",
    "            obs = obs.cuda()\n",
    "            returns = returns.cuda()\n",
    "            actions = actions.cuda()\n",
    "            \n",
    "        # get action log probs and state values\n",
    "        action_log_probs, state_values = self.net(obs)\n",
    "        # critic loss\n",
    "        critic_loss = F.smooth_l1_loss(state_values.squeeze(1), returns) # (1)\n",
    "        \n",
    "        self.optimizer.zero_grad()\n",
    "        critic_loss.backward()\n",
    "        self.optimizer.step()\n",
    "        \n",
    "    def learn_regret(self, obs, returns, masks, actions, values, selected_log_probs):\n",
    "        \n",
    "        # calculate the advantages\n",
    "        advs = returns - values\n",
    "        \n",
    "        # np.array -> torch.Tensor\n",
    "        obs = torch.FloatTensor(obs) # (m, 4, 84, 84)\n",
    "        returns = torch.FloatTensor(returns) # (m)\n",
    "        advs = torch.FloatTensor(advs) # (m)\n",
    "        actions = torch.LongTensor(actions) # (m)\n",
    "        if USE_GPU:\n",
    "            obs = obs.cuda()\n",
    "            returns = returns.cuda()\n",
    "            advs = advs.cuda()\n",
    "            actions = actions.cuda()\n",
    "        \n",
    "        # get action log probs and state values\n",
    "        action_log_probs, state_values = self.net(obs)\n",
    "        # (m, N_ACTIONS), (m, 1)\n",
    "        regret = self.regret_net(obs).gather(1,actions.unsqueeze(1)).squeeze(1)\n",
    "        regret_target = self.regret_net_old(obs).gather(1,actions.unsqueeze(1)).squeeze(1).detach() \\\n",
    "        + advs\n",
    "        # (m)\n",
    "        \n",
    "        # regret loss\n",
    "        regret_loss = F.smooth_l1_loss(regret, regret_target)\n",
    "        \n",
    "        self.regret_opt.zero_grad()\n",
    "        regret_loss.backward()\n",
    "        self.regret_opt.step()\n",
    "        return round(regret_loss.item(),4)\n",
    "    \n",
    "    def learn_policy(self, obs, returns, masks, actions, values, selected_log_probs):\n",
    "        \n",
    "        # np.array -> torch.Tensor\n",
    "        obs = torch.FloatTensor(obs) # (m, 4, 84, 84)\n",
    "        actions = torch.LongTensor(actions) # (m)\n",
    "        selected_log_probs = torch.FloatTensor(selected_log_probs) # (m)\n",
    "        if USE_GPU:\n",
    "            obs = obs.cuda()\n",
    "            actions = actions.cuda()\n",
    "            selected_log_probs = selected_log_probs.cuda()\n",
    "        \n",
    "        # get action log probs and state values\n",
    "        action_log_probs, state_values = self.net(obs)\n",
    "        # (m, N_ACTIONS), (m, 1)\n",
    "        target = F.softmax(self.regret_net(obs), dim=1).detach() # (m, N_ACTIONS)\n",
    "        \n",
    "        # calc probs\n",
    "        probs = F.softmax(action_log_probs, dim=1) # (m, N_ACTIONS)\n",
    "        # (m, N_ACTIONS)\n",
    "        \n",
    "        loss = torch.sum(probs * (action_log_probs - torch.log(target + 1e-8)))\n",
    "\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialize results!\n",
      "Collecting experience...\n",
      "N update:  1 | Mean ep 100 return:  0.88 /Used Time: 8.98 /Used Step: 4004\n",
      "N update:  2 | Mean ep 100 return:  0.81 /Used Time: 17.95 /Used Step: 8008\n",
      "N update:  3 | Mean ep 100 return:  0.9 /Used Time: 26.81 /Used Step: 12012\n",
      "N update:  4 | Mean ep 100 return:  1.01 /Used Time: 35.91 /Used Step: 16016\n",
      "N update:  5 | Mean ep 100 return:  1.08 /Used Time: 44.93 /Used Step: 20020\n",
      "N update:  6 | Mean ep 100 return:  1.3 /Used Time: 53.89 /Used Step: 24024\n",
      "N update:  7 | Mean ep 100 return:  1.29 /Used Time: 62.91 /Used Step: 28028\n",
      "N update:  8 | Mean ep 100 return:  1.44 /Used Time: 71.87 /Used Step: 32032\n",
      "N update:  9 | Mean ep 100 return:  1.46 /Used Time: 80.83 /Used Step: 36036\n",
      "N update:  10 | Mean ep 100 return:  1.61 /Used Time: 89.82 /Used Step: 40040\n",
      "N update:  11 | Mean ep 100 return:  1.81 /Used Time: 98.75 /Used Step: 44044\n",
      "N update:  12 | Mean ep 100 return:  1.98 /Used Time: 107.7 /Used Step: 48048\n",
      "N update:  13 | Mean ep 100 return:  1.98 /Used Time: 116.72 /Used Step: 52052\n",
      "N update:  14 | Mean ep 100 return:  2.25 /Used Time: 125.52 /Used Step: 56056\n",
      "N update:  15 | Mean ep 100 return:  2.42 /Used Time: 134.47 /Used Step: 60060\n",
      "N update:  16 | Mean ep 100 return:  2.49 /Used Time: 143.3 /Used Step: 64064\n",
      "N update:  17 | Mean ep 100 return:  2.65 /Used Time: 152.07 /Used Step: 68068\n",
      "N update:  18 | Mean ep 100 return:  2.85 /Used Time: 160.96 /Used Step: 72072\n",
      "N update:  19 | Mean ep 100 return:  3.2 /Used Time: 169.78 /Used Step: 76076\n",
      "N update:  20 | Mean ep 100 return:  3.16 /Used Time: 178.55 /Used Step: 80080\n",
      "N update:  21 | Mean ep 100 return:  3.35 /Used Time: 187.3 /Used Step: 84084\n",
      "N update:  22 | Mean ep 100 return:  3.58 /Used Time: 195.98 /Used Step: 88088\n",
      "N update:  23 | Mean ep 100 return:  3.72 /Used Time: 204.66 /Used Step: 92092\n",
      "N update:  24 | Mean ep 100 return:  3.88 /Used Time: 213.34 /Used Step: 96096\n",
      "N update:  25 | Mean ep 100 return:  3.81 /Used Time: 222.2 /Used Step: 100100\n",
      "N update:  26 | Mean ep 100 return:  3.94 /Used Time: 230.95 /Used Step: 104104\n",
      "N update:  27 | Mean ep 100 return:  4.21 /Used Time: 239.65 /Used Step: 108108\n",
      "N update:  28 | Mean ep 100 return:  4.21 /Used Time: 248.37 /Used Step: 112112\n",
      "N update:  29 | Mean ep 100 return:  4.29 /Used Time: 257.17 /Used Step: 116116\n",
      "N update:  30 | Mean ep 100 return:  4.43 /Used Time: 265.86 /Used Step: 120120\n",
      "N update:  31 | Mean ep 100 return:  4.57 /Used Time: 274.66 /Used Step: 124124\n",
      "N update:  32 | Mean ep 100 return:  4.84 /Used Time: 283.31 /Used Step: 128128\n",
      "N update:  33 | Mean ep 100 return:  5.06 /Used Time: 292.02 /Used Step: 132132\n",
      "N update:  34 | Mean ep 100 return:  5.39 /Used Time: 300.66 /Used Step: 136136\n",
      "N update:  35 | Mean ep 100 return:  5.53 /Used Time: 309.24 /Used Step: 140140\n",
      "N update:  36 | Mean ep 100 return:  5.9 /Used Time: 317.74 /Used Step: 144144\n",
      "N update:  37 | Mean ep 100 return:  6.11 /Used Time: 326.51 /Used Step: 148148\n",
      "N update:  38 | Mean ep 100 return:  6.29 /Used Time: 335.17 /Used Step: 152152\n",
      "N update:  39 | Mean ep 100 return:  6.48 /Used Time: 343.75 /Used Step: 156156\n",
      "N update:  40 | Mean ep 100 return:  6.88 /Used Time: 352.36 /Used Step: 160160\n",
      "N update:  41 | Mean ep 100 return:  7.01 /Used Time: 360.98 /Used Step: 164164\n",
      "N update:  42 | Mean ep 100 return:  7.1 /Used Time: 369.65 /Used Step: 168168\n",
      "N update:  43 | Mean ep 100 return:  6.98 /Used Time: 378.46 /Used Step: 172172\n",
      "N update:  44 | Mean ep 100 return:  7.21 /Used Time: 387.16 /Used Step: 176176\n",
      "N update:  45 | Mean ep 100 return:  7.18 /Used Time: 395.87 /Used Step: 180180\n",
      "N update:  46 | Mean ep 100 return:  7.23 /Used Time: 404.54 /Used Step: 184184\n",
      "N update:  47 | Mean ep 100 return:  7.39 /Used Time: 413.24 /Used Step: 188188\n",
      "N update:  48 | Mean ep 100 return:  7.57 /Used Time: 421.92 /Used Step: 192192\n",
      "N update:  49 | Mean ep 100 return:  7.6 /Used Time: 430.45 /Used Step: 196196\n",
      "N update:  50 | Mean ep 100 return:  7.61 /Used Time: 439.05 /Used Step: 200200\n",
      "N update:  51 | Mean ep 100 return:  7.67 /Used Time: 447.71 /Used Step: 204204\n",
      "N update:  52 | Mean ep 100 return:  7.99 /Used Time: 456.41 /Used Step: 208208\n",
      "N update:  53 | Mean ep 100 return:  8.28 /Used Time: 465.08 /Used Step: 212212\n",
      "N update:  54 | Mean ep 100 return:  8.57 /Used Time: 473.66 /Used Step: 216216\n",
      "N update:  55 | Mean ep 100 return:  8.8 /Used Time: 482.32 /Used Step: 220220\n",
      "N update:  56 | Mean ep 100 return:  9.17 /Used Time: 490.82 /Used Step: 224224\n",
      "N update:  57 | Mean ep 100 return:  9.5 /Used Time: 499.35 /Used Step: 228228\n",
      "N update:  58 | Mean ep 100 return:  9.82 /Used Time: 507.81 /Used Step: 232232\n",
      "N update:  59 | Mean ep 100 return:  10.08 /Used Time: 516.52 /Used Step: 236236\n",
      "N update:  60 | Mean ep 100 return:  10.27 /Used Time: 525.07 /Used Step: 240240\n",
      "N update:  61 | Mean ep 100 return:  10.15 /Used Time: 533.63 /Used Step: 244244\n",
      "N update:  62 | Mean ep 100 return:  10.36 /Used Time: 542.27 /Used Step: 248248\n",
      "N update:  63 | Mean ep 100 return:  10.46 /Used Time: 550.86 /Used Step: 252252\n",
      "N update:  64 | Mean ep 100 return:  10.48 /Used Time: 559.58 /Used Step: 256256\n",
      "N update:  65 | Mean ep 100 return:  10.39 /Used Time: 568.18 /Used Step: 260260\n",
      "N update:  66 | Mean ep 100 return:  10.47 /Used Time: 576.77 /Used Step: 264264\n",
      "N update:  67 | Mean ep 100 return:  10.33 /Used Time: 585.37 /Used Step: 268268\n",
      "N update:  68 | Mean ep 100 return:  10.3 /Used Time: 593.98 /Used Step: 272272\n",
      "N update:  69 | Mean ep 100 return:  10.28 /Used Time: 602.52 /Used Step: 276276\n",
      "N update:  70 | Mean ep 100 return:  10.22 /Used Time: 611.04 /Used Step: 280280\n",
      "N update:  71 | Mean ep 100 return:  10.38 /Used Time: 619.69 /Used Step: 284284\n",
      "N update:  72 | Mean ep 100 return:  10.63 /Used Time: 628.28 /Used Step: 288288\n",
      "N update:  73 | Mean ep 100 return:  10.88 /Used Time: 636.95 /Used Step: 292292\n",
      "N update:  74 | Mean ep 100 return:  11.1 /Used Time: 645.55 /Used Step: 296296\n",
      "N update:  75 | Mean ep 100 return:  11.23 /Used Time: 654.03 /Used Step: 300300\n",
      "N update:  76 | Mean ep 100 return:  11.26 /Used Time: 662.47 /Used Step: 304304\n",
      "N update:  77 | Mean ep 100 return:  11.5 /Used Time: 671.02 /Used Step: 308308\n",
      "N update:  78 | Mean ep 100 return:  11.66 /Used Time: 679.46 /Used Step: 312312\n",
      "N update:  79 | Mean ep 100 return:  11.66 /Used Time: 687.96 /Used Step: 316316\n",
      "N update:  80 | Mean ep 100 return:  11.81 /Used Time: 696.56 /Used Step: 320320\n",
      "N update:  81 | Mean ep 100 return:  11.81 /Used Time: 705.08 /Used Step: 324324\n",
      "N update:  82 | Mean ep 100 return:  11.79 /Used Time: 713.65 /Used Step: 328328\n",
      "N update:  83 | Mean ep 100 return:  11.76 /Used Time: 722.23 /Used Step: 332332\n",
      "N update:  84 | Mean ep 100 return:  11.75 /Used Time: 730.73 /Used Step: 336336\n",
      "N update:  85 | Mean ep 100 return:  11.5 /Used Time: 739.31 /Used Step: 340340\n",
      "N update:  86 | Mean ep 100 return:  11.66 /Used Time: 747.81 /Used Step: 344344\n",
      "N update:  87 | Mean ep 100 return:  11.77 /Used Time: 756.28 /Used Step: 348348\n",
      "N update:  88 | Mean ep 100 return:  11.74 /Used Time: 764.85 /Used Step: 352352\n",
      "N update:  89 | Mean ep 100 return:  11.78 /Used Time: 773.56 /Used Step: 356356\n",
      "N update:  90 | Mean ep 100 return:  11.86 /Used Time: 782.1 /Used Step: 360360\n",
      "N update:  91 | Mean ep 100 return:  12.05 /Used Time: 790.63 /Used Step: 364364\n",
      "N update:  92 | Mean ep 100 return:  12.0 /Used Time: 799.28 /Used Step: 368368\n",
      "N update:  93 | Mean ep 100 return:  12.06 /Used Time: 807.84 /Used Step: 372372\n",
      "N update:  94 | Mean ep 100 return:  12.38 /Used Time: 816.45 /Used Step: 376376\n",
      "N update:  95 | Mean ep 100 return:  12.54 /Used Time: 824.89 /Used Step: 380380\n",
      "N update:  96 | Mean ep 100 return:  12.72 /Used Time: 833.41 /Used Step: 384384\n",
      "N update:  97 | Mean ep 100 return:  12.72 /Used Time: 841.89 /Used Step: 388388\n",
      "N update:  98 | Mean ep 100 return:  13.04 /Used Time: 850.41 /Used Step: 392392\n",
      "N update:  99 | Mean ep 100 return:  12.95 /Used Time: 858.95 /Used Step: 396396\n",
      "N update:  100 | Mean ep 100 return:  12.71 /Used Time: 867.57 /Used Step: 400400\n",
      "N update:  101 | Mean ep 100 return:  12.79 /Used Time: 876.12 /Used Step: 404404\n",
      "N update:  102 | Mean ep 100 return:  12.7 /Used Time: 884.69 /Used Step: 408408\n",
      "N update:  103 | Mean ep 100 return:  12.8 /Used Time: 893.45 /Used Step: 412412\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N update:  104 | Mean ep 100 return:  12.64 /Used Time: 902.08 /Used Step: 416416\n",
      "N update:  105 | Mean ep 100 return:  12.76 /Used Time: 910.63 /Used Step: 420420\n",
      "N update:  106 | Mean ep 100 return:  12.83 /Used Time: 919.11 /Used Step: 424424\n",
      "N update:  107 | Mean ep 100 return:  12.8 /Used Time: 927.66 /Used Step: 428428\n",
      "N update:  108 | Mean ep 100 return:  12.77 /Used Time: 936.43 /Used Step: 432432\n",
      "N update:  109 | Mean ep 100 return:  13.13 /Used Time: 944.98 /Used Step: 436436\n",
      "N update:  110 | Mean ep 100 return:  13.16 /Used Time: 953.46 /Used Step: 440440\n",
      "N update:  111 | Mean ep 100 return:  13.39 /Used Time: 961.98 /Used Step: 444444\n",
      "N update:  112 | Mean ep 100 return:  13.22 /Used Time: 970.68 /Used Step: 448448\n",
      "N update:  113 | Mean ep 100 return:  13.59 /Used Time: 979.21 /Used Step: 452452\n",
      "N update:  114 | Mean ep 100 return:  13.75 /Used Time: 987.9 /Used Step: 456456\n",
      "N update:  115 | Mean ep 100 return:  13.97 /Used Time: 996.62 /Used Step: 460460\n",
      "N update:  116 | Mean ep 100 return:  14.24 /Used Time: 1005.46 /Used Step: 464464\n",
      "N update:  117 | Mean ep 100 return:  14.37 /Used Time: 1014.17 /Used Step: 468468\n",
      "N update:  118 | Mean ep 100 return:  14.52 /Used Time: 1022.75 /Used Step: 472472\n",
      "N update:  119 | Mean ep 100 return:  14.93 /Used Time: 1031.28 /Used Step: 476476\n"
     ]
    }
   ],
   "source": [
    "ppo = PPO()\n",
    "runner = Runner(env=env, model=ppo, nsteps=TRAJ_LEN, gamma=GAMMA, lam=LAMBDA)\n",
    "\n",
    "# model load with check\n",
    "if LOAD and os.path.isfile(PRED_PATH) and os.path.isfile(TARGET_PATH):\n",
    "    ppo.load_model()\n",
    "    pkl_file = open(RESULT_PATH,'rb')\n",
    "    result = pickle.load(pkl_file)\n",
    "    pkl_file.close()\n",
    "    print('Load complete!')\n",
    "else:\n",
    "    result = []\n",
    "    print('Initialize results!')\n",
    "\n",
    "print('Collecting experience...')\n",
    "\n",
    "# episode step for accumulate reward \n",
    "epinfobuf = deque(maxlen=100)\n",
    "# in PPO, we iterate over optimization step\n",
    "nbatch = N_ENVS * TRAJ_LEN\n",
    "nupdates = N_STEP// nbatch\n",
    "# check learning time\n",
    "start_time = time.time()\n",
    "\n",
    "for update in range(1, nupdates+1):\n",
    "    # get minibatch\n",
    "    obs, returns, masks, actions, values, neglogpacs, epinfos = runner.run()\n",
    "    epinfobuf.extend(epinfos)\n",
    "    \n",
    "    # calculate loss\n",
    "    inds = np.arange(nbatch)\n",
    "    for _ in range(N_OPT_EPOCHS):\n",
    "        a_losses, c_losses, e_losses, t_losses = list(), list(), list(), list()\n",
    "        # shuffle indices for i.i.d.\n",
    "        np.random.shuffle(inds)\n",
    "        # 0 to batch_size with batch_train_size step\n",
    "        for start in range(0, nbatch, BATCH_SIZE):\n",
    "            end = start + BATCH_SIZE\n",
    "            mbinds = inds[start:end]\n",
    "            slices = (arr[mbinds] for arr in (obs, returns, masks, actions, values, neglogpacs))\n",
    "            ppo.learn_value(*slices)\n",
    "            \n",
    "    # calculate loss\n",
    "    inds = np.arange(nbatch)\n",
    "    for _ in range(N_OPT_EPOCHS):\n",
    "        a_losses, c_losses, e_losses, t_losses = list(), list(), list(), list()\n",
    "        # shuffle indices for i.i.d.\n",
    "        np.random.shuffle(inds)\n",
    "        # 0 to batch_size with batch_train_size step\n",
    "        for start in range(0, nbatch, BATCH_SIZE):\n",
    "            end = start + BATCH_SIZE\n",
    "            mbinds = inds[start:end]\n",
    "            slices = (arr[mbinds] for arr in (obs, returns, masks, actions, values, neglogpacs))\n",
    "            regret_loss = ppo.learn_regret(*slices)\n",
    "            \n",
    "    ppo.update_target(ppo.regret_net_old, ppo.regret_net, 1.0)\n",
    "            \n",
    "    # calculate loss\n",
    "    inds = np.arange(nbatch)\n",
    "    for _ in range(N_OPT_EPOCHS):\n",
    "        a_losses, c_losses, e_losses, t_losses = list(), list(), list(), list()\n",
    "        # shuffle indices for i.i.d.\n",
    "        np.random.shuffle(inds)\n",
    "        # 0 to batch_size with batch_train_size step\n",
    "        for start in range(0, nbatch, BATCH_SIZE):\n",
    "            end = start + BATCH_SIZE\n",
    "            mbinds = inds[start:end]\n",
    "            slices = (arr[mbinds] for arr in (obs, returns, masks, actions, values, neglogpacs))\n",
    "            ppo.learn_policy(*slices)\n",
    "            \n",
    "    if update % LOG_FREQ == 0:\n",
    "        # print log and save\n",
    "        # check time interval\n",
    "        time_interval = round(time.time() - start_time, 2)\n",
    "        # calc mean return\n",
    "        mean_100_ep_return = round(np.mean([epinfo['r'] for epinfo in epinfobuf]),2)\n",
    "        result.append(mean_100_ep_return)\n",
    "        # print epi log\n",
    "        print('N update: ',update,\n",
    "              '| Mean ep 100 return: ', mean_100_ep_return,\n",
    "              '/Used Time:',time_interval,\n",
    "              '/Used Step:',ppo.memory_counter*N_ENVS)\n",
    "        # save model\n",
    "        if SAVE:\n",
    "            ppo.save_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 결과 시각화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(range(len(result)), result)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import animation\n",
    "\n",
    "def display_frames_as_gif(frames):\n",
    "    patch = plt.imshow(frames[0])\n",
    "    plt.axis('off')\n",
    "    def animate(i):\n",
    "        patch.set_data(frames[i])\n",
    "        \n",
    "    anim = animation.FuncAnimation(plt.gcf(), animate, frames = len(frames), interval=5)\n",
    "    anim.save('./cfh_breakout_result.gif', writer='imagemagick', fps=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = wrap(gym.make('BreakoutNoFrameskip-v4'))\n",
    "s = np.array(env.reset())\n",
    "total_reward = 0\n",
    "frames = []\n",
    "\n",
    "for t in range(10000):\n",
    "    # Render into buffer. \n",
    "    frames.append(env.render(mode = 'rgb_array'))\n",
    "    a, v, l = ppo.choose_action(np.expand_dims(s,axis=0))\n",
    "    # take action and get next state\n",
    "    s_, r, done, info = env.step(a)\n",
    "    s_ = np.array(s_)\n",
    "    total_reward += r\n",
    "    if done:\n",
    "        break\n",
    "    s = s_\n",
    "env.close()\n",
    "print('Total Reward : %.2f'%total_reward)\n",
    "display_frames_as_gif(frames)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](./ppo_pong_result.gif \"segment\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
