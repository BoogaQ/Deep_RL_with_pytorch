{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 필요한 모듈 설치 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "\n",
    "import random\n",
    "import os\n",
    "import pickle\n",
    "import time\n",
    "from tqdm import tqdm_notebook\n",
    "from collections import deque\n",
    "from replay_memory import ReplayBuffer, SilPrioritizedReplayBuffer\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "sns.set()\n",
    "\n",
    "from wrappers import wrap, wrap_cover, SubprocVecEnv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 하이퍼 파라미터 정의하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kaist_mlilab/gym/gym/logger.py:30: UserWarning: \u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.uint8'>. Please provide explicit dtype.\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n",
      "/home/kaist_mlilab/gym/gym/logger.py:30: UserWarning: \u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n",
      "/home/kaist_mlilab/gym/gym/logger.py:30: UserWarning: \u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.uint8'>. Please provide explicit dtype.\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n",
      "/home/kaist_mlilab/gym/gym/logger.py:30: UserWarning: \u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n",
      "/home/kaist_mlilab/gym/gym/logger.py:30: UserWarning: \u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.uint8'>. Please provide explicit dtype.\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n",
      "/home/kaist_mlilab/gym/gym/logger.py:30: UserWarning: \u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n",
      "/home/kaist_mlilab/gym/gym/logger.py:30: UserWarning: \u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.uint8'>. Please provide explicit dtype.\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N_ACTIONS :  4\n",
      "N_STATES :  (4, 84, 84)\n",
      "USE GPU: True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kaist_mlilab/gym/gym/logger.py:30: UserWarning: \u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n",
      "/home/kaist_mlilab/gym/gym/logger.py:30: UserWarning: \u001b[33mWARN: <class 'wrappers.FrameStack'> doesn't implement 'reset' method, but it implements deprecated '_reset' method.\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n",
      "/home/kaist_mlilab/gym/gym/logger.py:30: UserWarning: \u001b[33mWARN: <class 'wrappers.FrameStack'> doesn't implement 'reset' method, but it implements deprecated '_reset' method.\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n",
      "/home/kaist_mlilab/gym/gym/logger.py:30: UserWarning: \u001b[33mWARN: <class 'wrappers.FrameStack'> doesn't implement 'reset' method, but it implements deprecated '_reset' method.\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n",
      "/home/kaist_mlilab/gym/gym/logger.py:30: UserWarning: \u001b[33mWARN: <class 'wrappers.FrameStack'> doesn't implement 'reset' method, but it implements deprecated '_reset' method.\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    }
   ],
   "source": [
    "'''SAC Settings'''\n",
    "# coefficient of entropy regularization\n",
    "ENT_COEF = 1e-2\n",
    "# experience replay memory size\n",
    "MEMORY_CAPACITY = 10**6\n",
    "# learn start\n",
    "LEARN_START = int(1e+3)\n",
    "# learn frequency\n",
    "LEARN_FREQ = 1\n",
    "\n",
    "'''Environment Settings'''\n",
    "# sequential images to define state\n",
    "STATE_LEN = 4\n",
    "# openai gym env name\n",
    "ENV_NAME = 'BreakoutNoFrameskip-v4'\n",
    "# number of environments for SAC\n",
    "N_ENVS = 4\n",
    "# define gym \n",
    "env = SubprocVecEnv([wrap_cover(ENV_NAME) for i in range(N_ENVS)])\n",
    "# check gym setting\n",
    "N_ACTIONS = env.action_space.n;print('N_ACTIONS : ',N_ACTIONS) #  4\n",
    "N_STATES = env.observation_space.shape;print('N_STATES : ',N_STATES) # (4, 84, 84)\n",
    "# Total simulation step\n",
    "N_STEP = 10**7\n",
    "# gamma for MDP\n",
    "GAMMA = 0.99\n",
    "# visualize for agent playing\n",
    "RENDERING = False\n",
    "\n",
    "'''Training settings'''\n",
    "# check GPU usage\n",
    "USE_GPU = torch.cuda.is_available()\n",
    "print('USE GPU: '+str(USE_GPU))\n",
    "# mini-batch size\n",
    "BATCH_SIZE = 32\n",
    "# learning rage\n",
    "LR = 1e-4\n",
    "# clip gradient\n",
    "MAX_GRAD_NORM = 0.1\n",
    "\n",
    "'''Save&Load Settings'''\n",
    "# log frequency\n",
    "LOG_FREQ = int(1e+3)\n",
    "# check save/load\n",
    "SAVE = True\n",
    "LOAD = False\n",
    "# paths for predction net, target net, result log\n",
    "ACTOR_PATH = './data/model/sil_actor_net.pkl'\n",
    "CRITIC_PATH = './data/model/sil_critic_net.pkl'\n",
    "ACTION_CRITIC_PATH = './data/model/sil_action_critic_net.pkl'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 네트워크 구조 정의하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActorConvNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ActorConvNet, self).__init__()\n",
    "        # architecture def\n",
    "        self.feature_extraction = nn.Sequential(\n",
    "            nn.Conv2d(STATE_LEN, 32, kernel_size=8, stride=4),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, kernel_size=4, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, stride=1),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.fc = nn.Linear(7 * 7 * 64, 512)\n",
    "        # actor\n",
    "        self.actor = nn.Linear(512, N_ACTIONS)\n",
    "        \n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.orthogonal_(m.weight, gain = np.sqrt(2))\n",
    "                nn.init.constant_(m.bias, 0.0)\n",
    "            elif isinstance(m, nn.Linear):\n",
    "                nn.init.kaiming_normal_(m.weight)\n",
    "                nn.init.constant_(m.bias, 0.0)\n",
    "            \n",
    "\n",
    "    def forward(self, x):\n",
    "        # x is a tensor of (m, 4, 84, 84)\n",
    "        x = self.feature_extraction(x / 255.0)\n",
    "        # x.size(0) : mini-batch size\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = F.relu(self.fc(x))\n",
    "        # use log_softmax for numerical stability\n",
    "        action_log_prob = F.log_softmax(self.actor(x), dim=1)\n",
    "\n",
    "        return action_log_prob\n",
    "\n",
    "    def save(self, PATH):\n",
    "        torch.save(self.state_dict(),ACTOR_PATH)\n",
    "\n",
    "    def load(self, PATH):\n",
    "        self.load_state_dict(torch.load(ACTOR_PATH))\n",
    "        \n",
    "class CriticConvNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CriticConvNet, self).__init__()\n",
    "        # architecture def\n",
    "        self.feature_extraction = nn.Sequential(\n",
    "            nn.Conv2d(STATE_LEN, 32, kernel_size=8, stride=4),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, kernel_size=4, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, stride=1),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.fc = nn.Linear(7 * 7 * 64, 512)\n",
    "        # actor\n",
    "        self.critic = nn.Linear(512, 1)\n",
    "        \n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.orthogonal_(m.weight, gain = np.sqrt(2))\n",
    "                nn.init.constant_(m.bias, 0.0)\n",
    "            elif isinstance(m, nn.Linear):\n",
    "                nn.init.kaiming_normal_(m.weight)\n",
    "                nn.init.constant_(m.bias, 0.0)\n",
    "            \n",
    "\n",
    "    def forward(self, x):\n",
    "        # x is a tensor of (m, 4, 84, 84)\n",
    "        x = self.feature_extraction(x / 255.0)\n",
    "        # x.size(0) : mini-batch size\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = F.relu(self.fc(x))\n",
    "        # use log_softmax for numerical stability\n",
    "        value = self.critic(x)\n",
    "\n",
    "        return value\n",
    "\n",
    "    def save(self, PATH):\n",
    "        torch.save(self.state_dict(),CRITIC_PATH)\n",
    "\n",
    "    def load(self, PATH):\n",
    "        self.load_state_dict(torch.load(CRITIC_PATH))\n",
    "        \n",
    "class ActionCriticConvNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ActionCriticConvNet, self).__init__()\n",
    "        # architecture def\n",
    "        self.feature_extraction = nn.Sequential(\n",
    "            nn.Conv2d(STATE_LEN, 32, kernel_size=8, stride=4),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, kernel_size=4, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, stride=1),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.fc = nn.Linear(7 * 7 * 64, 512)\n",
    "        # actor\n",
    "        self.action_critic = nn.Linear(512, N_ACTIONS)\n",
    "        \n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.orthogonal_(m.weight, gain = np.sqrt(2))\n",
    "                nn.init.constant_(m.bias, 0.0)\n",
    "            elif isinstance(m, nn.Linear):\n",
    "                nn.init.kaiming_normal_(m.weight)\n",
    "                nn.init.constant_(m.bias, 0.0)\n",
    "            \n",
    "\n",
    "    def forward(self, x):\n",
    "        # x is a tensor of (m, 4, 84, 84)\n",
    "        x = self.feature_extraction(x / 255.0)\n",
    "        # x.size(0) : mini-batch size\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = F.relu(self.fc(x))\n",
    "        # use log_softmax for numerical stability\n",
    "        action_value = self.action_critic(x)\n",
    "\n",
    "        return action_value\n",
    "\n",
    "    def save(self, PATH):\n",
    "        torch.save(self.state_dict(),ACTION_CRITIC_PATH)\n",
    "\n",
    "    def load(self, PATH):\n",
    "        self.load_state_dict(torch.load(ACTION_CRITIC_PATH))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PPO 정의하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# part of this code is based on original SIL code \n",
    "# https://github.com/junhyukoh/self-imitation-learning/blob/master/baselines/common/self_imitation.py\n",
    "class SSAC:\n",
    "    def __init__(self):\n",
    "        self.actor_net = ActorConvNet()\n",
    "        self.critic_net = CriticConvNet()\n",
    "        self.critic_target = CriticConvNet()\n",
    "        self.action_critic_net = ActionCriticConvNet()\n",
    "        # use gpu\n",
    "        if USE_GPU:\n",
    "            self.actor_net = self.actor_net.cuda(device=3)\n",
    "            self.critic_net = self.critic_net.cuda(device=3)\n",
    "            # critic target network for stability\n",
    "            self.critic_target = self.critic_net.cuda(device=3)\n",
    "            self.action_critic_net = self.action_critic_net.cuda(device=3)\n",
    "        \n",
    "        # sync net and target\n",
    "        self.critic_target.load_state_dict(self.critic_net.state_dict())\n",
    "            \n",
    "        # simulator step conter\n",
    "        self.memory_counter = 0\n",
    "        self.learn_step_counter = 0\n",
    "        \n",
    "        # create running episode memory\n",
    "        self.running_episodes = [[] for _ in range(N_ENVS)]\n",
    "        \n",
    "        # Create the replay buffer\n",
    "        self.replay_buffer = ReplayBuffer(MEMORY_CAPACITY)\n",
    "        self.sil_buffer = SilPrioritizedReplayBuffer(MEMORY_CAPACITY, 0.6)\n",
    "            \n",
    "        # define optimizer\n",
    "        self.actor_opt = torch.optim.Adam(self.actor_net.parameters(), lr=LR)\n",
    "        self.critic_opt = torch.optim.Adam(self.critic_net.parameters(), lr=LR)\n",
    "        self.action_critic_opt = torch.optim.Adam(self.action_critic_net.parameters(), lr=LR)\n",
    "        \n",
    "    def update_target(self, target, pred, update_rate):\n",
    "        # update target network parameters using predcition network\n",
    "        for target_param, pred_param in zip(target.parameters(), pred.parameters()):\n",
    "            target_param.data.copy_((1.0 - update_rate) \\\n",
    "                                    * target_param.data + update_rate*pred_param.data)\n",
    "        \n",
    "    def save_model(self):\n",
    "        self.actor_net.cpu()\n",
    "        self.critic_net.cpu()\n",
    "        self.action_critic_net.cpu()\n",
    "        \n",
    "        self.actor_net.save(ACTOR_PATH)\n",
    "        self.critic_net.save(CRITIC_PATH)\n",
    "        self.action_critic_net.save(ACTION_CRITIC_PATH)\n",
    "        \n",
    "        if USE_GPU:\n",
    "            self.actor_net.cuda(device=3)\n",
    "            self.critic_net.cuda(device=3)\n",
    "            self.action_critic_net.cuda(device=3)\n",
    "            \n",
    "    def load_model(self):\n",
    "        self.actor_net.cpu()\n",
    "        self.critic_net.cpu()\n",
    "        self.action_critic_net.cpu()\n",
    "        \n",
    "        self.actor_net.load(ACTOR_PATH)\n",
    "        self.critic_net.load(CRITIC_PATH)\n",
    "        self.action_critic_net.load(ACTION_CRITIC_PATH)\n",
    "        \n",
    "        if USE_GPU:\n",
    "            self.actor_net.cuda(device=3)\n",
    "            self.critic_net.cuda(device=3)\n",
    "            self.action_critic_net.cuda(device=3)\n",
    "        \n",
    "    def choose_action(self, x):\n",
    "        # Assume that x is a np.array of shape (nenvs, 4, 84, 84)\n",
    "        x = torch.FloatTensor(x)\n",
    "        if USE_GPU:\n",
    "            x = x.cuda(device=3)\n",
    "        # get action log probs and state values\n",
    "        action_log_prob = self.actor_net(x)\n",
    "        action_prob = F.softmax(action_log_prob, dim=1).data.cpu().numpy()\n",
    "        # sample actions\n",
    "        action = np.array([np.random.choice(N_ACTIONS,p=action_prob[i]) for i in range(len(action_prob))])\n",
    "        return action\n",
    "    \n",
    "    def store_transition(self, s, a, r, s_, done):\n",
    "        self.memory_counter += 1\n",
    "        self.replay_buffer.add(s, a, r, s_, float(done))\n",
    "        \n",
    "    def sil_store_transition(self, s, a, r, done):\n",
    "        # sil episode caching\n",
    "        for n in range(N_ENVS):\n",
    "            self.running_episodes[n].append([s[n], a[n], r[n]])\n",
    "\n",
    "        for n, d in enumerate(done):\n",
    "            if d:\n",
    "                self.update_buffer(self.running_episodes[n])\n",
    "                self.running_episodes[n] = []\n",
    "                \n",
    "    # SIL update buffer\n",
    "    def update_buffer(self, trajectory):\n",
    "        positive_reward = False\n",
    "        for (ob, a, r) in trajectory:\n",
    "            if r > 0:\n",
    "                positive_reward = True\n",
    "                break\n",
    "        if positive_reward:\n",
    "            self.add_episode(trajectory)\n",
    "                \n",
    "    def add_episode(self, trajectory):\n",
    "        obs = []\n",
    "        actions = []\n",
    "        rewards = []\n",
    "        dones = []\n",
    "        \n",
    "        for (ob, action, reward) in trajectory:\n",
    "            obs.append(ob)\n",
    "            actions.append(action)\n",
    "            rewards.append(reward)\n",
    "            dones.append(False)\n",
    "        dones[len(dones)-1]=True\n",
    "        returns = self.discount_with_dones(rewards, dones, GAMMA)\n",
    "        for (ob, action, R) in list(zip(obs, actions, returns)):\n",
    "            self.sil_buffer.add(ob, action, R)\n",
    "            \n",
    "    def discount_with_dones(self, rewards, dones, GAMMA):\n",
    "        discounted = []\n",
    "        r = 0\n",
    "        for reward, done in zip(rewards[::-1], dones[::-1]):\n",
    "            r = reward + GAMMA*r*(1.-done) # fixed off by one bug\n",
    "            discounted.append(r)\n",
    "        return discounted[::-1]\n",
    "\n",
    "    def learn(self):\n",
    "        self.learn_step_counter += 1\n",
    "\n",
    "        # data sample from experience replay\n",
    "        b_state_memory, b_action_memory, b_reward_memory, \\\n",
    "        b_next_state_memory, b_done = self.replay_buffer.sample(BATCH_SIZE)\n",
    "        b_weights, b_idxes = np.ones_like(b_reward_memory), None\n",
    "\n",
    "        b_s = torch.FloatTensor(b_state_memory)\n",
    "        b_a = torch.LongTensor(b_action_memory)\n",
    "        b_r = torch.FloatTensor(b_reward_memory)\n",
    "        b_s_ = torch.FloatTensor(b_next_state_memory)\n",
    "        b_d = torch.FloatTensor(b_done)\n",
    "\n",
    "        if USE_GPU:\n",
    "            b_s, b_a, b_r, b_s_, b_d = b_s.cuda(device=3), b_a.cuda(device=3), b_r.cuda(device=3), b_s_.cuda(device=3), b_d.cuda(device=3)\n",
    "\n",
    "        # forward calc\n",
    "        action_log_prob = self.actor_net(b_s)\n",
    "        action_prob = F.softmax(action_log_prob, dim=1)\n",
    "        action_log_prob = F.log_softmax(action_log_prob, dim=1)\n",
    "        cur_value = self.critic_net(b_s).squeeze(1)\n",
    "        next_value = self.critic_target(b_s_)\n",
    "        action_value = self.action_critic_net(b_s)\n",
    "\n",
    "        # critic loss. eq (5) in SAC paper\n",
    "        value_target = (action_value - ENT_COEF * action_log_prob).gather(1, b_a.unsqueeze(1)).squeeze(1)\n",
    "        critic_loss = 0.5 * F.smooth_l1_loss(cur_value, value_target.detach())\n",
    "\n",
    "        # action critic loss. eq (7), (8) in SAC paper\n",
    "        action_value_target = b_r + GAMMA * (1-b_d) * next_value.squeeze(1)\n",
    "        action_critic_loss = 0.5 * F.smooth_l1_loss(action_value.gather(1, \n",
    "            b_a.unsqueeze(1)).squeeze(1), action_value_target.detach())\n",
    "\n",
    "        # actor loss. eq (10) in SAC paper\n",
    "        actor_loss = torch.mean(action_prob*(action_log_prob \\\n",
    "            - F.log_softmax(action_value.detach()/ENT_COEF, dim=1)))\n",
    "\n",
    "        self.actor_opt.zero_grad()\n",
    "        actor_loss.backward()\n",
    "        nn.utils.clip_grad_norm_(self.actor_net.parameters(), MAX_GRAD_NORM)\n",
    "        self.actor_opt.step()\n",
    "\n",
    "        self.critic_opt.zero_grad()\n",
    "        critic_loss.backward()\n",
    "        nn.utils.clip_grad_norm_(self.critic_net.parameters(), MAX_GRAD_NORM)\n",
    "        self.critic_opt.step()\n",
    "\n",
    "        self.action_critic_opt.zero_grad()\n",
    "        action_critic_loss.backward()\n",
    "        nn.utils.clip_grad_norm_(self.action_critic_net.parameters(), MAX_GRAD_NORM)\n",
    "        self.action_critic_opt.step()\n",
    "\n",
    "        self.update_target(self.critic_target, self.critic_net, 1e-3)\n",
    "\n",
    "        # SIL update\n",
    "        experience = self.sil_buffer.sample(BATCH_SIZE, beta=0)\n",
    "        (sil_b_s, sil_b_a, sil_b_r, sil_b_weights, sil_b_idxes) = experience\n",
    "\n",
    "        sil_b_s = torch.FloatTensor(sil_b_s)\n",
    "        sil_b_a = torch.LongTensor(sil_b_a)\n",
    "        sil_b_r = torch.FloatTensor(sil_b_r)\n",
    "        sil_b_w = torch.Tensor(sil_b_weights)\n",
    "\n",
    "        if USE_GPU:\n",
    "            sil_b_s, sil_b_a, sil_b_r, sil_b_w = sil_b_s.cuda(device=3), sil_b_a.cuda(device=3), sil_b_r.cuda(device=3), sil_b_w.cuda(device=3)\n",
    "\n",
    "        # forward calc\n",
    "        sil_action_log_prob = self.actor_net(sil_b_s)\n",
    "        sil_action_log_prob = F.log_softmax(sil_action_log_prob, dim=1)\n",
    "        sil_cur_value = self.critic_net(sil_b_s).squeeze(1)\n",
    "        sil_action_value = self.action_critic_net(sil_b_s)\n",
    "        sil_adv = (torch.clamp(F.relu(sil_b_r - sil_cur_value), 0.0, 1.0)).data.cpu().numpy()\n",
    "\n",
    "        # actor loss. eq (2) in SIL paper\n",
    "        sil_actor_loss = torch.mean( sil_b_w * (-sil_action_log_prob.gather(1,\n",
    "        sil_b_a.unsqueeze(1)).squeeze(1) * torch.clamp(F.relu(sil_b_r - sil_cur_value.detach()),0.0,1.0)))\n",
    "\n",
    "        # critic loss. eq (3) in SIL paper\n",
    "        sil_critic_loss = F.relu(sil_b_r - sil_cur_value)\n",
    "        sil_critic_loss = 0.5 * torch.mean(sil_b_w * (F.smooth_l1_loss(sil_critic_loss,\n",
    "        torch.zeros_like(sil_critic_loss), reduction='none')))\n",
    "        \n",
    "        # action critic_loss. this is not implemented in SIL paper\n",
    "        sil_action_critic_loss = F.relu(sil_b_r \\\n",
    "                            - sil_action_value.gather(1, sil_b_a.unsqueeze(1)).squeeze(1))\n",
    "        sil_action_critic_loss =0.5 * torch.mean(sil_b_w * \\\n",
    "                (F.smooth_l1_loss(sil_action_critic_loss, torch.zeros_like(sil_action_critic_loss)\\\n",
    "                , reduction='none')))\n",
    "\n",
    "        self.actor_opt.zero_grad()\n",
    "        sil_actor_loss.backward()\n",
    "        nn.utils.clip_grad_norm_(self.actor_net.parameters(), MAX_GRAD_NORM)\n",
    "        self.actor_opt.step()\n",
    "\n",
    "        self.critic_opt.zero_grad()\n",
    "        sil_critic_loss.backward()\n",
    "        nn.utils.clip_grad_norm_(self.critic_net.parameters(), MAX_GRAD_NORM)\n",
    "        self.critic_opt.step()\n",
    "        \n",
    "        self.action_critic_opt.zero_grad()\n",
    "        sil_action_critic_loss.backward()\n",
    "        nn.utils.clip_grad_norm_(self.action_critic_net.parameters(), MAX_GRAD_NORM)\n",
    "        self.action_critic_opt.step()\n",
    "\n",
    "        self.sil_buffer.update_priorities(sil_b_idxes, sil_adv)\n",
    "\n",
    "        return round(float(actor_loss), 4), round(float(critic_loss), 4), round(float(action_critic_loss), 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialize results!\n",
      "Collecting experience...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ead5e0ac8d9240b59cf363d37b5fc2b1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=2500000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Used Step: 4000 | Mean ep 100 return:  1.7 | Used Time: 8.59\n",
      "Used Step: 8000 | Mean ep 100 return:  1.52 | Used Time: 69.17\n",
      "Used Step: 12000 | Mean ep 100 return:  1.61 | Used Time: 126.41\n",
      "Used Step: 16000 | Mean ep 100 return:  1.82 | Used Time: 184.78\n",
      "Used Step: 20000 | Mean ep 100 return:  1.75 | Used Time: 242.84\n",
      "Used Step: 24000 | Mean ep 100 return:  1.96 | Used Time: 299.29\n",
      "Used Step: 28000 | Mean ep 100 return:  2.21 | Used Time: 354.39\n",
      "Used Step: 32000 | Mean ep 100 return:  2.53 | Used Time: 409.11\n",
      "Used Step: 36000 | Mean ep 100 return:  3.08 | Used Time: 462.77\n",
      "Used Step: 40000 | Mean ep 100 return:  3.54 | Used Time: 518.29\n",
      "Used Step: 44000 | Mean ep 100 return:  4.07 | Used Time: 572.92\n",
      "Used Step: 48000 | Mean ep 100 return:  4.63 | Used Time: 628.9\n",
      "Used Step: 52000 | Mean ep 100 return:  5.45 | Used Time: 682.24\n",
      "Used Step: 56000 | Mean ep 100 return:  6.07 | Used Time: 737.65\n",
      "Used Step: 60000 | Mean ep 100 return:  6.56 | Used Time: 792.36\n",
      "Used Step: 64000 | Mean ep 100 return:  7.25 | Used Time: 846.69\n",
      "Used Step: 68000 | Mean ep 100 return:  7.89 | Used Time: 901.83\n",
      "Used Step: 72000 | Mean ep 100 return:  8.85 | Used Time: 957.85\n",
      "Used Step: 76000 | Mean ep 100 return:  9.51 | Used Time: 1011.49\n",
      "Used Step: 80000 | Mean ep 100 return:  10.29 | Used Time: 1061.7\n",
      "Used Step: 84000 | Mean ep 100 return:  10.67 | Used Time: 1115.54\n",
      "Used Step: 88000 | Mean ep 100 return:  11.41 | Used Time: 1170.47\n",
      "Used Step: 92000 | Mean ep 100 return:  12.05 | Used Time: 1224.16\n",
      "Used Step: 96000 | Mean ep 100 return:  12.42 | Used Time: 1278.34\n",
      "Used Step: 100000 | Mean ep 100 return:  12.9 | Used Time: 1333.26\n",
      "Used Step: 104000 | Mean ep 100 return:  13.47 | Used Time: 1388.09\n",
      "Used Step: 108000 | Mean ep 100 return:  13.56 | Used Time: 1442.4\n",
      "Used Step: 112000 | Mean ep 100 return:  14.18 | Used Time: 1497.84\n",
      "Used Step: 116000 | Mean ep 100 return:  14.63 | Used Time: 1552.16\n",
      "Used Step: 120000 | Mean ep 100 return:  15.35 | Used Time: 1606.78\n",
      "Used Step: 124000 | Mean ep 100 return:  15.8 | Used Time: 1660.24\n",
      "Used Step: 128000 | Mean ep 100 return:  16.19 | Used Time: 1712.98\n",
      "Used Step: 132000 | Mean ep 100 return:  16.34 | Used Time: 1766.6\n",
      "Used Step: 136000 | Mean ep 100 return:  16.61 | Used Time: 1820.07\n",
      "Used Step: 140000 | Mean ep 100 return:  16.72 | Used Time: 1874.28\n",
      "Used Step: 144000 | Mean ep 100 return:  16.89 | Used Time: 1928.48\n",
      "Used Step: 148000 | Mean ep 100 return:  17.09 | Used Time: 1982.11\n",
      "Used Step: 152000 | Mean ep 100 return:  17.25 | Used Time: 2036.67\n",
      "Used Step: 156000 | Mean ep 100 return:  17.42 | Used Time: 2091.6\n",
      "Used Step: 160000 | Mean ep 100 return:  17.62 | Used Time: 2146.56\n",
      "Used Step: 164000 | Mean ep 100 return:  17.99 | Used Time: 2201.22\n",
      "Used Step: 168000 | Mean ep 100 return:  18.42 | Used Time: 2256.82\n",
      "Used Step: 172000 | Mean ep 100 return:  18.75 | Used Time: 2312.15\n",
      "Used Step: 176000 | Mean ep 100 return:  19.3 | Used Time: 2367.39\n",
      "Used Step: 180000 | Mean ep 100 return:  19.98 | Used Time: 2421.78\n",
      "Used Step: 184000 | Mean ep 100 return:  20.96 | Used Time: 2477.18\n",
      "Used Step: 188000 | Mean ep 100 return:  21.38 | Used Time: 2532.02\n",
      "Used Step: 192000 | Mean ep 100 return:  21.96 | Used Time: 2587.51\n",
      "Used Step: 196000 | Mean ep 100 return:  21.85 | Used Time: 2643.32\n",
      "Used Step: 200000 | Mean ep 100 return:  22.3 | Used Time: 2697.35\n",
      "Used Step: 204000 | Mean ep 100 return:  22.57 | Used Time: 2754.23\n",
      "Used Step: 208000 | Mean ep 100 return:  22.76 | Used Time: 2809.2\n",
      "Used Step: 212000 | Mean ep 100 return:  23.49 | Used Time: 2865.64\n",
      "Used Step: 216000 | Mean ep 100 return:  24.08 | Used Time: 2919.85\n",
      "Used Step: 220000 | Mean ep 100 return:  24.68 | Used Time: 2973.92\n",
      "Used Step: 224000 | Mean ep 100 return:  25.49 | Used Time: 3028.24\n",
      "Used Step: 228000 | Mean ep 100 return:  26.27 | Used Time: 3082.36\n",
      "Used Step: 232000 | Mean ep 100 return:  26.83 | Used Time: 3136.63\n",
      "Used Step: 236000 | Mean ep 100 return:  27.16 | Used Time: 3190.7\n",
      "Used Step: 240000 | Mean ep 100 return:  27.71 | Used Time: 3245.48\n",
      "Used Step: 244000 | Mean ep 100 return:  28.89 | Used Time: 3299.28\n",
      "Used Step: 248000 | Mean ep 100 return:  28.81 | Used Time: 3354.11\n",
      "Used Step: 252000 | Mean ep 100 return:  29.37 | Used Time: 3409.03\n",
      "Used Step: 256000 | Mean ep 100 return:  29.74 | Used Time: 3465.06\n",
      "Used Step: 260000 | Mean ep 100 return:  30.26 | Used Time: 3520.01\n",
      "Used Step: 264000 | Mean ep 100 return:  30.42 | Used Time: 3575.08\n",
      "Used Step: 268000 | Mean ep 100 return:  30.5 | Used Time: 3630.75\n",
      "Used Step: 272000 | Mean ep 100 return:  30.75 | Used Time: 3687.52\n",
      "Used Step: 276000 | Mean ep 100 return:  30.86 | Used Time: 3740.96\n",
      "Used Step: 280000 | Mean ep 100 return:  30.83 | Used Time: 3794.28\n",
      "Used Step: 284000 | Mean ep 100 return:  31.03 | Used Time: 3849.31\n",
      "Used Step: 288000 | Mean ep 100 return:  31.55 | Used Time: 3904.17\n",
      "Used Step: 292000 | Mean ep 100 return:  31.63 | Used Time: 3960.56\n",
      "Used Step: 296000 | Mean ep 100 return:  32.33 | Used Time: 4015.3\n",
      "Used Step: 300000 | Mean ep 100 return:  32.44 | Used Time: 4070.67\n",
      "Used Step: 304000 | Mean ep 100 return:  33.44 | Used Time: 4124.8\n",
      "Used Step: 308000 | Mean ep 100 return:  33.42 | Used Time: 4181.72\n",
      "Used Step: 312000 | Mean ep 100 return:  34.07 | Used Time: 4237.21\n",
      "Used Step: 316000 | Mean ep 100 return:  34.1 | Used Time: 4291.97\n",
      "Used Step: 320000 | Mean ep 100 return:  34.87 | Used Time: 4348.25\n",
      "Used Step: 324000 | Mean ep 100 return:  34.93 | Used Time: 4402.64\n",
      "Used Step: 328000 | Mean ep 100 return:  35.81 | Used Time: 4457.02\n",
      "Used Step: 332000 | Mean ep 100 return:  36.1 | Used Time: 4512.27\n",
      "Used Step: 336000 | Mean ep 100 return:  36.29 | Used Time: 4565.82\n",
      "Used Step: 340000 | Mean ep 100 return:  36.35 | Used Time: 4620.86\n",
      "Used Step: 344000 | Mean ep 100 return:  36.13 | Used Time: 4675.2\n",
      "Used Step: 348000 | Mean ep 100 return:  36.57 | Used Time: 4729.11\n",
      "Used Step: 352000 | Mean ep 100 return:  36.64 | Used Time: 4784.39\n",
      "Used Step: 356000 | Mean ep 100 return:  36.73 | Used Time: 4840.09\n",
      "Used Step: 360000 | Mean ep 100 return:  37.12 | Used Time: 4894.12\n",
      "Used Step: 364000 | Mean ep 100 return:  37.41 | Used Time: 4949.09\n",
      "Used Step: 368000 | Mean ep 100 return:  38.3 | Used Time: 5005.98\n",
      "Used Step: 372000 | Mean ep 100 return:  38.29 | Used Time: 5061.68\n",
      "Used Step: 376000 | Mean ep 100 return:  38.25 | Used Time: 5117.39\n",
      "Used Step: 380000 | Mean ep 100 return:  38.6 | Used Time: 5172.66\n",
      "Used Step: 384000 | Mean ep 100 return:  38.87 | Used Time: 5227.67\n",
      "Used Step: 388000 | Mean ep 100 return:  38.78 | Used Time: 5282.38\n",
      "Used Step: 392000 | Mean ep 100 return:  38.57 | Used Time: 5337.53\n",
      "Used Step: 396000 | Mean ep 100 return:  38.99 | Used Time: 5392.77\n",
      "Used Step: 400000 | Mean ep 100 return:  39.27 | Used Time: 5448.27\n",
      "Used Step: 404000 | Mean ep 100 return:  39.89 | Used Time: 5503.54\n",
      "Used Step: 408000 | Mean ep 100 return:  40.28 | Used Time: 5556.39\n",
      "Used Step: 412000 | Mean ep 100 return:  41.01 | Used Time: 5611.41\n",
      "Used Step: 416000 | Mean ep 100 return:  40.68 | Used Time: 5666.02\n",
      "Used Step: 420000 | Mean ep 100 return:  40.81 | Used Time: 5720.95\n",
      "Used Step: 424000 | Mean ep 100 return:  41.01 | Used Time: 5774.91\n",
      "Used Step: 428000 | Mean ep 100 return:  40.94 | Used Time: 5829.25\n",
      "Used Step: 432000 | Mean ep 100 return:  41.27 | Used Time: 5883.63\n",
      "Used Step: 436000 | Mean ep 100 return:  41.15 | Used Time: 5938.94\n",
      "Used Step: 440000 | Mean ep 100 return:  41.49 | Used Time: 5995.13\n",
      "Used Step: 444000 | Mean ep 100 return:  42.27 | Used Time: 6050.0\n",
      "Used Step: 448000 | Mean ep 100 return:  42.76 | Used Time: 6105.01\n",
      "Used Step: 452000 | Mean ep 100 return:  43.47 | Used Time: 6160.95\n",
      "Used Step: 456000 | Mean ep 100 return:  43.33 | Used Time: 6214.97\n",
      "Used Step: 460000 | Mean ep 100 return:  44.07 | Used Time: 6270.08\n",
      "Used Step: 464000 | Mean ep 100 return:  44.6 | Used Time: 6324.51\n",
      "Used Step: 468000 | Mean ep 100 return:  45.52 | Used Time: 6378.4\n",
      "Used Step: 472000 | Mean ep 100 return:  45.92 | Used Time: 6434.68\n",
      "Used Step: 476000 | Mean ep 100 return:  47.23 | Used Time: 6491.04\n",
      "Used Step: 480000 | Mean ep 100 return:  47.69 | Used Time: 6545.05\n",
      "Used Step: 484000 | Mean ep 100 return:  47.81 | Used Time: 6601.23\n",
      "Used Step: 488000 | Mean ep 100 return:  48.01 | Used Time: 6656.17\n",
      "Used Step: 492000 | Mean ep 100 return:  49.07 | Used Time: 6711.03\n",
      "Used Step: 496000 | Mean ep 100 return:  49.64 | Used Time: 6765.46\n",
      "Used Step: 500000 | Mean ep 100 return:  49.98 | Used Time: 6819.65\n",
      "Used Step: 504000 | Mean ep 100 return:  51.25 | Used Time: 6875.01\n",
      "Used Step: 508000 | Mean ep 100 return:  52.07 | Used Time: 6928.82\n",
      "Used Step: 512000 | Mean ep 100 return:  52.38 | Used Time: 6984.08\n",
      "Used Step: 516000 | Mean ep 100 return:  52.76 | Used Time: 7037.9\n",
      "Used Step: 520000 | Mean ep 100 return:  53.08 | Used Time: 7093.51\n",
      "Used Step: 524000 | Mean ep 100 return:  52.61 | Used Time: 7149.44\n",
      "Used Step: 528000 | Mean ep 100 return:  52.36 | Used Time: 7203.63\n",
      "Used Step: 532000 | Mean ep 100 return:  52.84 | Used Time: 7258.02\n",
      "Used Step: 536000 | Mean ep 100 return:  52.74 | Used Time: 7314.12\n",
      "Used Step: 540000 | Mean ep 100 return:  53.91 | Used Time: 7369.14\n",
      "Used Step: 544000 | Mean ep 100 return:  54.16 | Used Time: 7424.6\n",
      "Used Step: 548000 | Mean ep 100 return:  54.05 | Used Time: 7478.41\n",
      "Used Step: 552000 | Mean ep 100 return:  54.55 | Used Time: 7531.96\n",
      "Used Step: 556000 | Mean ep 100 return:  55.33 | Used Time: 7585.32\n",
      "Used Step: 560000 | Mean ep 100 return:  55.06 | Used Time: 7640.89\n",
      "Used Step: 564000 | Mean ep 100 return:  55.51 | Used Time: 7694.52\n",
      "Used Step: 568000 | Mean ep 100 return:  56.35 | Used Time: 7750.33\n",
      "Used Step: 572000 | Mean ep 100 return:  56.52 | Used Time: 7805.57\n",
      "Used Step: 576000 | Mean ep 100 return:  56.92 | Used Time: 7860.6\n",
      "Used Step: 580000 | Mean ep 100 return:  57.39 | Used Time: 7916.07\n",
      "Used Step: 584000 | Mean ep 100 return:  58.44 | Used Time: 7969.68\n",
      "Used Step: 588000 | Mean ep 100 return:  58.27 | Used Time: 8023.48\n",
      "Used Step: 592000 | Mean ep 100 return:  59.4 | Used Time: 8077.56\n",
      "Used Step: 596000 | Mean ep 100 return:  59.16 | Used Time: 8132.78\n",
      "Used Step: 600000 | Mean ep 100 return:  59.42 | Used Time: 8190.06\n",
      "Used Step: 604000 | Mean ep 100 return:  59.1 | Used Time: 8245.43\n",
      "Used Step: 608000 | Mean ep 100 return:  58.78 | Used Time: 8300.09\n",
      "Used Step: 612000 | Mean ep 100 return:  58.81 | Used Time: 8355.61\n",
      "Used Step: 616000 | Mean ep 100 return:  58.89 | Used Time: 8411.77\n",
      "Used Step: 620000 | Mean ep 100 return:  58.98 | Used Time: 8466.98\n",
      "Used Step: 624000 | Mean ep 100 return:  58.73 | Used Time: 8521.32\n",
      "Used Step: 628000 | Mean ep 100 return:  59.4 | Used Time: 8575.89\n",
      "Used Step: 632000 | Mean ep 100 return:  59.54 | Used Time: 8629.48\n",
      "Used Step: 636000 | Mean ep 100 return:  58.69 | Used Time: 8683.28\n",
      "Used Step: 640000 | Mean ep 100 return:  59.1 | Used Time: 8737.66\n",
      "Used Step: 644000 | Mean ep 100 return:  59.12 | Used Time: 8792.07\n",
      "Used Step: 648000 | Mean ep 100 return:  59.66 | Used Time: 8847.03\n",
      "Used Step: 652000 | Mean ep 100 return:  60.66 | Used Time: 8900.78\n",
      "Used Step: 656000 | Mean ep 100 return:  61.22 | Used Time: 8955.42\n",
      "Used Step: 660000 | Mean ep 100 return:  61.66 | Used Time: 9009.7\n",
      "Used Step: 664000 | Mean ep 100 return:  61.62 | Used Time: 9065.5\n",
      "Used Step: 668000 | Mean ep 100 return:  62.76 | Used Time: 9120.3\n",
      "Used Step: 672000 | Mean ep 100 return:  62.07 | Used Time: 9173.9\n",
      "Used Step: 676000 | Mean ep 100 return:  63.19 | Used Time: 9225.48\n",
      "Used Step: 680000 | Mean ep 100 return:  63.15 | Used Time: 9280.12\n",
      "Used Step: 684000 | Mean ep 100 return:  63.15 | Used Time: 9333.77\n",
      "Used Step: 688000 | Mean ep 100 return:  63.73 | Used Time: 9388.66\n",
      "Used Step: 692000 | Mean ep 100 return:  64.08 | Used Time: 9443.69\n",
      "Used Step: 696000 | Mean ep 100 return:  63.89 | Used Time: 9497.46\n",
      "Used Step: 700000 | Mean ep 100 return:  63.34 | Used Time: 9552.28\n",
      "Used Step: 704000 | Mean ep 100 return:  63.35 | Used Time: 9606.91\n",
      "Used Step: 708000 | Mean ep 100 return:  64.03 | Used Time: 9660.89\n",
      "Used Step: 712000 | Mean ep 100 return:  63.71 | Used Time: 9715.19\n",
      "Used Step: 716000 | Mean ep 100 return:  63.6 | Used Time: 9770.24\n",
      "Used Step: 720000 | Mean ep 100 return:  63.53 | Used Time: 9824.91\n",
      "Used Step: 724000 | Mean ep 100 return:  63.13 | Used Time: 9879.12\n",
      "Used Step: 728000 | Mean ep 100 return:  64.23 | Used Time: 9933.58\n",
      "Used Step: 732000 | Mean ep 100 return:  63.9 | Used Time: 9988.91\n",
      "Used Step: 736000 | Mean ep 100 return:  64.05 | Used Time: 10044.86\n",
      "Used Step: 740000 | Mean ep 100 return:  64.2 | Used Time: 10101.42\n",
      "Used Step: 744000 | Mean ep 100 return:  65.46 | Used Time: 10156.17\n",
      "Used Step: 748000 | Mean ep 100 return:  65.77 | Used Time: 10210.25\n",
      "Used Step: 752000 | Mean ep 100 return:  65.82 | Used Time: 10265.82\n",
      "Used Step: 756000 | Mean ep 100 return:  66.55 | Used Time: 10319.87\n",
      "Used Step: 760000 | Mean ep 100 return:  66.7 | Used Time: 10372.54\n",
      "Used Step: 764000 | Mean ep 100 return:  66.82 | Used Time: 10427.62\n",
      "Used Step: 768000 | Mean ep 100 return:  64.58 | Used Time: 10482.15\n",
      "Used Step: 772000 | Mean ep 100 return:  62.78 | Used Time: 10536.25\n",
      "Used Step: 776000 | Mean ep 100 return:  62.78 | Used Time: 10591.45\n",
      "Used Step: 780000 | Mean ep 100 return:  62.75 | Used Time: 10646.16\n",
      "Used Step: 784000 | Mean ep 100 return:  62.9 | Used Time: 10700.13\n",
      "Used Step: 788000 | Mean ep 100 return:  62.41 | Used Time: 10754.47\n",
      "Used Step: 792000 | Mean ep 100 return:  63.07 | Used Time: 10808.09\n",
      "Used Step: 796000 | Mean ep 100 return:  62.77 | Used Time: 10860.85\n",
      "Used Step: 800000 | Mean ep 100 return:  63.61 | Used Time: 10914.89\n",
      "Used Step: 804000 | Mean ep 100 return:  63.95 | Used Time: 10970.03\n",
      "Used Step: 808000 | Mean ep 100 return:  64.26 | Used Time: 11025.77\n",
      "Used Step: 812000 | Mean ep 100 return:  64.21 | Used Time: 11080.34\n",
      "Used Step: 816000 | Mean ep 100 return:  63.87 | Used Time: 11134.64\n",
      "Used Step: 820000 | Mean ep 100 return:  63.95 | Used Time: 11189.1\n",
      "Used Step: 824000 | Mean ep 100 return:  64.59 | Used Time: 11243.1\n",
      "Used Step: 828000 | Mean ep 100 return:  65.33 | Used Time: 11298.89\n",
      "Used Step: 832000 | Mean ep 100 return:  65.62 | Used Time: 11353.48\n",
      "Used Step: 836000 | Mean ep 100 return:  65.48 | Used Time: 11407.69\n",
      "Used Step: 840000 | Mean ep 100 return:  66.4 | Used Time: 11462.44\n",
      "Used Step: 844000 | Mean ep 100 return:  64.88 | Used Time: 11517.87\n",
      "Used Step: 848000 | Mean ep 100 return:  62.72 | Used Time: 11572.11\n",
      "Used Step: 852000 | Mean ep 100 return:  61.44 | Used Time: 11627.11\n",
      "Used Step: 856000 | Mean ep 100 return:  62.73 | Used Time: 11683.51\n",
      "Used Step: 860000 | Mean ep 100 return:  64.42 | Used Time: 11738.38\n",
      "Used Step: 864000 | Mean ep 100 return:  64.43 | Used Time: 11793.66\n",
      "Used Step: 868000 | Mean ep 100 return:  64.4 | Used Time: 11848.07\n",
      "Used Step: 872000 | Mean ep 100 return:  64.22 | Used Time: 11903.65\n",
      "Used Step: 876000 | Mean ep 100 return:  64.62 | Used Time: 11958.7\n",
      "Used Step: 880000 | Mean ep 100 return:  64.77 | Used Time: 12013.23\n",
      "Used Step: 884000 | Mean ep 100 return:  65.04 | Used Time: 12066.48\n",
      "Used Step: 888000 | Mean ep 100 return:  68.8 | Used Time: 12121.27\n",
      "Used Step: 892000 | Mean ep 100 return:  68.41 | Used Time: 12176.49\n",
      "Used Step: 896000 | Mean ep 100 return:  70.29 | Used Time: 12230.82\n",
      "Used Step: 900000 | Mean ep 100 return:  71.73 | Used Time: 12286.68\n",
      "Used Step: 904000 | Mean ep 100 return:  72.75 | Used Time: 12341.69\n",
      "Used Step: 908000 | Mean ep 100 return:  72.59 | Used Time: 12398.6\n",
      "Used Step: 912000 | Mean ep 100 return:  73.4 | Used Time: 12452.63\n",
      "Used Step: 916000 | Mean ep 100 return:  73.89 | Used Time: 12509.17\n",
      "Used Step: 920000 | Mean ep 100 return:  73.73 | Used Time: 12565.96\n",
      "Used Step: 924000 | Mean ep 100 return:  74.23 | Used Time: 12619.43\n",
      "Used Step: 928000 | Mean ep 100 return:  74.94 | Used Time: 12673.99\n",
      "Used Step: 932000 | Mean ep 100 return:  75.19 | Used Time: 12729.45\n",
      "Used Step: 936000 | Mean ep 100 return:  74.26 | Used Time: 12785.1\n",
      "Used Step: 940000 | Mean ep 100 return:  74.45 | Used Time: 12840.28\n",
      "Used Step: 944000 | Mean ep 100 return:  74.67 | Used Time: 12894.54\n",
      "Used Step: 948000 | Mean ep 100 return:  75.65 | Used Time: 12949.25\n",
      "Used Step: 952000 | Mean ep 100 return:  75.92 | Used Time: 13004.72\n",
      "Used Step: 956000 | Mean ep 100 return:  76.22 | Used Time: 13059.55\n",
      "Used Step: 960000 | Mean ep 100 return:  76.03 | Used Time: 13111.63\n",
      "Used Step: 964000 | Mean ep 100 return:  76.88 | Used Time: 13163.19\n",
      "Used Step: 968000 | Mean ep 100 return:  76.68 | Used Time: 13219.09\n",
      "Used Step: 972000 | Mean ep 100 return:  75.96 | Used Time: 13273.37\n",
      "Used Step: 976000 | Mean ep 100 return:  79.57 | Used Time: 13330.04\n",
      "Used Step: 980000 | Mean ep 100 return:  79.93 | Used Time: 13384.58\n",
      "Used Step: 984000 | Mean ep 100 return:  81.2 | Used Time: 13438.39\n",
      "Used Step: 988000 | Mean ep 100 return:  81.64 | Used Time: 13492.54\n",
      "Used Step: 992000 | Mean ep 100 return:  81.92 | Used Time: 13544.35\n",
      "Used Step: 996000 | Mean ep 100 return:  82.23 | Used Time: 13600.22\n",
      "Used Step: 1000000 | Mean ep 100 return:  83.97 | Used Time: 13656.78\n",
      "Used Step: 1004000 | Mean ep 100 return:  74.35 | Used Time: 13713.88\n",
      "Used Step: 1008000 | Mean ep 100 return:  69.81 | Used Time: 13769.82\n",
      "Used Step: 1012000 | Mean ep 100 return:  72.29 | Used Time: 13823.73\n",
      "Used Step: 1016000 | Mean ep 100 return:  72.92 | Used Time: 13877.16\n",
      "Used Step: 1020000 | Mean ep 100 return:  75.38 | Used Time: 13932.49\n",
      "Used Step: 1024000 | Mean ep 100 return:  75.44 | Used Time: 13990.29\n",
      "Used Step: 1028000 | Mean ep 100 return:  76.14 | Used Time: 14046.43\n",
      "Used Step: 1032000 | Mean ep 100 return:  76.58 | Used Time: 14102.26\n",
      "Used Step: 1036000 | Mean ep 100 return:  76.55 | Used Time: 14157.59\n",
      "Used Step: 1040000 | Mean ep 100 return:  76.94 | Used Time: 14212.99\n",
      "Used Step: 1044000 | Mean ep 100 return:  76.74 | Used Time: 14267.79\n",
      "Used Step: 1048000 | Mean ep 100 return:  75.97 | Used Time: 14322.47\n",
      "Used Step: 1052000 | Mean ep 100 return:  76.15 | Used Time: 14378.1\n",
      "Used Step: 1056000 | Mean ep 100 return:  76.38 | Used Time: 14432.72\n",
      "Used Step: 1060000 | Mean ep 100 return:  76.37 | Used Time: 14489.58\n",
      "Used Step: 1064000 | Mean ep 100 return:  76.63 | Used Time: 14546.13\n",
      "Used Step: 1068000 | Mean ep 100 return:  77.03 | Used Time: 14600.83\n",
      "Used Step: 1072000 | Mean ep 100 return:  76.31 | Used Time: 14655.43\n",
      "Used Step: 1076000 | Mean ep 100 return:  76.94 | Used Time: 14709.21\n",
      "Used Step: 1080000 | Mean ep 100 return:  78.37 | Used Time: 14763.14\n",
      "Used Step: 1084000 | Mean ep 100 return:  78.47 | Used Time: 14817.93\n",
      "Used Step: 1088000 | Mean ep 100 return:  78.41 | Used Time: 14874.55\n",
      "Used Step: 1092000 | Mean ep 100 return:  78.73 | Used Time: 14930.59\n",
      "Used Step: 1096000 | Mean ep 100 return:  78.19 | Used Time: 14986.72\n",
      "Used Step: 1100000 | Mean ep 100 return:  78.47 | Used Time: 15041.26\n",
      "Used Step: 1104000 | Mean ep 100 return:  77.62 | Used Time: 15094.79\n",
      "Used Step: 1108000 | Mean ep 100 return:  78.39 | Used Time: 15152.68\n",
      "Used Step: 1112000 | Mean ep 100 return:  79.07 | Used Time: 15208.95\n",
      "Used Step: 1116000 | Mean ep 100 return:  79.47 | Used Time: 15265.15\n",
      "Used Step: 1120000 | Mean ep 100 return:  80.37 | Used Time: 15321.3\n",
      "Used Step: 1124000 | Mean ep 100 return:  81.59 | Used Time: 15375.86\n",
      "Used Step: 1128000 | Mean ep 100 return:  79.96 | Used Time: 15431.39\n",
      "Used Step: 1132000 | Mean ep 100 return:  81.32 | Used Time: 15486.52\n",
      "Used Step: 1136000 | Mean ep 100 return:  84.25 | Used Time: 15542.1\n",
      "Used Step: 1140000 | Mean ep 100 return:  86.8 | Used Time: 15598.16\n",
      "Used Step: 1144000 | Mean ep 100 return:  88.71 | Used Time: 15652.84\n",
      "Used Step: 1148000 | Mean ep 100 return:  92.87 | Used Time: 15707.46\n",
      "Used Step: 1152000 | Mean ep 100 return:  94.02 | Used Time: 15762.56\n",
      "Used Step: 1156000 | Mean ep 100 return:  93.32 | Used Time: 15818.27\n",
      "Used Step: 1160000 | Mean ep 100 return:  91.38 | Used Time: 15872.17\n",
      "Used Step: 1164000 | Mean ep 100 return:  92.31 | Used Time: 15926.88\n",
      "Used Step: 1168000 | Mean ep 100 return:  92.58 | Used Time: 15983.7\n",
      "Used Step: 1172000 | Mean ep 100 return:  92.78 | Used Time: 16039.62\n",
      "Used Step: 1176000 | Mean ep 100 return:  92.51 | Used Time: 16094.6\n",
      "Used Step: 1180000 | Mean ep 100 return:  91.93 | Used Time: 16149.45\n",
      "Used Step: 1184000 | Mean ep 100 return:  92.22 | Used Time: 16204.58\n",
      "Used Step: 1188000 | Mean ep 100 return:  92.15 | Used Time: 16259.89\n",
      "Used Step: 1192000 | Mean ep 100 return:  91.63 | Used Time: 16315.19\n",
      "Used Step: 1196000 | Mean ep 100 return:  90.49 | Used Time: 16369.18\n",
      "Used Step: 1200000 | Mean ep 100 return:  91.76 | Used Time: 16424.07\n",
      "Used Step: 1204000 | Mean ep 100 return:  91.75 | Used Time: 16479.06\n",
      "Used Step: 1208000 | Mean ep 100 return:  93.83 | Used Time: 16536.16\n",
      "Used Step: 1212000 | Mean ep 100 return:  94.86 | Used Time: 16595.16\n",
      "Used Step: 1216000 | Mean ep 100 return:  94.2 | Used Time: 16651.03\n",
      "Used Step: 1220000 | Mean ep 100 return:  92.35 | Used Time: 16707.13\n",
      "Used Step: 1224000 | Mean ep 100 return:  92.43 | Used Time: 16759.87\n",
      "Used Step: 1228000 | Mean ep 100 return:  91.12 | Used Time: 16813.4\n",
      "Used Step: 1232000 | Mean ep 100 return:  90.78 | Used Time: 16867.26\n",
      "Used Step: 1236000 | Mean ep 100 return:  91.39 | Used Time: 16922.75\n",
      "Used Step: 1240000 | Mean ep 100 return:  91.5 | Used Time: 16978.9\n",
      "Used Step: 1244000 | Mean ep 100 return:  90.95 | Used Time: 17033.64\n",
      "Used Step: 1248000 | Mean ep 100 return:  90.85 | Used Time: 17090.17\n",
      "Used Step: 1252000 | Mean ep 100 return:  90.37 | Used Time: 17144.15\n",
      "Used Step: 1256000 | Mean ep 100 return:  90.88 | Used Time: 17200.37\n",
      "Used Step: 1260000 | Mean ep 100 return:  90.22 | Used Time: 17256.98\n",
      "Used Step: 1264000 | Mean ep 100 return:  89.25 | Used Time: 17311.68\n",
      "Used Step: 1268000 | Mean ep 100 return:  87.78 | Used Time: 17366.64\n",
      "Used Step: 1272000 | Mean ep 100 return:  86.8 | Used Time: 17422.19\n",
      "Used Step: 1276000 | Mean ep 100 return:  87.48 | Used Time: 17477.48\n",
      "Used Step: 1280000 | Mean ep 100 return:  86.46 | Used Time: 17532.45\n",
      "Used Step: 1284000 | Mean ep 100 return:  84.68 | Used Time: 17587.19\n",
      "Used Step: 1288000 | Mean ep 100 return:  84.61 | Used Time: 17640.49\n",
      "Used Step: 1292000 | Mean ep 100 return:  85.52 | Used Time: 17695.37\n",
      "Used Step: 1296000 | Mean ep 100 return:  85.37 | Used Time: 17750.73\n",
      "Used Step: 1300000 | Mean ep 100 return:  84.65 | Used Time: 17804.29\n",
      "Used Step: 1304000 | Mean ep 100 return:  85.9 | Used Time: 17859.32\n",
      "Used Step: 1308000 | Mean ep 100 return:  85.51 | Used Time: 17914.96\n",
      "Used Step: 1312000 | Mean ep 100 return:  87.45 | Used Time: 17970.02\n",
      "Used Step: 1316000 | Mean ep 100 return:  86.73 | Used Time: 18024.64\n",
      "Used Step: 1320000 | Mean ep 100 return:  87.04 | Used Time: 18078.99\n",
      "Used Step: 1324000 | Mean ep 100 return:  87.69 | Used Time: 18133.71\n",
      "Used Step: 1328000 | Mean ep 100 return:  90.15 | Used Time: 18184.93\n",
      "Used Step: 1332000 | Mean ep 100 return:  90.28 | Used Time: 18238.64\n",
      "Used Step: 1336000 | Mean ep 100 return:  91.19 | Used Time: 18294.32\n",
      "Used Step: 1340000 | Mean ep 100 return:  89.97 | Used Time: 18350.01\n",
      "Used Step: 1344000 | Mean ep 100 return:  87.88 | Used Time: 18403.78\n",
      "Used Step: 1348000 | Mean ep 100 return:  88.32 | Used Time: 18458.65\n",
      "Used Step: 1352000 | Mean ep 100 return:  87.47 | Used Time: 18513.48\n",
      "Used Step: 1356000 | Mean ep 100 return:  87.3 | Used Time: 18568.17\n",
      "Used Step: 1360000 | Mean ep 100 return:  88.42 | Used Time: 18622.74\n",
      "Used Step: 1364000 | Mean ep 100 return:  88.07 | Used Time: 18677.69\n",
      "Used Step: 1368000 | Mean ep 100 return:  90.72 | Used Time: 18731.88\n",
      "Used Step: 1372000 | Mean ep 100 return:  91.13 | Used Time: 18787.78\n",
      "Used Step: 1376000 | Mean ep 100 return:  91.96 | Used Time: 18843.0\n",
      "Used Step: 1380000 | Mean ep 100 return:  93.73 | Used Time: 18897.4\n",
      "Used Step: 1384000 | Mean ep 100 return:  97.8 | Used Time: 18952.88\n",
      "Used Step: 1388000 | Mean ep 100 return:  98.07 | Used Time: 19009.14\n",
      "Used Step: 1392000 | Mean ep 100 return:  98.54 | Used Time: 19066.25\n",
      "Used Step: 1396000 | Mean ep 100 return:  99.22 | Used Time: 19121.53\n",
      "Used Step: 1400000 | Mean ep 100 return:  99.29 | Used Time: 19174.8\n",
      "Used Step: 1404000 | Mean ep 100 return:  98.8 | Used Time: 19230.78\n",
      "Used Step: 1408000 | Mean ep 100 return:  97.79 | Used Time: 19285.88\n",
      "Used Step: 1412000 | Mean ep 100 return:  101.69 | Used Time: 19341.08\n",
      "Used Step: 1416000 | Mean ep 100 return:  104.21 | Used Time: 19395.57\n",
      "Used Step: 1420000 | Mean ep 100 return:  107.13 | Used Time: 19451.74\n",
      "Used Step: 1424000 | Mean ep 100 return:  109.18 | Used Time: 19506.97\n",
      "Used Step: 1428000 | Mean ep 100 return:  109.03 | Used Time: 19563.08\n",
      "Used Step: 1432000 | Mean ep 100 return:  109.47 | Used Time: 19617.12\n",
      "Used Step: 1436000 | Mean ep 100 return:  110.24 | Used Time: 19671.26\n",
      "Used Step: 1440000 | Mean ep 100 return:  110.47 | Used Time: 19726.93\n",
      "Used Step: 1444000 | Mean ep 100 return:  110.5 | Used Time: 19782.85\n",
      "Used Step: 1448000 | Mean ep 100 return:  112.09 | Used Time: 19837.91\n",
      "Used Step: 1452000 | Mean ep 100 return:  110.08 | Used Time: 19893.41\n",
      "Used Step: 1456000 | Mean ep 100 return:  108.76 | Used Time: 19948.14\n",
      "Used Step: 1460000 | Mean ep 100 return:  112.02 | Used Time: 20004.59\n",
      "Used Step: 1464000 | Mean ep 100 return:  111.55 | Used Time: 20059.6\n",
      "Used Step: 1468000 | Mean ep 100 return:  114.81 | Used Time: 20114.41\n",
      "Used Step: 1472000 | Mean ep 100 return:  114.06 | Used Time: 20168.83\n",
      "Used Step: 1476000 | Mean ep 100 return:  114.03 | Used Time: 20223.0\n",
      "Used Step: 1480000 | Mean ep 100 return:  115.9 | Used Time: 20277.54\n",
      "Used Step: 1484000 | Mean ep 100 return:  115.46 | Used Time: 20333.51\n",
      "Used Step: 1488000 | Mean ep 100 return:  117.62 | Used Time: 20387.86\n",
      "Used Step: 1492000 | Mean ep 100 return:  118.92 | Used Time: 20445.16\n",
      "Used Step: 1496000 | Mean ep 100 return:  121.27 | Used Time: 20502.46\n",
      "Used Step: 1500000 | Mean ep 100 return:  123.53 | Used Time: 20557.92\n",
      "Used Step: 1504000 | Mean ep 100 return:  123.78 | Used Time: 20611.94\n",
      "Used Step: 1508000 | Mean ep 100 return:  123.21 | Used Time: 20666.58\n",
      "Used Step: 1512000 | Mean ep 100 return:  125.54 | Used Time: 20721.6\n",
      "Used Step: 1516000 | Mean ep 100 return:  126.38 | Used Time: 20778.84\n",
      "Used Step: 1520000 | Mean ep 100 return:  127.47 | Used Time: 20833.14\n",
      "Used Step: 1524000 | Mean ep 100 return:  127.68 | Used Time: 20888.3\n",
      "Used Step: 1528000 | Mean ep 100 return:  124.88 | Used Time: 20943.17\n",
      "Used Step: 1532000 | Mean ep 100 return:  125.2 | Used Time: 20998.24\n",
      "Used Step: 1536000 | Mean ep 100 return:  124.23 | Used Time: 21053.1\n",
      "Used Step: 1540000 | Mean ep 100 return:  126.2 | Used Time: 21109.67\n",
      "Used Step: 1544000 | Mean ep 100 return:  128.19 | Used Time: 21165.26\n",
      "Used Step: 1548000 | Mean ep 100 return:  128.41 | Used Time: 21222.73\n",
      "Used Step: 1552000 | Mean ep 100 return:  128.45 | Used Time: 21277.92\n",
      "Used Step: 1556000 | Mean ep 100 return:  129.27 | Used Time: 21332.2\n",
      "Used Step: 1560000 | Mean ep 100 return:  128.51 | Used Time: 21387.04\n",
      "Used Step: 1564000 | Mean ep 100 return:  129.68 | Used Time: 21442.67\n",
      "Used Step: 1568000 | Mean ep 100 return:  129.15 | Used Time: 21497.02\n",
      "Used Step: 1572000 | Mean ep 100 return:  127.52 | Used Time: 21552.49\n",
      "Used Step: 1576000 | Mean ep 100 return:  128.51 | Used Time: 21608.73\n",
      "Used Step: 1580000 | Mean ep 100 return:  132.51 | Used Time: 21663.95\n",
      "Used Step: 1584000 | Mean ep 100 return:  131.71 | Used Time: 21718.56\n",
      "Used Step: 1588000 | Mean ep 100 return:  131.13 | Used Time: 21774.34\n",
      "Used Step: 1592000 | Mean ep 100 return:  132.49 | Used Time: 21828.81\n",
      "Used Step: 1596000 | Mean ep 100 return:  132.97 | Used Time: 21885.32\n",
      "Used Step: 1600000 | Mean ep 100 return:  134.41 | Used Time: 21940.3\n",
      "Used Step: 1604000 | Mean ep 100 return:  137.85 | Used Time: 21994.61\n",
      "Used Step: 1608000 | Mean ep 100 return:  135.73 | Used Time: 22049.54\n",
      "Used Step: 1612000 | Mean ep 100 return:  136.07 | Used Time: 22104.96\n",
      "Used Step: 1616000 | Mean ep 100 return:  135.98 | Used Time: 22159.68\n",
      "Used Step: 1620000 | Mean ep 100 return:  135.56 | Used Time: 22215.62\n",
      "Used Step: 1624000 | Mean ep 100 return:  139.36 | Used Time: 22264.72\n",
      "Used Step: 1628000 | Mean ep 100 return:  140.44 | Used Time: 22314.37\n",
      "Used Step: 1632000 | Mean ep 100 return:  141.61 | Used Time: 22369.62\n",
      "Used Step: 1636000 | Mean ep 100 return:  144.15 | Used Time: 22423.95\n",
      "Used Step: 1640000 | Mean ep 100 return:  142.79 | Used Time: 22478.83\n",
      "Used Step: 1644000 | Mean ep 100 return:  143.0 | Used Time: 22534.57\n",
      "Used Step: 1648000 | Mean ep 100 return:  144.93 | Used Time: 22587.63\n",
      "Used Step: 1652000 | Mean ep 100 return:  144.7 | Used Time: 22642.62\n",
      "Used Step: 1656000 | Mean ep 100 return:  147.33 | Used Time: 22697.87\n",
      "Used Step: 1660000 | Mean ep 100 return:  149.5 | Used Time: 22753.53\n",
      "Used Step: 1664000 | Mean ep 100 return:  148.44 | Used Time: 22808.49\n",
      "Used Step: 1668000 | Mean ep 100 return:  146.89 | Used Time: 22863.89\n",
      "Used Step: 1672000 | Mean ep 100 return:  149.45 | Used Time: 22918.62\n",
      "Used Step: 1676000 | Mean ep 100 return:  150.34 | Used Time: 22972.59\n",
      "Used Step: 1680000 | Mean ep 100 return:  150.5 | Used Time: 23027.25\n",
      "Used Step: 1684000 | Mean ep 100 return:  151.89 | Used Time: 23082.19\n",
      "Used Step: 1688000 | Mean ep 100 return:  149.65 | Used Time: 23138.21\n",
      "Used Step: 1692000 | Mean ep 100 return:  147.74 | Used Time: 23192.66\n",
      "Used Step: 1696000 | Mean ep 100 return:  148.12 | Used Time: 23247.12\n",
      "Used Step: 1700000 | Mean ep 100 return:  149.69 | Used Time: 23301.16\n",
      "Used Step: 1704000 | Mean ep 100 return:  151.96 | Used Time: 23356.02\n",
      "Used Step: 1708000 | Mean ep 100 return:  152.61 | Used Time: 23413.76\n",
      "Used Step: 1712000 | Mean ep 100 return:  152.36 | Used Time: 23468.16\n",
      "Used Step: 1716000 | Mean ep 100 return:  155.45 | Used Time: 23523.32\n",
      "Used Step: 1720000 | Mean ep 100 return:  156.69 | Used Time: 23578.04\n",
      "Used Step: 1724000 | Mean ep 100 return:  158.18 | Used Time: 23633.08\n",
      "Used Step: 1728000 | Mean ep 100 return:  158.68 | Used Time: 23686.61\n",
      "Used Step: 1732000 | Mean ep 100 return:  161.42 | Used Time: 23741.47\n",
      "Used Step: 1736000 | Mean ep 100 return:  162.15 | Used Time: 23797.48\n",
      "Used Step: 1740000 | Mean ep 100 return:  163.16 | Used Time: 23852.67\n",
      "Used Step: 1744000 | Mean ep 100 return:  161.46 | Used Time: 23907.03\n",
      "Used Step: 1748000 | Mean ep 100 return:  161.99 | Used Time: 23961.16\n",
      "Used Step: 1752000 | Mean ep 100 return:  162.03 | Used Time: 24016.43\n",
      "Used Step: 1756000 | Mean ep 100 return:  162.51 | Used Time: 24073.58\n",
      "Used Step: 1760000 | Mean ep 100 return:  163.4 | Used Time: 24129.29\n",
      "Used Step: 1764000 | Mean ep 100 return:  163.47 | Used Time: 24183.48\n",
      "Used Step: 1768000 | Mean ep 100 return:  165.37 | Used Time: 24237.87\n",
      "Used Step: 1772000 | Mean ep 100 return:  163.11 | Used Time: 24292.31\n",
      "Used Step: 1776000 | Mean ep 100 return:  163.11 | Used Time: 24345.84\n",
      "Used Step: 1780000 | Mean ep 100 return:  166.16 | Used Time: 24400.72\n",
      "Used Step: 1784000 | Mean ep 100 return:  165.8 | Used Time: 24455.4\n",
      "Used Step: 1788000 | Mean ep 100 return:  164.26 | Used Time: 24509.0\n",
      "Used Step: 1792000 | Mean ep 100 return:  165.92 | Used Time: 24563.81\n",
      "Used Step: 1796000 | Mean ep 100 return:  165.81 | Used Time: 24619.81\n",
      "Used Step: 1800000 | Mean ep 100 return:  168.3 | Used Time: 24677.75\n",
      "Used Step: 1804000 | Mean ep 100 return:  167.44 | Used Time: 24732.26\n",
      "Used Step: 1808000 | Mean ep 100 return:  168.88 | Used Time: 24787.25\n",
      "Used Step: 1812000 | Mean ep 100 return:  167.98 | Used Time: 24842.81\n",
      "Used Step: 1816000 | Mean ep 100 return:  166.66 | Used Time: 24897.81\n",
      "Used Step: 1820000 | Mean ep 100 return:  170.39 | Used Time: 24952.06\n",
      "Used Step: 1824000 | Mean ep 100 return:  171.25 | Used Time: 25007.78\n",
      "Used Step: 1828000 | Mean ep 100 return:  173.26 | Used Time: 25062.84\n",
      "Used Step: 1832000 | Mean ep 100 return:  171.36 | Used Time: 25118.38\n",
      "Used Step: 1836000 | Mean ep 100 return:  171.77 | Used Time: 25172.64\n",
      "Used Step: 1840000 | Mean ep 100 return:  175.13 | Used Time: 25228.82\n",
      "Used Step: 1844000 | Mean ep 100 return:  177.06 | Used Time: 25283.68\n",
      "Used Step: 1848000 | Mean ep 100 return:  177.43 | Used Time: 25338.8\n",
      "Used Step: 1852000 | Mean ep 100 return:  179.5 | Used Time: 25394.79\n",
      "Used Step: 1856000 | Mean ep 100 return:  178.36 | Used Time: 25449.74\n",
      "Used Step: 1860000 | Mean ep 100 return:  177.52 | Used Time: 25503.61\n",
      "Used Step: 1864000 | Mean ep 100 return:  180.22 | Used Time: 25558.26\n",
      "Used Step: 1868000 | Mean ep 100 return:  182.04 | Used Time: 25612.22\n",
      "Used Step: 1872000 | Mean ep 100 return:  182.73 | Used Time: 25666.93\n",
      "Used Step: 1876000 | Mean ep 100 return:  179.95 | Used Time: 25720.78\n",
      "Used Step: 1880000 | Mean ep 100 return:  177.46 | Used Time: 25776.86\n",
      "Used Step: 1884000 | Mean ep 100 return:  176.95 | Used Time: 25831.87\n",
      "Used Step: 1888000 | Mean ep 100 return:  174.97 | Used Time: 25885.49\n",
      "Used Step: 1892000 | Mean ep 100 return:  175.76 | Used Time: 25941.28\n",
      "Used Step: 1896000 | Mean ep 100 return:  177.37 | Used Time: 25995.45\n",
      "Used Step: 1900000 | Mean ep 100 return:  179.76 | Used Time: 26049.03\n",
      "Used Step: 1904000 | Mean ep 100 return:  180.84 | Used Time: 26103.4\n",
      "Used Step: 1908000 | Mean ep 100 return:  182.16 | Used Time: 26158.12\n"
     ]
    }
   ],
   "source": [
    "sac = SSAC()\n",
    "\n",
    "# model load with check\n",
    "if LOAD and os.path.isfile(PRED_PATH) and os.path.isfile(TARGET_PATH):\n",
    "    sac.load_model()\n",
    "    pkl_file = open(RESULT_PATH,'rb')\n",
    "    result = pickle.load(pkl_file)\n",
    "    pkl_file.close()\n",
    "    print('Load complete!')\n",
    "else:\n",
    "    result = []\n",
    "    print('Initialize results!')\n",
    "\n",
    "print('Collecting experience...')\n",
    "\n",
    "# episode step for accumulate reward \n",
    "epinfobuf = deque(maxlen=100)\n",
    "# check learning time\n",
    "start_time = time.time()\n",
    "\n",
    "# env reset\n",
    "s = np.array(env.reset())\n",
    "\n",
    "for step in tqdm_notebook(range(1, N_STEP//N_ENVS + 1)):\n",
    "    \n",
    "    a = sac.choose_action(s)\n",
    "    \n",
    "    # take action and get next state\n",
    "    s_, r, done, infos = env.step(a)\n",
    "    s_ = np.array(s_)\n",
    "    \n",
    "    # log arrange\n",
    "    for info in infos:\n",
    "        maybeepinfo = info.get('episode')\n",
    "        if maybeepinfo: epinfobuf.append(maybeepinfo)\n",
    "            \n",
    "    # store transition\n",
    "    sac.sil_store_transition(s,a,r,done)\n",
    "    for i in range(len(s_)):\n",
    "        sac.store_transition(s[i],a[i],r[i],s_[i], done[i])\n",
    "        \n",
    "    if (step >= LEARN_START) and (step % LEARN_FREQ == 0):\n",
    "        sac.learn()\n",
    "        \n",
    "    s = s_\n",
    "            \n",
    "    if step % LOG_FREQ == 0:\n",
    "        # print log and save\n",
    "        # check time interval\n",
    "        time_interval = round(time.time() - start_time, 2)\n",
    "        # calc mean return\n",
    "        mean_100_ep_return = round(np.mean([epinfo['r'] for epinfo in epinfobuf]),2)\n",
    "        result.append(mean_100_ep_return)\n",
    "        # print epi log\n",
    "        print('Used Step:',sac.memory_counter,\n",
    "              '| Mean ep 100 return: ', mean_100_ep_return,\n",
    "              '| Used Time:',time_interval)\n",
    "        # save model\n",
    "        if SAVE:\n",
    "            sac.save_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 결과 시각화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(range(len(result)), result)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import animation\n",
    "\n",
    "def display_frames_as_gif(frames):\n",
    "    patch = plt.imshow(frames[0])\n",
    "    plt.axis('off')\n",
    "    def animate(i):\n",
    "        patch.set_data(frames[i])\n",
    "        \n",
    "    anim = animation.FuncAnimation(plt.gcf(), animate, frames = len(frames), interval=5)\n",
    "    anim.save('./ssac_breakout_result.gif', writer='imagemagick', fps=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = wrap(gym.make('BreakoutNoFrameskip-v4'))\n",
    "s = np.array(env.reset())\n",
    "total_reward = 0\n",
    "frames = []\n",
    "done_stack = 0\n",
    "\n",
    "for t in range(10000):\n",
    "    # Render into buffer. \n",
    "    frames.append(env.render(mode = 'rgb_array'))\n",
    "    a = ssac.choose_action(np.expand_dims(s,axis=0))\n",
    "    # take action and get next state\n",
    "    s_, r, done, info = env.step(a)\n",
    "    s_ = np.array(s_)\n",
    "    total_reward += r\n",
    "    if done:\n",
    "        done_stack += 1\n",
    "        if done_stack == 5:\n",
    "            break\n",
    "    s = s_\n",
    "env.close()\n",
    "print('Total Reward : %.2f'%total_reward)\n",
    "display_frames_as_gif(frames)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](./ppo_pong_result.gif \"segment\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
