{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 필요한 모듈 설치 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from replay_memory import ReplayBuffer, PrioritizedReplayBuffer\n",
    "\n",
    "import random\n",
    "import os\n",
    "import pickle\n",
    "import time\n",
    "from collections import deque\n",
    "from copy import deepcopy\n",
    "from tqdm import tqdm\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "sns.set()\n",
    "\n",
    "from wrappers import wrap, wrap_cover, SubprocVecEnv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 하이퍼 파라미터 정의하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sungyubkim/anaconda3/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.uint8'>. Please provide explicit dtype.\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n",
      "/home/sungyubkim/anaconda3/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n",
      "/home/sungyubkim/anaconda3/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.uint8'>. Please provide explicit dtype.\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "USE GPU: True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sungyubkim/anaconda3/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.uint8'>. Please provide explicit dtype.\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n",
      "/home/sungyubkim/anaconda3/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n",
      "/home/sungyubkim/anaconda3/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.uint8'>. Please provide explicit dtype.\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n",
      "/home/sungyubkim/anaconda3/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.uint8'>. Please provide explicit dtype.\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n",
      "/home/sungyubkim/anaconda3/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n",
      "/home/sungyubkim/anaconda3/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.uint8'>. Please provide explicit dtype.\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n",
      "/home/sungyubkim/anaconda3/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.uint8'>. Please provide explicit dtype.\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n",
      "/home/sungyubkim/anaconda3/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n",
      "/home/sungyubkim/anaconda3/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.uint8'>. Please provide explicit dtype.\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n",
      "/home/sungyubkim/anaconda3/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: <class 'wrappers.FrameStack'> doesn't implement 'reset' method, but it implements deprecated '_reset' method.\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n",
      "/home/sungyubkim/anaconda3/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: <class 'wrappers.FrameStack'> doesn't implement 'reset' method, but it implements deprecated '_reset' method.\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n",
      "/home/sungyubkim/anaconda3/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: <class 'wrappers.FrameStack'> doesn't implement 'reset' method, but it implements deprecated '_reset' method.\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n",
      "/home/sungyubkim/anaconda3/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: <class 'wrappers.FrameStack'> doesn't implement 'reset' method, but it implements deprecated '_reset' method.\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    }
   ],
   "source": [
    "'''DQN settings'''\n",
    "# sequential images to define state\n",
    "STATE_LEN = 4\n",
    "# target policy sync interval\n",
    "TARGET_REPLACE_ITER = 1\n",
    "# simulator steps for start learning\n",
    "LEARN_START = int(1e+3)\n",
    "# (prioritized) experience replay memory size\n",
    "MEMORY_CAPACITY = int(1e+5)\n",
    "# simulator steps for learning interval\n",
    "LEARN_FREQ = 1\n",
    "# quantile numbers for QR-DQN\n",
    "N_QUANT = 50\n",
    "# quantiles\n",
    "QUANTS = np.linspace(0.0, 1.0, N_QUANT + 1)[1:]\n",
    "\n",
    "'''Environment Settings'''\n",
    "# number of environments for C51\n",
    "N_ENVS = 4\n",
    "# openai gym env name\n",
    "ENV_NAME = 'BreakoutNoFrameskip-v4'\n",
    "env = SubprocVecEnv([wrap_cover(ENV_NAME) for i in range(N_ENVS)])\n",
    "N_ACTIONS = env.action_space.n\n",
    "N_STATES = env.observation_space.shape\n",
    "# Total simulation step\n",
    "STEP_NUM = int(1e+7)\n",
    "# gamma for MDP\n",
    "GAMMA = 0.99\n",
    "# visualize for agent playing\n",
    "RENDERING = False\n",
    "\n",
    "'''Training settings'''\n",
    "# check GPU usage\n",
    "USE_GPU = torch.cuda.is_available()\n",
    "print('USE GPU: '+str(USE_GPU))\n",
    "# mini-batch size\n",
    "BATCH_SIZE = 32\n",
    "# learning rage\n",
    "LR = 1e-4\n",
    "# epsilon-greedy\n",
    "EPSILON = 1.0\n",
    "\n",
    "'''Save&Load Settings'''\n",
    "# check save/load\n",
    "SAVE = True\n",
    "LOAD = False\n",
    "# save frequency\n",
    "SAVE_FREQ = int(1e+3)\n",
    "# paths for predction net, target net, result log\n",
    "PRED_PATH = './data/model/qr_pred_net.pkl'\n",
    "TARGET_PATH = './data/model/qr_target_net.pkl'\n",
    "RESULT_PATH = './data/plots/result.pkl'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 네트워크 구조 정의하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ConvNet, self).__init__()\n",
    "        # nn.Sequential을 사용하면 다음과 같입 코드를 간결하게 바꿀 수 있습니다.\n",
    "        self.feature_extraction = nn.Sequential(\n",
    "            nn.Conv2d(STATE_LEN, 32, kernel_size=8, stride=4),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, kernel_size=4, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, stride=1),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.fc = nn.Linear(7 * 7 * 64, 512)\n",
    "        \n",
    "        # action value distribution\n",
    "        self.fc_q = nn.Linear(512, N_ACTIONS * N_QUANT) \n",
    "            \n",
    "        # 파라미터 값 초기화 코드는 다음과 같이 간결하게 바꿀 수 있습니다.\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.orthogonal_(m.weight, gain = np.sqrt(2))\n",
    "                nn.init.constant_(m.bias, 0.0)\n",
    "            elif isinstance(m, nn.Linear):\n",
    "                nn.init.kaiming_normal_(m.weight)\n",
    "                nn.init.constant_(m.bias, 0.0)\n",
    "            \n",
    "\n",
    "    def forward(self, x):\n",
    "        # x.size(0) : minibatch size\n",
    "        mb_size = x.size(0)\n",
    "        # x는 (m, 84, 84, 4)의 tensor\n",
    "        x = self.feature_extraction(x / 255.0)\n",
    "        # x.size(0) : mini-batch size\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = F.relu(self.fc(x))\n",
    "        \n",
    "        # note that output of QR-DQN is quantile values of value distribution\n",
    "        action_value = self.fc_q(x).view(mb_size, N_ACTIONS, N_QUANT)\n",
    "\n",
    "        return action_value\n",
    "\n",
    "    def save(self, PATH):\n",
    "        torch.save(self.state_dict(),PATH)\n",
    "\n",
    "    def load(self, PATH):\n",
    "        self.load_state_dict(torch.load(PATH))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## C51 정의하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN(object):\n",
    "    def __init__(self):\n",
    "        self.pred_net, self.target_net = ConvNet(), ConvNet()\n",
    "        # sync eval target\n",
    "        self.update_target(self.target_net, self.pred_net, 1.0)\n",
    "        # use gpu\n",
    "        if USE_GPU:\n",
    "            self.pred_net.cuda()\n",
    "            self.target_net.cuda()\n",
    "            \n",
    "        # simulator step conter\n",
    "        self.memory_counter = 0\n",
    "        # target network step counter\n",
    "        self.learn_step_counter = 0\n",
    "        \n",
    "        # ceate the replay buffer\n",
    "        self.replay_buffer = ReplayBuffer(MEMORY_CAPACITY)\n",
    "        \n",
    "        # define optimizer\n",
    "        self.optimizer = torch.optim.Adam(self.pred_net.parameters(), lr=LR)\n",
    "        \n",
    "    def update_target(self, target, pred, update_rate):\n",
    "        # update target network parameters using predcition network\n",
    "        for target_param, pred_param in zip(target.parameters(), pred.parameters()):\n",
    "            target_param.data.copy_((1.0 - update_rate) \\\n",
    "                                    * target_param.data + update_rate*pred_param.data)\n",
    "            \n",
    "    def save_model(self):\n",
    "        # save prediction network and target network\n",
    "        self.pred_net.save(PRED_PATH)\n",
    "        self.target_net.save(TARGET_PATH)\n",
    "\n",
    "    def load_model(self):\n",
    "        # load prediction network and target network\n",
    "        self.pred_net.load(PRED_PATH)\n",
    "        self.target_net.load(TARGET_PATH)\n",
    "\n",
    "    def choose_action(self, x, EPSILON):\n",
    "        x = torch.FloatTensor(x)\n",
    "        if USE_GPU:\n",
    "            x = x.cuda()\n",
    "\n",
    "        if np.random.uniform() >= EPSILON:\n",
    "            # greedy case\n",
    "            action_value = self.pred_net(x).mean(dim=2) # (N_ENVS, N_ACTIONS)\n",
    "            action = torch.argmax(action_value, dim=1).data.cpu().numpy()\n",
    "        else:\n",
    "            # random exploration case\n",
    "            action = np.random.randint(0, N_ACTIONS, (x.size(0)))\n",
    "        return action\n",
    "\n",
    "    def store_transition(self, s, a, r, s_, done):\n",
    "        self.memory_counter += 1\n",
    "        self.replay_buffer.add(s, a, r, s_, float(done))\n",
    "\n",
    "    def learn(self):\n",
    "        self.learn_step_counter += 1\n",
    "        # target parameter update\n",
    "        if self.learn_step_counter % TARGET_REPLACE_ITER == 0:\n",
    "            self.update_target(self.target_net, self.pred_net, 1e-2)\n",
    "    \n",
    "        b_s, b_a, b_r,b_s_, b_d = self.replay_buffer.sample(BATCH_SIZE)\n",
    "        b_w, b_idxes = np.ones_like(b_r), None\n",
    "            \n",
    "        b_s = torch.FloatTensor(b_s)\n",
    "        b_a = torch.LongTensor(b_a)\n",
    "        b_r = torch.FloatTensor(b_r)\n",
    "        b_s_ = torch.FloatTensor(b_s_)\n",
    "        b_d = torch.FloatTensor(b_d)\n",
    "\n",
    "        if USE_GPU:\n",
    "            b_s, b_a, b_r, b_s_, b_d = b_s.cuda(), b_a.cuda(), b_r.cuda(), b_s_.cuda(), b_d.cuda()\n",
    "\n",
    "        # action value distribution prediction\n",
    "        q_eval = self.pred_net(b_s) # (m, N_ACTIONS, N_QUANT)\n",
    "        mb_size = q_eval.size(0)\n",
    "        q_eval = torch.stack([q_eval[i].index_select(0, b_a[i]) for i in range(mb_size)]).squeeze(1) \n",
    "        # (m, N_QUANT)\n",
    "        q_eval = q_eval.unsqueeze(2) # (m, N_QUANT, 1)\n",
    "        # note that dim 1 is for present quantile, dim 2 si for next quantile\n",
    "        \n",
    "        # get next state value\n",
    "        q_next = self.target_net(b_s_).detach() # (m, N_ACTIONS, N_QUANT)\n",
    "        best_actions = q_next.mean(dim=2).argmax(dim=1) # (m)\n",
    "        q_next = torch.stack([q_next[i].index_select(0, best_actions[i]) for i in range(mb_size)]).squeeze(1)\n",
    "        # (m, N_QUANT)\n",
    "        q_target = b_r.unsqueeze(1) + GAMMA * (1. -b_d.unsqueeze(1)) * q_next \n",
    "        # (m, N_QUANT)\n",
    "        q_target = q_target.unsqueeze(1) # (m , 1, N_QUANT)\n",
    "\n",
    "        # quantile Huber loss\n",
    "        u = q_target.detach() - q_eval # (m, N_QUANT, N_QUANT)\n",
    "        tau = torch.FloatTensor(QUANTS).view(1, -1, 1) # (1, N_QUANT, 1)\n",
    "        # note that tau is for present quantile\n",
    "        if USE_GPU:\n",
    "            tau = tau.cuda()\n",
    "        weight = torch.abs(tau - u.le(0.).float()) # (m, N_QUANT, N_QUANT)\n",
    "        loss = F.smooth_l1_loss(q_eval, q_target.detach(), reduction='none')\n",
    "        # (m, N_QUANT, N_QUANT)\n",
    "        loss = torch.mean(weight * loss, dim=1).mean(dim=1)\n",
    "        \n",
    "        # calc importance weighted loss\n",
    "        b_w = torch.Tensor(b_w)\n",
    "        if USE_GPU:\n",
    "            b_w = b_w.cuda()\n",
    "        loss = torch.mean(b_w*loss)\n",
    "        \n",
    "        # backprop loss\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "#         torch.nn.utils.clip_grad_norm_(self.pred_net.parameters(),0.1)\n",
    "        self.optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 38/2500000 [00:00<1:51:10, 374.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialize results!\n",
      "Collecting experience...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 1010/2500000 [00:13<12:07:37, 57.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Used Step: 4000 EPS:  0.1 | Mean ep 100 return:  1.6 | Used Time: 13.68\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 2010/2500000 [00:31<12:09:01, 57.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Used Step: 8000 EPS:  0.09 | Mean ep 100 return:  1.42 | Used Time: 30.96\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 3011/2500000 [00:48<11:55:41, 58.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Used Step: 12000 EPS:  0.08 | Mean ep 100 return:  1.48 | Used Time: 48.55\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 4010/2500000 [01:06<11:55:24, 58.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Used Step: 16000 EPS:  0.07 | Mean ep 100 return:  1.41 | Used Time: 66.09\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 5008/2500000 [01:23<12:42:23, 54.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Used Step: 20000 EPS:  0.06 | Mean ep 100 return:  1.44 | Used Time: 83.59\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 6010/2500000 [01:41<12:12:19, 56.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Used Step: 24000 EPS:  0.05 | Mean ep 100 return:  1.67 | Used Time: 100.92\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 7007/2500000 [01:58<12:16:37, 56.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Used Step: 28000 EPS:  0.04 | Mean ep 100 return:  1.93 | Used Time: 118.15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 8007/2500000 [02:15<12:45:21, 54.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Used Step: 32000 EPS:  0.03 | Mean ep 100 return:  2.05 | Used Time: 135.52\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 9006/2500000 [02:33<12:27:48, 55.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Used Step: 36000 EPS:  0.02 | Mean ep 100 return:  2.56 | Used Time: 152.92\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 10010/2500000 [02:50<11:51:47, 58.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Used Step: 40000 EPS:  0.01 | Mean ep 100 return:  2.77 | Used Time: 170.18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 11008/2500000 [03:07<12:20:21, 56.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Used Step: 44000 EPS:  0.01 | Mean ep 100 return:  2.95 | Used Time: 187.45\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 12010/2500000 [03:24<11:49:13, 58.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Used Step: 48000 EPS:  0.01 | Mean ep 100 return:  3.32 | Used Time: 204.04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 13006/2500000 [03:41<12:17:47, 56.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Used Step: 52000 EPS:  0.01 | Mean ep 100 return:  3.73 | Used Time: 221.09\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 14010/2500000 [03:58<11:26:14, 60.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Used Step: 56000 EPS:  0.01 | Mean ep 100 return:  4.17 | Used Time: 238.14\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 15007/2500000 [04:15<11:31:22, 59.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Used Step: 60000 EPS:  0.01 | Mean ep 100 return:  4.48 | Used Time: 255.22\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 16005/2500000 [04:32<11:52:32, 58.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Used Step: 64000 EPS:  0.01 | Mean ep 100 return:  4.79 | Used Time: 272.25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 17007/2500000 [04:49<11:37:42, 59.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Used Step: 68000 EPS:  0.01 | Mean ep 100 return:  4.92 | Used Time: 289.38\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 18011/2500000 [05:06<12:12:11, 56.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Used Step: 72000 EPS:  0.01 | Mean ep 100 return:  4.84 | Used Time: 306.6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 19009/2500000 [05:24<11:51:39, 58.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Used Step: 76000 EPS:  0.01 | Mean ep 100 return:  4.12 | Used Time: 323.94\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 20009/2500000 [05:41<11:57:03, 57.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Used Step: 80000 EPS:  0.01 | Mean ep 100 return:  3.48 | Used Time: 341.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 21004/2500000 [05:58<12:08:51, 56.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Used Step: 84000 EPS:  0.01 | Mean ep 100 return:  3.01 | Used Time: 358.3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 22005/2500000 [06:15<11:58:15, 57.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Used Step: 88000 EPS:  0.01 | Mean ep 100 return:  2.13 | Used Time: 375.64\n"
     ]
    }
   ],
   "source": [
    "dqn = DQN()\n",
    "\n",
    "# model load with check\n",
    "if LOAD and os.path.isfile(PRED_PATH) and os.path.isfile(TARGET_PATH):\n",
    "    dqn.load_model()\n",
    "    pkl_file = open(RESULT_PATH,'rb')\n",
    "    result = pickle.load(pkl_file)\n",
    "    pkl_file.close()\n",
    "    print('Load complete!')\n",
    "else:\n",
    "    result = []\n",
    "    print('Initialize results!')\n",
    "\n",
    "print('Collecting experience...')\n",
    "\n",
    "# episode step for accumulate reward \n",
    "epinfobuf = deque(maxlen=100)\n",
    "# check learning time\n",
    "start_time = time.time()\n",
    "\n",
    "# env reset\n",
    "s = np.array(env.reset())\n",
    "\n",
    "for step in tqdm(range(1, STEP_NUM//N_ENVS+1)):\n",
    "    a = dqn.choose_action(s, EPSILON)\n",
    "\n",
    "    # take action and get next state\n",
    "    s_, r, done, infos = env.step(a)\n",
    "    # log arrange\n",
    "    for info in infos:\n",
    "        maybeepinfo = info.get('episode')\n",
    "        if maybeepinfo: epinfobuf.append(maybeepinfo)\n",
    "    s_ = np.array(s_)\n",
    "\n",
    "    # clip rewards for numerical stability\n",
    "    clip_r = np.sign(r)\n",
    "\n",
    "    # store the transition\n",
    "    for i in range(N_ENVS):\n",
    "        dqn.store_transition(s[i], a[i], clip_r[i], s_[i], done[i])\n",
    "\n",
    "    # annealing the epsilon(exploration strategy)\n",
    "    if step <= int(1e+3):\n",
    "        # linear annealing to 0.9 until million step\n",
    "        EPSILON -= 0.9/1e+3\n",
    "    elif step <= int(1e+4):\n",
    "        # linear annealing to 0.99 until the end\n",
    "        EPSILON -= 0.09/(1e+4 - 1e+3)\n",
    "\n",
    "    # if memory fill 50K and mod 4 = 0(for speed issue), learn pred net\n",
    "    if (LEARN_START <= dqn.memory_counter) and (dqn.memory_counter % LEARN_FREQ == 0):\n",
    "        dqn.learn()\n",
    "\n",
    "    # print log and save\n",
    "    if step % SAVE_FREQ == 0:\n",
    "        # check time interval\n",
    "        time_interval = round(time.time() - start_time, 2)\n",
    "        # calc mean return\n",
    "        mean_100_ep_return = round(np.mean([epinfo['r'] for epinfo in epinfobuf]),2)\n",
    "        result.append(mean_100_ep_return)\n",
    "        # print log\n",
    "        print('Used Step:',dqn.memory_counter,\n",
    "              'EPS: ', round(EPSILON, 3),\n",
    "              '| Mean ep 100 return: ', mean_100_ep_return,\n",
    "              '| Used Time:',time_interval)\n",
    "        # save model\n",
    "        dqn.save_model()\n",
    "        pkl_file = open(RESULT_PATH, 'wb')\n",
    "        pickle.dump(np.array(result), pkl_file)\n",
    "        pkl_file.close()\n",
    "\n",
    "    s = s_\n",
    "\n",
    "    if RENDERING:\n",
    "        env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 결과 시각화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(range(len(entire_ep_rs)), entire_ep_rs)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import animation\n",
    "\n",
    "def display_frames_as_gif(frames):\n",
    "    patch = plt.imshow(frames[0])\n",
    "    plt.axis('off')\n",
    "    def animate(i):\n",
    "        patch.set_data(frames[i])\n",
    "        \n",
    "    anim = animation.FuncAnimation(plt.gcf(), animate, frames = len(frames), interval=5)\n",
    "    anim.save('./qr_breakout_result.gif', writer='imagemagick', fps=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "env = wrap(gym.make('BreakoutNoFrameskip-v4'))\n",
    "s = np.array(env.reset())\n",
    "total_reward = 0\n",
    "frames = []\n",
    "\n",
    "for t in range(10000):\n",
    "    # Render into buffer. \n",
    "    frames.append(env.render(mode = 'rgb_array'))\n",
    "    a, v, l = dqn.choose_action(np.expand_dims(s,axis=0))\n",
    "    # take action and get next state\n",
    "    s_, r, done, info = env.step(a)\n",
    "    s_ = np.array(s_)\n",
    "    total_reward += r\n",
    "    if done:\n",
    "        break\n",
    "    s = s_\n",
    "env.close()\n",
    "print('Total Reward : %.2f'%total_reward)\n",
    "display_frames_as_gif(frames)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](./qr_breakout_result.gif \"segment\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
