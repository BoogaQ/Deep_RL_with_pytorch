{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 필요한 모듈 설치 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from replay_memory import ReplayBuffer, PrioritizedReplayBuffer\n",
    "\n",
    "import random\n",
    "import os\n",
    "import pickle\n",
    "import time\n",
    "from copy import deepcopy\n",
    "\n",
    "# 만약 opencv-python이 설치되어있지 않다면 다음을 통해서 설치해주세요.\n",
    "# pip install opencv-python\n",
    "# 만약 설치에 오류가 발생한다면 다음을 참고해주세요.\n",
    "# https://opencv-python-tutroals.readthedocs.io/en/latest/py_tutorials/py_setup/py_table_of_contents_setup/py_table_of_contents_setup.html#py-table-of-content-setup\n",
    "from wrappers import wrap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 하이퍼 파라미터 정의하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "USE GPU: True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sungyubkim/gym/gym/logger.py:30: UserWarning: \u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.uint8'>. Please provide explicit dtype.\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n",
      "/home/sungyubkim/gym/gym/logger.py:30: UserWarning: \u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    }
   ],
   "source": [
    "'''DQN settings'''\n",
    "# sequential images to define state\n",
    "STATE_LEN = 4\n",
    "# target policy synchronize interval\n",
    "TARGET_REPLACE_ITER = 10**4\n",
    "# (prioritized) experience replay memory size\n",
    "MEMORY_CAPACITY = 10**6\n",
    "# check per\n",
    "is_per = True\n",
    "# alpha of PER\n",
    "PER_ALPHA = 0.6\n",
    "PER_BETA = 0.4\n",
    "PER_EPSILON = 1e-6\n",
    "# Double DQN\n",
    "DOUBLE = False\n",
    "# Dueling architecture\n",
    "DUEL = False\n",
    "\n",
    "'''Environment Settings'''\n",
    "# openai gym env name\n",
    "ENV_NAME = 'PongNoFrameskip-v4'\n",
    "env = wrap(gym.make(ENV_NAME))\n",
    "N_ACTIONS = env.action_space.n\n",
    "N_STATES = env.observation_space.shape\n",
    "# Total simulation step\n",
    "STEP_NUM = 2*10**7\n",
    "# gamma for MDP\n",
    "GAMMA = 0.99\n",
    "# visualize for agent playing\n",
    "RENDERING = False\n",
    "\n",
    "'''Training settings'''\n",
    "# check GPU usage\n",
    "USE_GPU = torch.cuda.is_available()\n",
    "print('USE GPU: '+str(USE_GPU))\n",
    "# mini-batch size\n",
    "BATCH_SIZE = 32\n",
    "# learning rage\n",
    "LR = 1e-4\n",
    "# epsilon-greedy\n",
    "EPSILON = 0.0\n",
    "\n",
    "'''Save&Load Settings'''\n",
    "# check save/load\n",
    "SAVE = True\n",
    "LOAD = False\n",
    "# save frequency\n",
    "SAVE_FREQ = 10**4\n",
    "# paths for predction net, target net, result log\n",
    "PRED_PATH = './data/model/pred_net.pkl'\n",
    "TARGET_PATH = './data/model/target_net.pkl'\n",
    "RESULT_PATH = './data/plots/result.pkl'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 네트워크 구조 정의하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ConvNet, self).__init__()\n",
    "        # nn.Sequential을 사용하면 다음과 같입 코드를 간결하게 바꿀 수 있습니다.\n",
    "        self.feature_extraction = nn.Sequential(\n",
    "            nn.Conv2d(STATE_LEN, 32, kernel_size=8, stride=4),\n",
    "            nn.LeakyReLU(negative_slope=0.01),\n",
    "            nn.Conv2d(32, 64, kernel_size=4, stride=2),\n",
    "            nn.LeakyReLU(negative_slope=0.01),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, stride=1),\n",
    "            nn.LeakyReLU(negative_slope=0.01),\n",
    "        )\n",
    "        self.fc = nn.Linear(7 * 7 * 64, 512)\n",
    "\n",
    "        if DUEL:\n",
    "            # advantage function/ state value function\n",
    "            self.fc_adv = nn.Linear(512, N_ACTIONS)\n",
    "            self.fc_val = nn.Linear(512, 1)\n",
    "        else:\n",
    "            # action value function\n",
    "            self.fc_q = nn.Linear(512, N_ACTIONS) \n",
    "            \n",
    "        # 파라미터 값 초기화 코드는 다음과 같이 간결하게 바꿀 수 있습니다.\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.orthogonal_(m.weight, gain = np.sqrt(2))\n",
    "                nn.init.constant_(m.bias, 0.0)\n",
    "            elif isinstance(m, nn.Linear):\n",
    "                nn.init.kaiming_normal_(m.weight)\n",
    "                nn.init.constant_(m.bias, 0.0)\n",
    "            \n",
    "\n",
    "    def forward(self, x):\n",
    "        # x는 (m, 84, 84, 4)의 tensor\n",
    "        x = self.feature_extraction(x / 255.0)\n",
    "        # x.size(0) : mini-batch size\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = F.leaky_relu(self.fc(x), negative_slope=0.01)\n",
    "        \n",
    "        if DUEL:\n",
    "            adv = self.fc_adv(x)\n",
    "            val = self.fc_val(x)\n",
    "            action_value = val + adv - adv.mean(1).unsqueeze(1)\n",
    "        else:\n",
    "            action_value = self.fc_q(x)\n",
    "\n",
    "        return action_value\n",
    "\n",
    "    def save(self, PATH):\n",
    "        torch.save(self.state_dict(),PATH)\n",
    "\n",
    "    def load(self, PATH):\n",
    "        self.load_state_dict(torch.load(PATH))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DQN 정의하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN(object):\n",
    "    def __init__(self):\n",
    "        self.pred_net, self.target_net = ConvNet(), ConvNet()\n",
    "        # sync eval target\n",
    "        self.update_target(self.target_net, self.pred_net, 1.0)\n",
    "        # use gpu\n",
    "        if USE_GPU:\n",
    "            self.pred_net.cuda()\n",
    "            self.target_net.cuda()\n",
    "            \n",
    "        # simulator step conter\n",
    "        self.memory_counter = 0\n",
    "        # target network step counter\n",
    "        self.learn_step_counter = 0\n",
    "        \n",
    "        # ceate the replay buffer\n",
    "        if is_per:\n",
    "            self.replay_buffer = PrioritizedReplayBuffer(MEMORY_CAPACITY, alpha=PER_ALPHA)\n",
    "        else:\n",
    "            self.replay_buffer = ReplayBuffer(MEMORY_CAPACITY)\n",
    "        \n",
    "        # define optimizer\n",
    "        self.optimizer = torch.optim.Adam(self.pred_net.parameters(), lr=LR)\n",
    "        \n",
    "    def update_target(self, target, pred, update_rate):\n",
    "        # update target network parameters using predcition network\n",
    "        for target_param, pred_param in zip(target.parameters(), pred.parameters()):\n",
    "            target_param.data.copy_((1.0 - update_rate) \\\n",
    "                                    * target_param.data + update_rate*pred_param.data)\n",
    "            \n",
    "    def save_model(self):\n",
    "        # save prediction network and target network\n",
    "        self.pred_net.save(PRED_PATH)\n",
    "        self.target_net.save(TARGET_PATH)\n",
    "\n",
    "    def load_model(self):\n",
    "        # load prediction network and target network\n",
    "        self.pred_net.load(PRED_PATH)\n",
    "        self.target_net.load(TARGET_PATH)\n",
    "\n",
    "    def choose_action(self, x, EPSILON):\n",
    "        x = torch.FloatTensor(x)\n",
    "        if USE_GPU:\n",
    "            x = x.cuda()\n",
    "\n",
    "        if np.random.uniform() < EPSILON:\n",
    "            # greedy case\n",
    "            action_value = self.pred_net(x.unsqueeze(0))\n",
    "            action = torch.argmax(action_value).data.cpu().numpy()\n",
    "        else:\n",
    "            # random exploration case\n",
    "            action = np.random.randint(0, N_ACTIONS)\n",
    "        return action\n",
    "\n",
    "    def store_transition(self, s, a, r, s_, done):\n",
    "        self.memory_counter += 1\n",
    "        self.replay_buffer.add(s, a, r, s_, float(done))\n",
    "\n",
    "    def learn(self, beta):\n",
    "        self.learn_step_counter += 1\n",
    "        # target parameter update\n",
    "        if self.learn_step_counter % TARGET_REPLACE_ITER == 0:\n",
    "            self.update_target(self.target_net, self.pred_net, 1.0)\n",
    "        \n",
    "        # data sample from experience replay\n",
    "        if is_per:\n",
    "            experience = self.replay_buffer.sample(BATCH_SIZE, beta=beta)\n",
    "            (b_state_memory, b_action_memory, b_reward_memory, \\\n",
    "             b_next_state_memory, b_done, b_weights, b_idxes) = experience\n",
    "        else:\n",
    "            b_state_memory, b_action_memory, b_reward_memory, \\\n",
    "            b_next_state_memory, b_done = self.replay_buffer.sample(BATCH_SIZE)\n",
    "            b_weights, b_idxes = np.ones_like(b_reward_memory), None\n",
    "            \n",
    "        b_s = torch.FloatTensor(b_state_memory)\n",
    "        b_a = torch.LongTensor(b_action_memory)\n",
    "        b_r = torch.FloatTensor(b_reward_memory)\n",
    "        b_s_ = torch.FloatTensor(b_next_state_memory)\n",
    "        b_d = torch.FloatTensor(b_done)\n",
    "\n",
    "        if USE_GPU:\n",
    "            b_s, b_a, b_r, b_s_, b_d = b_s.cuda(), b_a.cuda(), b_r.cuda(), b_s_.cuda(), b_d.cuda()\n",
    "\n",
    "        # action value prediction\n",
    "        q_eval = self.pred_net(b_s).gather(1, b_a.unsqueeze(1)).view(-1)\n",
    "        # shape : (m, 1)\n",
    "\n",
    "        if DOUBLE:\n",
    "            # get best actions of next state\n",
    "            _ , best_actions = self.pred_net(b_s_).detach().max(1)\n",
    "            # get next state value\n",
    "            q_next = self.target_net(b_s_).detach()\n",
    "            # get target value\n",
    "            q_target = b_r + GAMMA *(1.-b_d)* q_next.gather(1, best_actions.unsqueeze(1)).squeeze(1)\n",
    "            # shape (m, 1)\n",
    "        else:\n",
    "            # get next state value\n",
    "            q_next = self.target_net(b_s_).detach()\n",
    "            # get target value\n",
    "            q_target = b_r + GAMMA *(1.-b_d)* q_next.max(1)[0]\n",
    "            # shape (m, 1)\n",
    "            \n",
    "        # calc huber loss, dont reduce for importance weight\n",
    "        loss = F.smooth_l1_loss(q_eval, q_target, reduce= False)\n",
    "        # calc importance weighted loss\n",
    "        loss = torch.mean(torch.Tensor(b_weights).cuda()*loss)\n",
    "        # get td error\n",
    "        td_error = (q_target - q_eval).data.cpu().numpy()\n",
    "        \n",
    "        # update importance weight\n",
    "        if is_per:\n",
    "            new_priorities = np.abs(td_error) + PER_EPSILON\n",
    "            self.replay_buffer.update_priorities(b_idxes, new_priorities)\n",
    "        \n",
    "        # backprop loss\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(self.pred_net.parameters(),10.)\n",
    "        self.optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialize results!\n",
      "Collecting experience...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sungyubkim/gym/gym/logger.py:30: UserWarning: \u001b[33mWARN: <class 'wrappers.FrameStack'> doesn't implement 'reset' method, but it implements deprecated '_reset' method.\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep:  10 | Mean ep 100 return:  -104.0 /Used Time: 9.7 /Used Step: 10000\n",
      "Save complete!\n",
      "Ep:  21 | Mean ep 100 return:  -101.0 /Used Time: 19.42 /Used Step: 20000\n",
      "Save complete!\n",
      "Ep:  32 | Mean ep 100 return:  -101.4 /Used Time: 29.18 /Used Step: 30000\n",
      "Save complete!\n",
      "Ep:  42 | Mean ep 100 return:  -101.14 /Used Time: 38.98 /Used Step: 40000\n",
      "Save complete!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sungyubkim/anaconda3/lib/python3.7/site-packages/torch/nn/_reduction.py:49: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.\n",
      "  warnings.warn(warning.format(ret))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep:  53 | Mean ep 100 return:  -100.89 /Used Time: 49.01 /Used Step: 50000\n",
      "Save complete!\n",
      "Ep:  64 | Mean ep 100 return:  -101.0 /Used Time: 81.78 /Used Step: 60000\n",
      "Save complete!\n",
      "Ep:  74 | Mean ep 100 return:  -100.92 /Used Time: 115.1 /Used Step: 70000\n",
      "Save complete!\n",
      "Ep:  86 | Mean ep 100 return:  -100.88 /Used Time: 148.52 /Used Step: 80000\n",
      "Save complete!\n",
      "Ep:  97 | Mean ep 100 return:  -100.72 /Used Time: 181.5 /Used Step: 90000\n",
      "Save complete!\n",
      "Ep:  108 | Mean ep 100 return:  -100.95 /Used Time: 213.89 /Used Step: 100000\n",
      "Save complete!\n",
      "Ep:  118 | Mean ep 100 return:  -100.86 /Used Time: 246.43 /Used Step: 110000\n",
      "Save complete!\n",
      "Ep:  129 | Mean ep 100 return:  -100.79 /Used Time: 279.31 /Used Step: 120000\n",
      "Save complete!\n",
      "Ep:  140 | Mean ep 100 return:  -101.04 /Used Time: 311.77 /Used Step: 130000\n",
      "Save complete!\n",
      "Ep:  151 | Mean ep 100 return:  -101.03 /Used Time: 344.32 /Used Step: 140000\n",
      "Save complete!\n",
      "Ep:  162 | Mean ep 100 return:  -101.06 /Used Time: 376.72 /Used Step: 150000\n",
      "Save complete!\n",
      "Ep:  172 | Mean ep 100 return:  -101.03 /Used Time: 409.12 /Used Step: 160000\n",
      "Save complete!\n",
      "Ep:  183 | Mean ep 100 return:  -100.97 /Used Time: 442.66 /Used Step: 170000\n",
      "Save complete!\n",
      "Ep:  193 | Mean ep 100 return:  -100.86 /Used Time: 476.74 /Used Step: 180000\n",
      "Save complete!\n",
      "Ep:  203 | Mean ep 100 return:  -100.9 /Used Time: 509.76 /Used Step: 190000\n",
      "Save complete!\n",
      "Ep:  214 | Mean ep 100 return:  -100.83 /Used Time: 543.66 /Used Step: 200000\n",
      "Save complete!\n",
      "Ep:  224 | Mean ep 100 return:  -100.79 /Used Time: 577.28 /Used Step: 210000\n",
      "Save complete!\n",
      "Ep:  235 | Mean ep 100 return:  -100.76 /Used Time: 610.07 /Used Step: 220000\n",
      "Save complete!\n",
      "Ep:  245 | Mean ep 100 return:  -100.9 /Used Time: 644.43 /Used Step: 230000\n",
      "Save complete!\n",
      "Ep:  256 | Mean ep 100 return:  -100.9 /Used Time: 678.13 /Used Step: 240000\n",
      "Save complete!\n"
     ]
    }
   ],
   "source": [
    "dqn = DQN()\n",
    "\n",
    "# model load with check\n",
    "if LOAD and os.path.isfile(EVAL_PATH) and os.path.isfile(TARGET_PATH):\n",
    "    dqn.load_model()\n",
    "    pkl_file = open(RESULT_PATH,'rb')\n",
    "    result = pickle.load(pkl_file)\n",
    "    pkl_file.close()\n",
    "    print('Load complete!')\n",
    "else:\n",
    "    result = []\n",
    "    print('Initialize results!')\n",
    "\n",
    "print('Collecting experience...')\n",
    "\n",
    "# episode step for accumulate reward (since we are using EpisodicLifeEnv of OpenAI gym wrapper)\n",
    "epi_step = 0\n",
    "# accumulate return of current episode\n",
    "entire_ep_r = 0.\n",
    "# log for accumulate returns\n",
    "entire_ep_rs = []\n",
    "# check learning time\n",
    "start_time = time.time()\n",
    "\n",
    "while dqn.memory_counter <= STEP_NUM:\n",
    "    # env reset\n",
    "    s = np.array(env.reset())\n",
    "    \n",
    "    # initialize one episode reward\n",
    "    ep_r = 0.\n",
    "\n",
    "    while True:\n",
    "        a = dqn.choose_action(s, EPSILON)\n",
    "\n",
    "        # take action and get next state\n",
    "        s_, r, done, info = env.step(a)\n",
    "        s_ = np.array(s_)\n",
    "        \n",
    "        # accumulate return\n",
    "        ep_r += r\n",
    "        # clip rewards for numerical stability\n",
    "        clip_r = np.sign(r)\n",
    "\n",
    "        # store the transition\n",
    "        dqn.store_transition(s, a, clip_r, s_, float(done))\n",
    "\n",
    "        # annealing the epsilon(exploration strategy), beta(per smoothing)\n",
    "        if dqn.memory_counter <= 1e+6:\n",
    "            # linear annealing to 0.9 until million step\n",
    "            EPSILON += 0.9/(1e+6)\n",
    "        elif dqn.memory_counter <= (2e+7):\n",
    "            # linear annealing to 0.99 until the end\n",
    "            EPSILON += 0.09/(2e+7 - 1e+6)\n",
    "            # linear annealing to 1 until the end\n",
    "            PER_BETA += (1.0 - PER_BETA) /(2e+7 - 1e+6)\n",
    "\n",
    "        # if memory fill 50K and mod 4 = 0(for speed issue), learn pred net\n",
    "        if (5e+4 <= dqn.memory_counter) and (dqn.memory_counter % 4 == 0):\n",
    "            dqn.learn(PER_BETA)\n",
    "            \n",
    "        # print log and save\n",
    "        if dqn.memory_counter % SAVE_FREQ == 0:\n",
    "            # check time interval\n",
    "            time_interval = round(time.time() - start_time, 2)\n",
    "            # calc mean return\n",
    "            mean_100_ep_return = round(np.mean(entire_ep_rs[-101:-1]),2)\n",
    "            result.append(mean_100_ep_return)\n",
    "            # print log\n",
    "            print('Ep: ',epi_step,\n",
    "                  '| Mean ep 100 return: ', mean_100_ep_return,\n",
    "                  '/Used Time:',time_interval,\n",
    "                  '/Used Step:',dqn.memory_counter)\n",
    "            # save model\n",
    "            dqn.save_model()\n",
    "            pkl_file = open(RESULT_PATH, 'wb')\n",
    "            pickle.dump(np.array(result), pkl_file)\n",
    "            pkl_file.close()\n",
    "            print('Save complete!')\n",
    "            \n",
    "        # if agent meets end-of-life, update return, acc return\n",
    "        if done:\n",
    "            entire_ep_r += ep_r\n",
    "            epi_step += 1\n",
    "            if epi_step % 5 == 0:\n",
    "                entire_ep_rs.append(entire_ep_r)\n",
    "                entire_ep_r = 0.\n",
    "            break\n",
    "\n",
    "        s = s_\n",
    "\n",
    "        if RENDERING:\n",
    "            env.render()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
