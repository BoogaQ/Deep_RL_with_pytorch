{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 필요한 모듈 설치 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from replay_memory import ReplayBuffer, PrioritizedReplayBuffer\n",
    "\n",
    "import random\n",
    "import os\n",
    "import pickle\n",
    "import time\n",
    "from copy import deepcopy\n",
    "\n",
    "# 만약 opencv-python이 설치되어있지 않다면 다음을 통해서 설치해주세요.\n",
    "# pip install opencv-python\n",
    "# 만약 설치에 오류가 발생한다면 다음을 참고해주세요.\n",
    "# https://opencv-python-tutroals.readthedocs.io/en/latest/py_tutorials/py_setup/py_table_of_contents_setup/py_table_of_contents_setup.html#py-table-of-content-setup\n",
    "from wrappers import wrap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 하이퍼 파라미터 정의하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "USE GPU: True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sungyubkim/gym/gym/logger.py:30: UserWarning: \u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.uint8'>. Please provide explicit dtype.\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n",
      "/home/sungyubkim/gym/gym/logger.py:30: UserWarning: \u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    }
   ],
   "source": [
    "'''DQN settings'''\n",
    "# sequential images to define state\n",
    "STATE_LEN = 4\n",
    "# target policy synchronize interval\n",
    "TARGET_REPLACE_ITER = 10**4\n",
    "# (prioritized) experience replay memory size\n",
    "MEMORY_CAPACITY = 10**6\n",
    "# check per\n",
    "is_per = True\n",
    "# alpha of PER\n",
    "PER_ALPHA = 0.6\n",
    "PER_BETA = 0.4\n",
    "PER_EPSILON = 1e-6\n",
    "# Double DQN\n",
    "DOUBLE = False\n",
    "# Dueling architecture\n",
    "DUEL = False\n",
    "\n",
    "'''Environment Settings'''\n",
    "# openai gym env name\n",
    "ENV_NAME = 'PongNoFrameskip-v4'\n",
    "env = wrap(gym.make(ENV_NAME))\n",
    "N_ACTIONS = env.action_space.n\n",
    "N_STATES = env.observation_space.shape\n",
    "# Total simulation step\n",
    "STEP_NUM = 2*10**7\n",
    "# gamma for MDP\n",
    "GAMMA = 0.99\n",
    "# visualize for agent playing\n",
    "RENDERING = False\n",
    "\n",
    "'''Training settings'''\n",
    "# check GPU usage\n",
    "USE_GPU = torch.cuda.is_available()\n",
    "print('USE GPU: '+str(USE_GPU))\n",
    "# mini-batch size\n",
    "BATCH_SIZE = 32\n",
    "# learning rage\n",
    "LR = 1e-4\n",
    "# epsilon-greedy\n",
    "EPSILON = 0.0\n",
    "\n",
    "'''Save&Load Settings'''\n",
    "# check save/load\n",
    "SAVE = True\n",
    "LOAD = False\n",
    "# save frequency\n",
    "SAVE_FREQ = 10**4\n",
    "# paths for predction net, target net, result log\n",
    "PRED_PATH = './data/model/pred_net.pkl'\n",
    "TARGET_PATH = './data/model/target_net.pkl'\n",
    "RESULT_PATH = './data/plots/result.pkl'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 네트워크 구조 정의하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ConvNet, self).__init__()\n",
    "        # nn.Sequential을 사용하면 다음과 같입 코드를 간결하게 바꿀 수 있습니다.\n",
    "        self.feature_extraction = nn.Sequential(\n",
    "            nn.Conv2d(STATE_LEN, 32, kernel_size=8, stride=4),\n",
    "            nn.LeakyReLU(negative_slope=0.01),\n",
    "            nn.Conv2d(32, 64, kernel_size=4, stride=2),\n",
    "            nn.LeakyReLU(negative_slope=0.01),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, stride=1),\n",
    "            nn.LeakyReLU(negative_slope=0.01),\n",
    "        )\n",
    "        self.fc = nn.Linear(7 * 7 * 64, 512)\n",
    "\n",
    "        if DUEL:\n",
    "            # advantage function/ state value function\n",
    "            self.fc_adv = nn.Linear(512, N_ACTIONS)\n",
    "            self.fc_val = nn.Linear(512, 1)\n",
    "        else:\n",
    "            # action value function\n",
    "            self.fc_q = nn.Linear(512, N_ACTIONS) \n",
    "            \n",
    "        # 파라미터 값 초기화 코드는 다음과 같이 간결하게 바꿀 수 있습니다.\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.orthogonal_(m.weight, gain = np.sqrt(2))\n",
    "                nn.init.constant_(m.bias, 0.0)\n",
    "            elif isinstance(m, nn.Linear):\n",
    "                nn.init.kaiming_normal_(m.weight)\n",
    "                nn.init.constant_(m.bias, 0.0)\n",
    "            \n",
    "\n",
    "    def forward(self, x):\n",
    "        # x는 (m, 84, 84, 4)의 tensor\n",
    "        x = self.feature_extraction(x / 255.0)\n",
    "        # x.size(0) : mini-batch size\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = F.leaky_relu(self.fc(x), negative_slope=0.01)\n",
    "        \n",
    "        if DUEL:\n",
    "            adv = self.fc_adv(x)\n",
    "            val = self.fc_val(x)\n",
    "            action_value = val + adv - adv.mean(1).unsqueeze(1)\n",
    "        else:\n",
    "            action_value = self.fc_q(x)\n",
    "\n",
    "        return action_value\n",
    "\n",
    "    def save(self, PATH):\n",
    "        torch.save(self.state_dict(),PATH)\n",
    "\n",
    "    def load(self, PATH):\n",
    "        self.load_state_dict(torch.load(PATH))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DQN 정의하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN(object):\n",
    "    def __init__(self):\n",
    "        self.pred_net, self.target_net = ConvNet(), ConvNet()\n",
    "        # sync eval target\n",
    "        self.update_target(self.target_net, self.pred_net, 1.0)\n",
    "        # use gpu\n",
    "        if USE_GPU:\n",
    "            self.pred_net.cuda()\n",
    "            self.target_net.cuda()\n",
    "            \n",
    "        # simulator step conter\n",
    "        self.memory_counter = 0\n",
    "        # target network step counter\n",
    "        self.learn_step_counter = 0\n",
    "        \n",
    "        # ceate the replay buffer\n",
    "        if is_per:\n",
    "            self.replay_buffer = PrioritizedReplayBuffer(MEMORY_CAPACITY, alpha=PER_ALPHA)\n",
    "        else:\n",
    "            self.replay_buffer = ReplayBuffer(MEMORY_CAPACITY)\n",
    "        \n",
    "        # define optimizer\n",
    "        self.optimizer = torch.optim.Adam(self.pred_net.parameters(), lr=LR)\n",
    "        \n",
    "    def update_target(self, target, pred, update_rate):\n",
    "        # update target network parameters using predcition network\n",
    "        for target_param, pred_param in zip(target.parameters(), pred.parameters()):\n",
    "            target_param.data.copy_((1.0 - update_rate) \\\n",
    "                                    * target_param.data + update_rate*pred_param.data)\n",
    "            \n",
    "    def save_model(self):\n",
    "        # save prediction network and target network\n",
    "        self.pred_net.save(PRED_PATH)\n",
    "        self.target_net.save(TARGET_PATH)\n",
    "\n",
    "    def load_model(self):\n",
    "        # load prediction network and target network\n",
    "        self.pred_net.load(PRED_PATH)\n",
    "        self.target_net.load(TARGET_PATH)\n",
    "\n",
    "    def choose_action(self, x, EPSILON):\n",
    "        x = torch.FloatTensor(x)\n",
    "        if USE_GPU:\n",
    "            x = x.cuda()\n",
    "\n",
    "        if np.random.uniform() < EPSILON:\n",
    "            # greedy case\n",
    "            action_value = self.pred_net(x.unsqueeze(0))\n",
    "            action = torch.argmax(action_value).data.cpu().numpy()\n",
    "        else:\n",
    "            # random exploration case\n",
    "            action = np.random.randint(0, N_ACTIONS)\n",
    "        return action\n",
    "\n",
    "    def store_transition(self, s, a, r, s_, done):\n",
    "        self.memory_counter += 1\n",
    "        self.replay_buffer.add(s, a, r, s_, float(done))\n",
    "\n",
    "    def learn(self, beta):\n",
    "        self.learn_step_counter += 1\n",
    "        # target parameter update\n",
    "        if self.learn_step_counter % TARGET_REPLACE_ITER == 0:\n",
    "            self.update_target(self.target_net, self.pred_net, 1.0)\n",
    "        \n",
    "        # data sample from experience replay\n",
    "        if is_per:\n",
    "            experience = self.replay_buffer.sample(BATCH_SIZE, beta=beta)\n",
    "            (b_state_memory, b_action_memory, b_reward_memory, \\\n",
    "             b_next_state_memory, b_done, b_weights, b_idxes) = experience\n",
    "        else:\n",
    "            b_state_memory, b_action_memory, b_reward_memory, \\\n",
    "            b_next_state_memory, b_done = self.replay_buffer.sample(BATCH_SIZE)\n",
    "            b_weights, b_idxes = np.ones_like(b_reward_memory), None\n",
    "            \n",
    "        b_s = torch.FloatTensor(b_state_memory)\n",
    "        b_a = torch.LongTensor(b_action_memory)\n",
    "        b_r = torch.FloatTensor(b_reward_memory)\n",
    "        b_s_ = torch.FloatTensor(b_next_state_memory)\n",
    "        b_d = torch.FloatTensor(b_done)\n",
    "\n",
    "        if USE_GPU:\n",
    "            b_s, b_a, b_r, b_s_, b_d = b_s.cuda(), b_a.cuda(), b_r.cuda(), b_s_.cuda(), b_d.cuda()\n",
    "\n",
    "        # action value prediction\n",
    "        q_eval = self.pred_net(b_s).gather(1, b_a.unsqueeze(1)).view(-1)\n",
    "        # shape : (m, 1)\n",
    "\n",
    "        if DOUBLE:\n",
    "            # get best actions of next state\n",
    "            _ , best_actions = self.pred_net(b_s_).detach().max(1)\n",
    "            # get next state value\n",
    "            q_next = self.target_net(b_s_).detach()\n",
    "            # get target value\n",
    "            q_target = b_r + GAMMA *(1.-b_d)* q_next.gather(1, best_actions.unsqueeze(1)).squeeze(1)\n",
    "            # shape (m, 1)\n",
    "        else:\n",
    "            # get next state value\n",
    "            q_next = self.target_net(b_s_).detach()\n",
    "            # get target value\n",
    "            q_target = b_r + GAMMA *(1.-b_d)* q_next.max(1)[0]\n",
    "            # shape (m, 1)\n",
    "            \n",
    "        # calc huber loss, dont reduce for importance weight\n",
    "        loss = F.smooth_l1_loss(q_eval, q_target, reduce= False)\n",
    "        # calc importance weighted loss\n",
    "        loss = torch.mean(torch.Tensor(b_weights).cuda()*loss)\n",
    "        # get td error\n",
    "        td_error = (q_target - q_eval).data.cpu().numpy()\n",
    "        \n",
    "        # update importance weight\n",
    "        if is_per:\n",
    "            new_priorities = np.abs(td_error) + PER_EPSILON\n",
    "            self.replay_buffer.update_priorities(b_idxes, new_priorities)\n",
    "        \n",
    "        # backprop loss\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(self.pred_net.parameters(),10.)\n",
    "        self.optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialize results!\n",
      "Collecting experience...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sungyubkim/gym/gym/logger.py:30: UserWarning: \u001b[33mWARN: <class 'wrappers.FrameStack'> doesn't implement 'reset' method, but it implements deprecated '_reset' method.\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep:  10 | Mean ep 100 return:  -104.0 /Used Time: 9.7 /Used Step: 10000\n",
      "Save complete!\n",
      "Ep:  21 | Mean ep 100 return:  -101.0 /Used Time: 19.42 /Used Step: 20000\n",
      "Save complete!\n",
      "Ep:  32 | Mean ep 100 return:  -101.4 /Used Time: 29.18 /Used Step: 30000\n",
      "Save complete!\n",
      "Ep:  42 | Mean ep 100 return:  -101.14 /Used Time: 38.98 /Used Step: 40000\n",
      "Save complete!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sungyubkim/anaconda3/lib/python3.7/site-packages/torch/nn/_reduction.py:49: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.\n",
      "  warnings.warn(warning.format(ret))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep:  53 | Mean ep 100 return:  -100.89 /Used Time: 49.01 /Used Step: 50000\n",
      "Save complete!\n",
      "Ep:  64 | Mean ep 100 return:  -101.0 /Used Time: 81.78 /Used Step: 60000\n",
      "Save complete!\n",
      "Ep:  74 | Mean ep 100 return:  -100.92 /Used Time: 115.1 /Used Step: 70000\n",
      "Save complete!\n",
      "Ep:  86 | Mean ep 100 return:  -100.88 /Used Time: 148.52 /Used Step: 80000\n",
      "Save complete!\n",
      "Ep:  97 | Mean ep 100 return:  -100.72 /Used Time: 181.5 /Used Step: 90000\n",
      "Save complete!\n",
      "Ep:  108 | Mean ep 100 return:  -100.95 /Used Time: 213.89 /Used Step: 100000\n",
      "Save complete!\n",
      "Ep:  118 | Mean ep 100 return:  -100.86 /Used Time: 246.43 /Used Step: 110000\n",
      "Save complete!\n",
      "Ep:  129 | Mean ep 100 return:  -100.79 /Used Time: 279.31 /Used Step: 120000\n",
      "Save complete!\n",
      "Ep:  140 | Mean ep 100 return:  -101.04 /Used Time: 311.77 /Used Step: 130000\n",
      "Save complete!\n",
      "Ep:  151 | Mean ep 100 return:  -101.03 /Used Time: 344.32 /Used Step: 140000\n",
      "Save complete!\n",
      "Ep:  162 | Mean ep 100 return:  -101.06 /Used Time: 376.72 /Used Step: 150000\n",
      "Save complete!\n",
      "Ep:  172 | Mean ep 100 return:  -101.03 /Used Time: 409.12 /Used Step: 160000\n",
      "Save complete!\n",
      "Ep:  183 | Mean ep 100 return:  -100.97 /Used Time: 442.66 /Used Step: 170000\n",
      "Save complete!\n",
      "Ep:  193 | Mean ep 100 return:  -100.86 /Used Time: 476.74 /Used Step: 180000\n",
      "Save complete!\n",
      "Ep:  203 | Mean ep 100 return:  -100.9 /Used Time: 509.76 /Used Step: 190000\n",
      "Save complete!\n",
      "Ep:  214 | Mean ep 100 return:  -100.83 /Used Time: 543.66 /Used Step: 200000\n",
      "Save complete!\n",
      "Ep:  224 | Mean ep 100 return:  -100.79 /Used Time: 577.28 /Used Step: 210000\n",
      "Save complete!\n",
      "Ep:  235 | Mean ep 100 return:  -100.76 /Used Time: 610.07 /Used Step: 220000\n",
      "Save complete!\n",
      "Ep:  245 | Mean ep 100 return:  -100.9 /Used Time: 644.43 /Used Step: 230000\n",
      "Save complete!\n",
      "Ep:  256 | Mean ep 100 return:  -100.9 /Used Time: 678.13 /Used Step: 240000\n",
      "Save complete!\n",
      "Ep:  267 | Mean ep 100 return:  -100.9 /Used Time: 712.62 /Used Step: 250000\n",
      "Save complete!\n",
      "Ep:  278 | Mean ep 100 return:  -100.89 /Used Time: 746.63 /Used Step: 260000\n",
      "Save complete!\n",
      "Ep:  289 | Mean ep 100 return:  -100.79 /Used Time: 779.76 /Used Step: 270000\n",
      "Save complete!\n",
      "Ep:  300 | Mean ep 100 return:  -100.88 /Used Time: 812.68 /Used Step: 280000\n",
      "Save complete!\n",
      "Ep:  311 | Mean ep 100 return:  -100.95 /Used Time: 846.18 /Used Step: 290000\n",
      "Save complete!\n",
      "Ep:  321 | Mean ep 100 return:  -100.95 /Used Time: 879.4 /Used Step: 300000\n",
      "Save complete!\n",
      "Ep:  332 | Mean ep 100 return:  -100.94 /Used Time: 913.24 /Used Step: 310000\n",
      "Save complete!\n",
      "Ep:  341 | Mean ep 100 return:  -100.99 /Used Time: 946.98 /Used Step: 320000\n",
      "Save complete!\n",
      "Ep:  351 | Mean ep 100 return:  -100.93 /Used Time: 981.87 /Used Step: 330000\n",
      "Save complete!\n",
      "Ep:  361 | Mean ep 100 return:  -100.96 /Used Time: 1016.23 /Used Step: 340000\n",
      "Save complete!\n",
      "Ep:  371 | Mean ep 100 return:  -100.92 /Used Time: 1049.68 /Used Step: 350000\n",
      "Save complete!\n",
      "Ep:  380 | Mean ep 100 return:  -100.88 /Used Time: 1084.27 /Used Step: 360000\n",
      "Save complete!\n",
      "Ep:  390 | Mean ep 100 return:  -100.84 /Used Time: 1118.56 /Used Step: 370000\n",
      "Save complete!\n",
      "Ep:  399 | Mean ep 100 return:  -100.85 /Used Time: 1152.43 /Used Step: 380000\n",
      "Save complete!\n",
      "Ep:  407 | Mean ep 100 return:  -100.78 /Used Time: 1186.83 /Used Step: 390000\n",
      "Save complete!\n",
      "Ep:  416 | Mean ep 100 return:  -100.56 /Used Time: 1220.21 /Used Step: 400000\n",
      "Save complete!\n",
      "Ep:  424 | Mean ep 100 return:  -100.59 /Used Time: 1253.67 /Used Step: 410000\n",
      "Save complete!\n",
      "Ep:  433 | Mean ep 100 return:  -100.49 /Used Time: 1287.09 /Used Step: 420000\n",
      "Save complete!\n",
      "Ep:  441 | Mean ep 100 return:  -100.4 /Used Time: 1320.78 /Used Step: 430000\n",
      "Save complete!\n",
      "Ep:  448 | Mean ep 100 return:  -100.39 /Used Time: 1354.39 /Used Step: 440000\n",
      "Save complete!\n",
      "Ep:  456 | Mean ep 100 return:  -100.19 /Used Time: 1387.97 /Used Step: 450000\n",
      "Save complete!\n",
      "Ep:  464 | Mean ep 100 return:  -100.14 /Used Time: 1421.78 /Used Step: 460000\n",
      "Save complete!\n",
      "Ep:  471 | Mean ep 100 return:  -99.98 /Used Time: 1455.52 /Used Step: 470000\n",
      "Save complete!\n",
      "Ep:  479 | Mean ep 100 return:  -99.9 /Used Time: 1489.15 /Used Step: 480000\n",
      "Save complete!\n",
      "Ep:  486 | Mean ep 100 return:  -99.79 /Used Time: 1522.55 /Used Step: 490000\n",
      "Save complete!\n",
      "Ep:  494 | Mean ep 100 return:  -99.77 /Used Time: 1556.32 /Used Step: 500000\n",
      "Save complete!\n",
      "Ep:  501 | Mean ep 100 return:  -99.69 /Used Time: 1590.23 /Used Step: 510000\n",
      "Save complete!\n",
      "Ep:  508 | Mean ep 100 return:  -99.62 /Used Time: 1624.06 /Used Step: 520000\n",
      "Save complete!\n",
      "Ep:  515 | Mean ep 100 return:  -99.53 /Used Time: 1658.04 /Used Step: 530000\n",
      "Save complete!\n",
      "Ep:  521 | Mean ep 100 return:  -99.44 /Used Time: 1691.91 /Used Step: 540000\n",
      "Save complete!\n",
      "Ep:  528 | Mean ep 100 return:  -99.24 /Used Time: 1725.6 /Used Step: 550000\n",
      "Save complete!\n",
      "Ep:  535 | Mean ep 100 return:  -99.09 /Used Time: 1759.92 /Used Step: 560000\n",
      "Save complete!\n",
      "Ep:  540 | Mean ep 100 return:  -98.98 /Used Time: 1793.76 /Used Step: 570000\n",
      "Save complete!\n",
      "Ep:  547 | Mean ep 100 return:  -98.8 /Used Time: 1827.91 /Used Step: 580000\n",
      "Save complete!\n",
      "Ep:  553 | Mean ep 100 return:  -98.65 /Used Time: 1862.24 /Used Step: 590000\n",
      "Save complete!\n",
      "Ep:  558 | Mean ep 100 return:  -98.52 /Used Time: 1896.56 /Used Step: 600000\n",
      "Save complete!\n",
      "Ep:  564 | Mean ep 100 return:  -98.36 /Used Time: 1930.89 /Used Step: 610000\n",
      "Save complete!\n",
      "Ep:  570 | Mean ep 100 return:  -98.07 /Used Time: 1965.36 /Used Step: 620000\n",
      "Save complete!\n",
      "Ep:  577 | Mean ep 100 return:  -97.96 /Used Time: 1999.94 /Used Step: 630000\n",
      "Save complete!\n",
      "Ep:  582 | Mean ep 100 return:  -97.85 /Used Time: 2034.07 /Used Step: 640000\n",
      "Save complete!\n",
      "Ep:  588 | Mean ep 100 return:  -97.64 /Used Time: 2068.39 /Used Step: 650000\n",
      "Save complete!\n",
      "Ep:  593 | Mean ep 100 return:  -97.49 /Used Time: 2102.94 /Used Step: 660000\n",
      "Save complete!\n",
      "Ep:  598 | Mean ep 100 return:  -97.28 /Used Time: 2137.39 /Used Step: 670000\n",
      "Save complete!\n",
      "Ep:  604 | Mean ep 100 return:  -96.98 /Used Time: 2171.58 /Used Step: 680000\n",
      "Save complete!\n",
      "Ep:  609 | Mean ep 100 return:  -96.84 /Used Time: 2206.06 /Used Step: 690000\n",
      "Save complete!\n",
      "Ep:  614 | Mean ep 100 return:  -96.67 /Used Time: 2240.62 /Used Step: 700000\n",
      "Save complete!\n",
      "Ep:  619 | Mean ep 100 return:  -96.47 /Used Time: 2275.24 /Used Step: 710000\n",
      "Save complete!\n",
      "Ep:  624 | Mean ep 100 return:  -96.27 /Used Time: 2309.98 /Used Step: 720000\n",
      "Save complete!\n",
      "Ep:  628 | Mean ep 100 return:  -96.08 /Used Time: 2344.74 /Used Step: 730000\n",
      "Save complete!\n",
      "Ep:  634 | Mean ep 100 return:  -95.94 /Used Time: 2379.69 /Used Step: 740000\n",
      "Save complete!\n",
      "Ep:  638 | Mean ep 100 return:  -95.64 /Used Time: 2414.55 /Used Step: 750000\n",
      "Save complete!\n",
      "Ep:  643 | Mean ep 100 return:  -95.44 /Used Time: 2449.79 /Used Step: 760000\n",
      "Save complete!\n",
      "Ep:  648 | Mean ep 100 return:  -95.13 /Used Time: 2484.44 /Used Step: 770000\n",
      "Save complete!\n",
      "Ep:  652 | Mean ep 100 return:  -94.86 /Used Time: 2519.02 /Used Step: 780000\n",
      "Save complete!\n",
      "Ep:  658 | Mean ep 100 return:  -94.61 /Used Time: 2553.91 /Used Step: 790000\n",
      "Save complete!\n",
      "Ep:  662 | Mean ep 100 return:  -94.29 /Used Time: 2588.7 /Used Step: 800000\n",
      "Save complete!\n",
      "Ep:  667 | Mean ep 100 return:  -94.09 /Used Time: 2623.79 /Used Step: 810000\n",
      "Save complete!\n",
      "Ep:  671 | Mean ep 100 return:  -93.76 /Used Time: 2658.74 /Used Step: 820000\n",
      "Save complete!\n",
      "Ep:  676 | Mean ep 100 return:  -93.52 /Used Time: 2693.67 /Used Step: 830000\n",
      "Save complete!\n",
      "Ep:  680 | Mean ep 100 return:  -93.24 /Used Time: 2728.41 /Used Step: 840000\n",
      "Save complete!\n",
      "Ep:  684 | Mean ep 100 return:  -93.24 /Used Time: 2763.09 /Used Step: 850000\n",
      "Save complete!\n",
      "Ep:  688 | Mean ep 100 return:  -92.97 /Used Time: 2798.48 /Used Step: 860000\n",
      "Save complete!\n",
      "Ep:  693 | Mean ep 100 return:  -92.66 /Used Time: 2833.39 /Used Step: 870000\n",
      "Save complete!\n",
      "Ep:  697 | Mean ep 100 return:  -92.41 /Used Time: 2868.55 /Used Step: 880000\n",
      "Save complete!\n",
      "Ep:  701 | Mean ep 100 return:  -92.09 /Used Time: 2903.92 /Used Step: 890000\n",
      "Save complete!\n",
      "Ep:  705 | Mean ep 100 return:  -91.83 /Used Time: 2938.89 /Used Step: 900000\n",
      "Save complete!\n",
      "Ep:  709 | Mean ep 100 return:  -91.83 /Used Time: 2974.04 /Used Step: 910000\n",
      "Save complete!\n",
      "Ep:  713 | Mean ep 100 return:  -91.49 /Used Time: 3009.02 /Used Step: 920000\n",
      "Save complete!\n",
      "Ep:  717 | Mean ep 100 return:  -91.11 /Used Time: 3044.47 /Used Step: 930000\n",
      "Save complete!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep:  721 | Mean ep 100 return:  -90.82 /Used Time: 3079.87 /Used Step: 940000\n",
      "Save complete!\n",
      "Ep:  724 | Mean ep 100 return:  -90.82 /Used Time: 3115.14 /Used Step: 950000\n",
      "Save complete!\n",
      "Ep:  727 | Mean ep 100 return:  -90.39 /Used Time: 3150.28 /Used Step: 960000\n",
      "Save complete!\n",
      "Ep:  731 | Mean ep 100 return:  -89.99 /Used Time: 3185.73 /Used Step: 970000\n",
      "Save complete!\n",
      "Ep:  734 | Mean ep 100 return:  -89.99 /Used Time: 3221.26 /Used Step: 980000\n",
      "Save complete!\n",
      "Ep:  737 | Mean ep 100 return:  -89.45 /Used Time: 3258.1 /Used Step: 990000\n",
      "Save complete!\n",
      "Ep:  740 | Mean ep 100 return:  -88.71 /Used Time: 3293.6 /Used Step: 1000000\n",
      "Save complete!\n",
      "Ep:  743 | Mean ep 100 return:  -88.71 /Used Time: 3329.31 /Used Step: 1010000\n",
      "Save complete!\n",
      "Ep:  746 | Mean ep 100 return:  -88.12 /Used Time: 3365.13 /Used Step: 1020000\n",
      "Save complete!\n",
      "Ep:  749 | Mean ep 100 return:  -88.12 /Used Time: 3400.81 /Used Step: 1030000\n",
      "Save complete!\n",
      "Ep:  752 | Mean ep 100 return:  -87.25 /Used Time: 3436.36 /Used Step: 1040000\n",
      "Save complete!\n",
      "Ep:  755 | Mean ep 100 return:  -86.42 /Used Time: 3471.9 /Used Step: 1050000\n",
      "Save complete!\n",
      "Ep:  758 | Mean ep 100 return:  -86.42 /Used Time: 3507.62 /Used Step: 1060000\n",
      "Save complete!\n",
      "Ep:  761 | Mean ep 100 return:  -85.72 /Used Time: 3543.19 /Used Step: 1070000\n",
      "Save complete!\n",
      "Ep:  764 | Mean ep 100 return:  -85.72 /Used Time: 3578.69 /Used Step: 1080000\n",
      "Save complete!\n",
      "Ep:  767 | Mean ep 100 return:  -84.74 /Used Time: 3614.36 /Used Step: 1090000\n",
      "Save complete!\n",
      "Ep:  770 | Mean ep 100 return:  -83.74 /Used Time: 3649.93 /Used Step: 1100000\n",
      "Save complete!\n",
      "Ep:  773 | Mean ep 100 return:  -83.74 /Used Time: 3685.33 /Used Step: 1110000\n",
      "Save complete!\n",
      "Ep:  775 | Mean ep 100 return:  -82.78 /Used Time: 3720.61 /Used Step: 1120000\n",
      "Save complete!\n",
      "Ep:  778 | Mean ep 100 return:  -82.78 /Used Time: 3756.18 /Used Step: 1130000\n",
      "Save complete!\n",
      "Ep:  781 | Mean ep 100 return:  -81.71 /Used Time: 3792.09 /Used Step: 1140000\n",
      "Save complete!\n",
      "Ep:  784 | Mean ep 100 return:  -81.71 /Used Time: 3828.04 /Used Step: 1150000\n",
      "Save complete!\n",
      "Ep:  787 | Mean ep 100 return:  -80.7 /Used Time: 3863.73 /Used Step: 1160000\n",
      "Save complete!\n",
      "Ep:  790 | Mean ep 100 return:  -79.57 /Used Time: 3898.93 /Used Step: 1170000\n",
      "Save complete!\n",
      "Ep:  793 | Mean ep 100 return:  -79.57 /Used Time: 3934.64 /Used Step: 1180000\n",
      "Save complete!\n",
      "Ep:  796 | Mean ep 100 return:  -78.41 /Used Time: 3970.11 /Used Step: 1190000\n",
      "Save complete!\n",
      "Ep:  799 | Mean ep 100 return:  -78.41 /Used Time: 4005.79 /Used Step: 1200000\n",
      "Save complete!\n",
      "Ep:  802 | Mean ep 100 return:  -77.35 /Used Time: 4041.46 /Used Step: 1210000\n",
      "Save complete!\n",
      "Ep:  805 | Mean ep 100 return:  -76.11 /Used Time: 4077.04 /Used Step: 1220000\n",
      "Save complete!\n",
      "Ep:  808 | Mean ep 100 return:  -76.11 /Used Time: 4112.65 /Used Step: 1230000\n",
      "Save complete!\n",
      "Ep:  811 | Mean ep 100 return:  -75.06 /Used Time: 4148.44 /Used Step: 1240000\n",
      "Save complete!\n",
      "Ep:  815 | Mean ep 100 return:  -73.99 /Used Time: 4184.04 /Used Step: 1250000\n",
      "Save complete!\n",
      "Ep:  818 | Mean ep 100 return:  -73.99 /Used Time: 4219.41 /Used Step: 1260000\n",
      "Save complete!\n",
      "Ep:  821 | Mean ep 100 return:  -72.98 /Used Time: 4254.98 /Used Step: 1270000\n",
      "Save complete!\n",
      "Ep:  824 | Mean ep 100 return:  -72.98 /Used Time: 4290.62 /Used Step: 1280000\n",
      "Save complete!\n",
      "Ep:  828 | Mean ep 100 return:  -71.77 /Used Time: 4326.14 /Used Step: 1290000\n",
      "Save complete!\n",
      "Ep:  831 | Mean ep 100 return:  -70.85 /Used Time: 4361.64 /Used Step: 1300000\n",
      "Save complete!\n",
      "Ep:  834 | Mean ep 100 return:  -70.85 /Used Time: 4397.01 /Used Step: 1310000\n",
      "Save complete!\n",
      "Ep:  837 | Mean ep 100 return:  -69.52 /Used Time: 4432.73 /Used Step: 1320000\n",
      "Save complete!\n",
      "Ep:  840 | Mean ep 100 return:  -68.6 /Used Time: 4468.3 /Used Step: 1330000\n",
      "Save complete!\n",
      "Ep:  844 | Mean ep 100 return:  -68.6 /Used Time: 4503.9 /Used Step: 1340000\n",
      "Save complete!\n",
      "Ep:  847 | Mean ep 100 return:  -67.55 /Used Time: 4539.38 /Used Step: 1350000\n",
      "Save complete!\n",
      "Ep:  851 | Mean ep 100 return:  -66.2 /Used Time: 4574.7 /Used Step: 1360000\n",
      "Save complete!\n",
      "Ep:  854 | Mean ep 100 return:  -66.2 /Used Time: 4610.22 /Used Step: 1370000\n",
      "Save complete!\n",
      "Ep:  857 | Mean ep 100 return:  -64.96 /Used Time: 4645.61 /Used Step: 1380000\n",
      "Save complete!\n",
      "Ep:  861 | Mean ep 100 return:  -63.62 /Used Time: 4681.13 /Used Step: 1390000\n",
      "Save complete!\n",
      "Ep:  864 | Mean ep 100 return:  -63.62 /Used Time: 4716.76 /Used Step: 1400000\n",
      "Save complete!\n",
      "Ep:  867 | Mean ep 100 return:  -62.08 /Used Time: 4752.22 /Used Step: 1410000\n",
      "Save complete!\n",
      "Ep:  871 | Mean ep 100 return:  -60.77 /Used Time: 4787.79 /Used Step: 1420000\n",
      "Save complete!\n",
      "Ep:  874 | Mean ep 100 return:  -60.77 /Used Time: 4823.22 /Used Step: 1430000\n",
      "Save complete!\n",
      "Ep:  877 | Mean ep 100 return:  -59.42 /Used Time: 4858.54 /Used Step: 1440000\n",
      "Save complete!\n",
      "Ep:  881 | Mean ep 100 return:  -58.11 /Used Time: 4894.05 /Used Step: 1450000\n",
      "Save complete!\n",
      "Ep:  885 | Mean ep 100 return:  -56.9 /Used Time: 4929.61 /Used Step: 1460000\n",
      "Save complete!\n",
      "Ep:  889 | Mean ep 100 return:  -56.9 /Used Time: 4965.44 /Used Step: 1470000\n",
      "Save complete!\n",
      "Ep:  892 | Mean ep 100 return:  -55.25 /Used Time: 5001.52 /Used Step: 1480000\n",
      "Save complete!\n",
      "Ep:  896 | Mean ep 100 return:  -53.71 /Used Time: 5036.87 /Used Step: 1490000\n",
      "Save complete!\n"
     ]
    }
   ],
   "source": [
    "dqn = DQN()\n",
    "\n",
    "# model load with check\n",
    "if LOAD and os.path.isfile(EVAL_PATH) and os.path.isfile(TARGET_PATH):\n",
    "    dqn.load_model()\n",
    "    pkl_file = open(RESULT_PATH,'rb')\n",
    "    result = pickle.load(pkl_file)\n",
    "    pkl_file.close()\n",
    "    print('Load complete!')\n",
    "else:\n",
    "    result = []\n",
    "    print('Initialize results!')\n",
    "\n",
    "print('Collecting experience...')\n",
    "\n",
    "# episode step for accumulate reward (since we are using EpisodicLifeEnv of OpenAI gym wrapper)\n",
    "epi_step = 0\n",
    "# accumulate return of current episode\n",
    "entire_ep_r = 0.\n",
    "# log for accumulate returns\n",
    "entire_ep_rs = []\n",
    "# check learning time\n",
    "start_time = time.time()\n",
    "\n",
    "while dqn.memory_counter <= STEP_NUM:\n",
    "    # env reset\n",
    "    s = np.array(env.reset())\n",
    "    \n",
    "    # initialize one episode reward\n",
    "    ep_r = 0.\n",
    "\n",
    "    while True:\n",
    "        a = dqn.choose_action(s, EPSILON)\n",
    "\n",
    "        # take action and get next state\n",
    "        s_, r, done, info = env.step(a)\n",
    "        s_ = np.array(s_)\n",
    "        \n",
    "        # accumulate return\n",
    "        ep_r += r\n",
    "        # clip rewards for numerical stability\n",
    "        clip_r = np.sign(r)\n",
    "\n",
    "        # store the transition\n",
    "        dqn.store_transition(s, a, clip_r, s_, float(done))\n",
    "\n",
    "        # annealing the epsilon(exploration strategy), beta(per smoothing)\n",
    "        if dqn.memory_counter <= 1e+6:\n",
    "            # linear annealing to 0.9 until million step\n",
    "            EPSILON += 0.9/(1e+6)\n",
    "        elif dqn.memory_counter <= (2e+7):\n",
    "            # linear annealing to 0.99 until the end\n",
    "            EPSILON += 0.09/(2e+7 - 1e+6)\n",
    "            # linear annealing to 1 until the end\n",
    "            PER_BETA += (1.0 - PER_BETA) /(2e+7 - 1e+6)\n",
    "\n",
    "        # if memory fill 50K and mod 4 = 0(for speed issue), learn pred net\n",
    "        if (5e+4 <= dqn.memory_counter) and (dqn.memory_counter % 4 == 0):\n",
    "            dqn.learn(PER_BETA)\n",
    "            \n",
    "        # print log and save\n",
    "        if dqn.memory_counter % SAVE_FREQ == 0:\n",
    "            # check time interval\n",
    "            time_interval = round(time.time() - start_time, 2)\n",
    "            # calc mean return\n",
    "            mean_100_ep_return = round(np.mean(entire_ep_rs[-101:-1]),2)\n",
    "            result.append(mean_100_ep_return)\n",
    "            # print log\n",
    "            print('Ep: ',epi_step,\n",
    "                  '| Mean ep 100 return: ', mean_100_ep_return,\n",
    "                  '/Used Time:',time_interval,\n",
    "                  '/Used Step:',dqn.memory_counter)\n",
    "            # save model\n",
    "            dqn.save_model()\n",
    "            pkl_file = open(RESULT_PATH, 'wb')\n",
    "            pickle.dump(np.array(result), pkl_file)\n",
    "            pkl_file.close()\n",
    "            print('Save complete!')\n",
    "            \n",
    "        # if agent meets end-of-life, update return, acc return\n",
    "        if done:\n",
    "            entire_ep_r += ep_r\n",
    "            epi_step += 1\n",
    "            if epi_step % 5 == 0:\n",
    "                entire_ep_rs.append(entire_ep_r)\n",
    "                entire_ep_r = 0.\n",
    "            break\n",
    "\n",
    "        s = s_\n",
    "\n",
    "        if RENDERING:\n",
    "            env.render()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
