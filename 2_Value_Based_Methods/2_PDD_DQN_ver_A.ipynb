{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 필요한 모듈 설치 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from replay_memory import ReplayBuffer, PrioritizedReplayBuffer\n",
    "\n",
    "import random\n",
    "import os\n",
    "import pickle\n",
    "import time\n",
    "from copy import deepcopy\n",
    "\n",
    "# 만약 opencv-python이 설치되어있지 않다면 다음을 통해서 설치해주세요.\n",
    "# pip install opencv-python\n",
    "# 만약 설치에 오류가 발생한다면 다음을 참고해주세요.\n",
    "# https://opencv-python-tutroals.readthedocs.io/en/latest/py_tutorials/py_setup/py_table_of_contents_setup/py_table_of_contents_setup.html#py-table-of-content-setup\n",
    "from wrappers import wrap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 하이퍼 파라미터 정의하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "USE GPU: True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sungyubkim/gym/gym/logger.py:30: UserWarning: \u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.uint8'>. Please provide explicit dtype.\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n",
      "/home/sungyubkim/gym/gym/logger.py:30: UserWarning: \u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    }
   ],
   "source": [
    "'''DQN settings'''\n",
    "# sequential images to define state\n",
    "STATE_LEN = 4\n",
    "# target policy sync interval\n",
    "TARGET_REPLACE_ITER = 10**3\n",
    "# simulator steps for start learning\n",
    "LEARN_START = 10**4\n",
    "# (prioritized) experience replay memory size\n",
    "MEMORY_CAPACITY = 10**6\n",
    "# simulator steps for learning interval\n",
    "LEARN_FREQ = 4\n",
    "# check per\n",
    "is_per = True\n",
    "# alpha of PER\n",
    "PER_ALPHA = 0.6\n",
    "PER_BETA = 0.4\n",
    "PER_EPSILON = 1e-6\n",
    "# Double DQN\n",
    "DOUBLE = True\n",
    "# Dueling architecture\n",
    "DUEL = True\n",
    "\n",
    "'''Environment Settings'''\n",
    "# openai gym env name\n",
    "ENV_NAME = 'PongNoFrameskip-v4'\n",
    "env = wrap(gym.make(ENV_NAME))\n",
    "N_ACTIONS = env.action_space.n\n",
    "N_STATES = env.observation_space.shape\n",
    "# Total simulation step\n",
    "STEP_NUM = 10**7\n",
    "# gamma for MDP\n",
    "GAMMA = 0.99\n",
    "# visualize for agent playing\n",
    "RENDERING = False\n",
    "\n",
    "'''Training settings'''\n",
    "# check GPU usage\n",
    "USE_GPU = torch.cuda.is_available()\n",
    "print('USE GPU: '+str(USE_GPU))\n",
    "# mini-batch size\n",
    "BATCH_SIZE = 32\n",
    "# learning rage\n",
    "LR = 1e-4\n",
    "# epsilon-greedy\n",
    "EPSILON = 0.0\n",
    "\n",
    "'''Save&Load Settings'''\n",
    "# check save/load\n",
    "SAVE = True\n",
    "LOAD = False\n",
    "# save frequency\n",
    "SAVE_FREQ = 10**5\n",
    "# paths for predction net, target net, result log\n",
    "PRED_PATH = './data/model/pred_net.pkl'\n",
    "TARGET_PATH = './data/model/target_net.pkl'\n",
    "RESULT_PATH = './data/plots/result.pkl'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 네트워크 구조 정의하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ConvNet, self).__init__()\n",
    "        # nn.Sequential을 사용하면 다음과 같입 코드를 간결하게 바꿀 수 있습니다.\n",
    "        self.feature_extraction = nn.Sequential(\n",
    "            nn.Conv2d(STATE_LEN, 32, kernel_size=8, stride=4),\n",
    "            nn.LeakyReLU(negative_slope=0.01),\n",
    "            nn.Conv2d(32, 64, kernel_size=4, stride=2),\n",
    "            nn.LeakyReLU(negative_slope=0.01),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, stride=1),\n",
    "            nn.LeakyReLU(negative_slope=0.01),\n",
    "        )\n",
    "        self.fc = nn.Linear(7 * 7 * 64, 256)\n",
    "\n",
    "        if DUEL:\n",
    "            # advantage function/ state value function\n",
    "            self.fc_adv = nn.Linear(256, N_ACTIONS)\n",
    "            self.fc_val = nn.Linear(256, 1)\n",
    "        else:\n",
    "            # action value function\n",
    "            self.fc_q = nn.Linear(256, N_ACTIONS) \n",
    "            \n",
    "        # 파라미터 값 초기화 코드는 다음과 같이 간결하게 바꿀 수 있습니다.\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.orthogonal_(m.weight, gain = np.sqrt(2))\n",
    "                nn.init.constant_(m.bias, 0.0)\n",
    "            elif isinstance(m, nn.Linear):\n",
    "                nn.init.kaiming_normal_(m.weight)\n",
    "                nn.init.constant_(m.bias, 0.0)\n",
    "            \n",
    "\n",
    "    def forward(self, x):\n",
    "        # x는 (m, 84, 84, 4)의 tensor\n",
    "        x = self.feature_extraction(x / 255.0)\n",
    "        # x.size(0) : mini-batch size\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = F.leaky_relu(self.fc(x), negative_slope=0.01)\n",
    "        \n",
    "        if DUEL:\n",
    "            adv = self.fc_adv(x)\n",
    "            val = self.fc_val(x)\n",
    "            action_value = val + adv - adv.mean(1).unsqueeze(1)\n",
    "        else:\n",
    "            action_value = self.fc_q(x)\n",
    "\n",
    "        return action_value\n",
    "\n",
    "    def save(self, PATH):\n",
    "        torch.save(self.state_dict(),PATH)\n",
    "\n",
    "    def load(self, PATH):\n",
    "        self.load_state_dict(torch.load(PATH))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DQN 정의하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN(object):\n",
    "    def __init__(self):\n",
    "        self.pred_net, self.target_net = ConvNet(), ConvNet()\n",
    "        # sync eval target\n",
    "        self.update_target(self.target_net, self.pred_net, 1.0)\n",
    "        # use gpu\n",
    "        if USE_GPU:\n",
    "            self.pred_net.cuda()\n",
    "            self.target_net.cuda()\n",
    "            \n",
    "        # simulator step conter\n",
    "        self.memory_counter = 0\n",
    "        # target network step counter\n",
    "        self.learn_step_counter = 0\n",
    "        \n",
    "        # ceate the replay buffer\n",
    "        if is_per:\n",
    "            self.replay_buffer = PrioritizedReplayBuffer(MEMORY_CAPACITY, alpha=PER_ALPHA)\n",
    "        else:\n",
    "            self.replay_buffer = ReplayBuffer(MEMORY_CAPACITY)\n",
    "        \n",
    "        # define optimizer\n",
    "        self.optimizer = torch.optim.Adam(self.pred_net.parameters(), lr=LR)\n",
    "        \n",
    "    def update_target(self, target, pred, update_rate):\n",
    "        # update target network parameters using predcition network\n",
    "        for target_param, pred_param in zip(target.parameters(), pred.parameters()):\n",
    "            target_param.data.copy_((1.0 - update_rate) \\\n",
    "                                    * target_param.data + update_rate*pred_param.data)\n",
    "            \n",
    "    def save_model(self):\n",
    "        # save prediction network and target network\n",
    "        self.pred_net.save(PRED_PATH)\n",
    "        self.target_net.save(TARGET_PATH)\n",
    "\n",
    "    def load_model(self):\n",
    "        # load prediction network and target network\n",
    "        self.pred_net.load(PRED_PATH)\n",
    "        self.target_net.load(TARGET_PATH)\n",
    "\n",
    "    def choose_action(self, x, EPSILON):\n",
    "        x = torch.FloatTensor(x)\n",
    "        if USE_GPU:\n",
    "            x = x.cuda()\n",
    "\n",
    "        if np.random.uniform() < EPSILON:\n",
    "            # greedy case\n",
    "            action_value = self.pred_net(x.unsqueeze(0))\n",
    "            action = torch.argmax(action_value).data.cpu().numpy()\n",
    "        else:\n",
    "            # random exploration case\n",
    "            action = np.random.randint(0, N_ACTIONS)\n",
    "        return action\n",
    "\n",
    "    def store_transition(self, s, a, r, s_, done):\n",
    "        self.memory_counter += 1\n",
    "        self.replay_buffer.add(s, a, r, s_, float(done))\n",
    "\n",
    "    def learn(self, beta):\n",
    "        self.learn_step_counter += 1\n",
    "        # target parameter update\n",
    "        if self.learn_step_counter % TARGET_REPLACE_ITER == 0:\n",
    "            self.update_target(self.target_net, self.pred_net, 1.0)\n",
    "        \n",
    "        # data sample from experience replay\n",
    "        if is_per:\n",
    "            experience = self.replay_buffer.sample(BATCH_SIZE, beta=beta)\n",
    "            (b_state_memory, b_action_memory, b_reward_memory, \\\n",
    "             b_next_state_memory, b_done, b_weights, b_idxes) = experience\n",
    "        else:\n",
    "            b_state_memory, b_action_memory, b_reward_memory, \\\n",
    "            b_next_state_memory, b_done = self.replay_buffer.sample(BATCH_SIZE)\n",
    "            b_weights, b_idxes = np.ones_like(b_reward_memory), None\n",
    "            \n",
    "        b_s = torch.FloatTensor(b_state_memory)\n",
    "        b_a = torch.LongTensor(b_action_memory)\n",
    "        b_r = torch.FloatTensor(b_reward_memory)\n",
    "        b_s_ = torch.FloatTensor(b_next_state_memory)\n",
    "        b_d = torch.FloatTensor(b_done)\n",
    "\n",
    "        if USE_GPU:\n",
    "            b_s, b_a, b_r, b_s_, b_d = b_s.cuda(), b_a.cuda(), b_r.cuda(), b_s_.cuda(), b_d.cuda()\n",
    "\n",
    "        # action value prediction\n",
    "        q_eval = self.pred_net(b_s).gather(1, b_a.unsqueeze(1)).view(-1)\n",
    "        # shape : (m)\n",
    "\n",
    "        if DOUBLE:\n",
    "            # get best actions of next state\n",
    "            _ , best_actions = self.pred_net(b_s_).detach().max(1)\n",
    "            # get next state value\n",
    "            q_next = self.target_net(b_s_).detach()\n",
    "            # get target value\n",
    "            q_target = b_r + GAMMA *(1.-b_d)* q_next.gather(1, best_actions.unsqueeze(1)).squeeze(1)\n",
    "            # shape (m)\n",
    "        else:\n",
    "            # get next state value\n",
    "            q_next = self.target_net(b_s_).detach()\n",
    "            # get target value\n",
    "            q_target = b_r + GAMMA *(1.-b_d)* q_next.max(1)[0]\n",
    "            # shape (m)\n",
    "            \n",
    "        # calc huber loss, dont reduce for importance weight\n",
    "        loss = F.smooth_l1_loss(q_eval, q_target, reduction='none')\n",
    "        # calc importance weighted loss\n",
    "        loss = torch.mean(torch.Tensor(b_weights).cuda()*loss)\n",
    "        # get td error\n",
    "        td_error = (q_target - q_eval).data.cpu().numpy()\n",
    "        \n",
    "        # update importance weight\n",
    "        if is_per:\n",
    "            new_priorities = np.abs(td_error) + PER_EPSILON\n",
    "            self.replay_buffer.update_priorities(b_idxes, new_priorities)\n",
    "        \n",
    "        # backprop loss\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(self.pred_net.parameters(),10.)\n",
    "        self.optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialize results!\n",
      "Collecting experience...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sungyubkim/gym/gym/logger.py:30: UserWarning: \u001b[33mWARN: <class 'wrappers.FrameStack'> doesn't implement 'reset' method, but it implements deprecated '_reset' method.\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n",
      "/home/sungyubkim/anaconda3/lib/python3.7/site-packages/torch/nn/_reduction.py:49: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.\n",
      "  warnings.warn(warning.format(ret))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep:  105 | Mean ep 10 return:  -19.89 /Used Time: 224.92 /Used Step: 100000\n",
      "Save complete!\n",
      "Ep:  201 | Mean ep 10 return:  -19.11 /Used Time: 579.91 /Used Step: 200000\n",
      "Save complete!\n",
      "Ep:  290 | Mean ep 10 return:  -19.33 /Used Time: 939.82 /Used Step: 300000\n",
      "Save complete!\n",
      "Ep:  372 | Mean ep 10 return:  -18.11 /Used Time: 1305.22 /Used Step: 400000\n",
      "Save complete!\n",
      "Ep:  441 | Mean ep 10 return:  -17.22 /Used Time: 1675.59 /Used Step: 500000\n",
      "Save complete!\n",
      "Ep:  502 | Mean ep 10 return:  -14.89 /Used Time: 2051.08 /Used Step: 600000\n",
      "Save complete!\n",
      "Ep:  551 | Mean ep 10 return:  -12.78 /Used Time: 2429.26 /Used Step: 700000\n",
      "Save complete!\n",
      "Ep:  593 | Mean ep 10 return:  -8.33 /Used Time: 2813.95 /Used Step: 800000\n",
      "Save complete!\n",
      "Ep:  626 | Mean ep 10 return:  -1.0 /Used Time: 3201.12 /Used Step: 900000\n",
      "Save complete!\n",
      "Ep:  661 | Mean ep 10 return:  12.56 /Used Time: 3593.12 /Used Step: 1000000\n",
      "Save complete!\n",
      "Ep:  699 | Mean ep 10 return:  11.78 /Used Time: 3988.74 /Used Step: 1100000\n",
      "Save complete!\n",
      "Ep:  739 | Mean ep 10 return:  14.0 /Used Time: 4384.18 /Used Step: 1200000\n",
      "Save complete!\n",
      "Ep:  781 | Mean ep 10 return:  12.22 /Used Time: 4779.43 /Used Step: 1300000\n",
      "Save complete!\n",
      "Ep:  820 | Mean ep 10 return:  10.56 /Used Time: 5175.49 /Used Step: 1400000\n",
      "Save complete!\n",
      "Ep:  862 | Mean ep 10 return:  17.56 /Used Time: 5571.74 /Used Step: 1500000\n",
      "Save complete!\n",
      "Ep:  906 | Mean ep 10 return:  14.67 /Used Time: 5968.45 /Used Step: 1600000\n",
      "Save complete!\n",
      "Ep:  950 | Mean ep 10 return:  15.89 /Used Time: 6366.15 /Used Step: 1700000\n",
      "Save complete!\n",
      "Ep:  998 | Mean ep 10 return:  17.22 /Used Time: 6761.89 /Used Step: 1800000\n",
      "Save complete!\n",
      "Ep:  1046 | Mean ep 10 return:  17.0 /Used Time: 7157.94 /Used Step: 1900000\n",
      "Save complete!\n",
      "Ep:  1095 | Mean ep 10 return:  16.22 /Used Time: 7560.32 /Used Step: 2000000\n",
      "Save complete!\n",
      "Ep:  1142 | Mean ep 10 return:  16.56 /Used Time: 7963.41 /Used Step: 2100000\n",
      "Save complete!\n",
      "Ep:  1192 | Mean ep 10 return:  18.11 /Used Time: 8370.91 /Used Step: 2200000\n",
      "Save complete!\n",
      "Ep:  1242 | Mean ep 10 return:  16.11 /Used Time: 8782.35 /Used Step: 2300000\n",
      "Save complete!\n",
      "Ep:  1291 | Mean ep 10 return:  16.89 /Used Time: 9196.58 /Used Step: 2400000\n",
      "Save complete!\n",
      "Ep:  1341 | Mean ep 10 return:  18.22 /Used Time: 9606.89 /Used Step: 2500000\n",
      "Save complete!\n",
      "Ep:  1391 | Mean ep 10 return:  18.22 /Used Time: 10020.58 /Used Step: 2600000\n",
      "Save complete!\n",
      "Ep:  1440 | Mean ep 10 return:  17.56 /Used Time: 10435.3 /Used Step: 2700000\n",
      "Save complete!\n",
      "Ep:  1490 | Mean ep 10 return:  16.89 /Used Time: 10850.61 /Used Step: 2800000\n",
      "Save complete!\n",
      "Ep:  1539 | Mean ep 10 return:  17.11 /Used Time: 11268.65 /Used Step: 2900000\n",
      "Save complete!\n",
      "Ep:  1589 | Mean ep 10 return:  16.56 /Used Time: 11690.87 /Used Step: 3000000\n",
      "Save complete!\n",
      "Ep:  1638 | Mean ep 10 return:  17.22 /Used Time: 12125.07 /Used Step: 3100000\n",
      "Save complete!\n",
      "Ep:  1690 | Mean ep 10 return:  17.78 /Used Time: 12561.51 /Used Step: 3200000\n",
      "Save complete!\n",
      "Ep:  1743 | Mean ep 10 return:  18.67 /Used Time: 13005.65 /Used Step: 3300000\n",
      "Save complete!\n",
      "Ep:  1792 | Mean ep 10 return:  17.0 /Used Time: 13451.01 /Used Step: 3400000\n",
      "Save complete!\n",
      "Ep:  1843 | Mean ep 10 return:  17.33 /Used Time: 13896.7 /Used Step: 3500000\n",
      "Save complete!\n",
      "Ep:  1894 | Mean ep 10 return:  17.44 /Used Time: 14350.07 /Used Step: 3600000\n",
      "Save complete!\n",
      "Ep:  1943 | Mean ep 10 return:  16.67 /Used Time: 14799.04 /Used Step: 3700000\n",
      "Save complete!\n",
      "Ep:  1994 | Mean ep 10 return:  18.89 /Used Time: 15284.3 /Used Step: 3800000\n",
      "Save complete!\n",
      "Ep:  2046 | Mean ep 10 return:  18.33 /Used Time: 15774.72 /Used Step: 3900000\n",
      "Save complete!\n",
      "Ep:  2099 | Mean ep 10 return:  18.78 /Used Time: 16270.54 /Used Step: 4000000\n",
      "Save complete!\n",
      "Ep:  2152 | Mean ep 10 return:  18.67 /Used Time: 16791.26 /Used Step: 4100000\n",
      "Save complete!\n",
      "Ep:  2204 | Mean ep 10 return:  18.89 /Used Time: 17264.61 /Used Step: 4200000\n",
      "Save complete!\n",
      "Ep:  2258 | Mean ep 10 return:  19.44 /Used Time: 17771.19 /Used Step: 4300000\n",
      "Save complete!\n",
      "Ep:  2312 | Mean ep 10 return:  18.33 /Used Time: 18260.43 /Used Step: 4400000\n",
      "Save complete!\n",
      "Ep:  2365 | Mean ep 10 return:  17.67 /Used Time: 18718.11 /Used Step: 4500000\n",
      "Save complete!\n",
      "Ep:  2418 | Mean ep 10 return:  19.0 /Used Time: 19200.66 /Used Step: 4600000\n",
      "Save complete!\n",
      "Ep:  2470 | Mean ep 10 return:  17.67 /Used Time: 19705.47 /Used Step: 4700000\n",
      "Save complete!\n",
      "Ep:  2523 | Mean ep 10 return:  18.0 /Used Time: 20191.73 /Used Step: 4800000\n",
      "Save complete!\n",
      "Ep:  2577 | Mean ep 10 return:  18.78 /Used Time: 20674.63 /Used Step: 4900000\n",
      "Save complete!\n",
      "Ep:  2631 | Mean ep 10 return:  19.44 /Used Time: 21160.66 /Used Step: 5000000\n",
      "Save complete!\n",
      "Ep:  2685 | Mean ep 10 return:  19.67 /Used Time: 21643.22 /Used Step: 5100000\n",
      "Save complete!\n",
      "Ep:  2739 | Mean ep 10 return:  18.11 /Used Time: 22128.93 /Used Step: 5200000\n",
      "Save complete!\n",
      "Ep:  2794 | Mean ep 10 return:  18.22 /Used Time: 22594.58 /Used Step: 5300000\n",
      "Save complete!\n",
      "Ep:  2847 | Mean ep 10 return:  17.33 /Used Time: 23052.42 /Used Step: 5400000\n",
      "Save complete!\n",
      "Ep:  2902 | Mean ep 10 return:  18.33 /Used Time: 23506.93 /Used Step: 5500000\n",
      "Save complete!\n",
      "Ep:  2956 | Mean ep 10 return:  19.22 /Used Time: 23963.83 /Used Step: 5600000\n",
      "Save complete!\n",
      "Ep:  3011 | Mean ep 10 return:  18.67 /Used Time: 24419.94 /Used Step: 5700000\n",
      "Save complete!\n",
      "Ep:  3066 | Mean ep 10 return:  18.89 /Used Time: 24875.67 /Used Step: 5800000\n",
      "Save complete!\n",
      "Ep:  3121 | Mean ep 10 return:  18.89 /Used Time: 25341.65 /Used Step: 5900000\n",
      "Save complete!\n",
      "Ep:  3176 | Mean ep 10 return:  19.67 /Used Time: 25816.77 /Used Step: 6000000\n",
      "Save complete!\n",
      "Ep:  3231 | Mean ep 10 return:  18.67 /Used Time: 26296.06 /Used Step: 6100000\n",
      "Save complete!\n",
      "Ep:  3288 | Mean ep 10 return:  18.89 /Used Time: 26775.18 /Used Step: 6200000\n",
      "Save complete!\n",
      "Ep:  3343 | Mean ep 10 return:  18.22 /Used Time: 27260.18 /Used Step: 6300000\n",
      "Save complete!\n",
      "Ep:  3398 | Mean ep 10 return:  17.22 /Used Time: 27748.02 /Used Step: 6400000\n",
      "Save complete!\n",
      "Ep:  3454 | Mean ep 10 return:  19.56 /Used Time: 28236.61 /Used Step: 6500000\n",
      "Save complete!\n",
      "Ep:  3511 | Mean ep 10 return:  19.67 /Used Time: 28726.51 /Used Step: 6600000\n",
      "Save complete!\n",
      "Ep:  3566 | Mean ep 10 return:  18.56 /Used Time: 29219.05 /Used Step: 6700000\n",
      "Save complete!\n",
      "Ep:  3622 | Mean ep 10 return:  18.67 /Used Time: 29717.61 /Used Step: 6800000\n",
      "Save complete!\n",
      "Ep:  3678 | Mean ep 10 return:  19.56 /Used Time: 30198.98 /Used Step: 6900000\n",
      "Save complete!\n",
      "Ep:  3734 | Mean ep 10 return:  19.67 /Used Time: 30694.32 /Used Step: 7000000\n",
      "Save complete!\n",
      "Ep:  3789 | Mean ep 10 return:  19.0 /Used Time: 31183.55 /Used Step: 7100000\n",
      "Save complete!\n",
      "Ep:  3843 | Mean ep 10 return:  19.11 /Used Time: 31674.11 /Used Step: 7200000\n",
      "Save complete!\n",
      "Ep:  3900 | Mean ep 10 return:  20.0 /Used Time: 32156.8 /Used Step: 7300000\n",
      "Save complete!\n",
      "Ep:  3957 | Mean ep 10 return:  19.11 /Used Time: 32623.61 /Used Step: 7400000\n",
      "Save complete!\n",
      "Ep:  4014 | Mean ep 10 return:  19.11 /Used Time: 33089.92 /Used Step: 7500000\n",
      "Save complete!\n",
      "Ep:  4072 | Mean ep 10 return:  20.11 /Used Time: 33556.03 /Used Step: 7600000\n",
      "Save complete!\n",
      "Ep:  4128 | Mean ep 10 return:  18.33 /Used Time: 34022.59 /Used Step: 7700000\n",
      "Save complete!\n",
      "Ep:  4186 | Mean ep 10 return:  19.22 /Used Time: 34501.2 /Used Step: 7800000\n",
      "Save complete!\n",
      "Ep:  4242 | Mean ep 10 return:  18.22 /Used Time: 34982.56 /Used Step: 7900000\n",
      "Save complete!\n",
      "Ep:  4300 | Mean ep 10 return:  20.0 /Used Time: 35469.77 /Used Step: 8000000\n",
      "Save complete!\n",
      "Ep:  4358 | Mean ep 10 return:  20.11 /Used Time: 35961.05 /Used Step: 8100000\n",
      "Save complete!\n",
      "Ep:  4416 | Mean ep 10 return:  20.0 /Used Time: 36451.46 /Used Step: 8200000\n",
      "Save complete!\n",
      "Ep:  4476 | Mean ep 10 return:  20.33 /Used Time: 36944.87 /Used Step: 8300000\n",
      "Save complete!\n",
      "Ep:  4535 | Mean ep 10 return:  19.33 /Used Time: 37434.57 /Used Step: 8400000\n",
      "Save complete!\n",
      "Ep:  4595 | Mean ep 10 return:  20.33 /Used Time: 37927.78 /Used Step: 8500000\n",
      "Save complete!\n",
      "Ep:  4652 | Mean ep 10 return:  19.33 /Used Time: 38425.52 /Used Step: 8600000\n",
      "Save complete!\n",
      "Ep:  4710 | Mean ep 10 return:  18.89 /Used Time: 38924.83 /Used Step: 8700000\n",
      "Save complete!\n",
      "Ep:  4770 | Mean ep 10 return:  20.11 /Used Time: 39430.99 /Used Step: 8800000\n",
      "Save complete!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep:  4830 | Mean ep 10 return:  20.22 /Used Time: 39929.11 /Used Step: 8900000\n",
      "Save complete!\n",
      "Ep:  4889 | Mean ep 10 return:  20.33 /Used Time: 40436.03 /Used Step: 9000000\n",
      "Save complete!\n",
      "Ep:  4949 | Mean ep 10 return:  20.0 /Used Time: 40938.02 /Used Step: 9100000\n",
      "Save complete!\n",
      "Ep:  5009 | Mean ep 10 return:  19.78 /Used Time: 41425.99 /Used Step: 9200000\n",
      "Save complete!\n",
      "Ep:  5069 | Mean ep 10 return:  20.56 /Used Time: 41907.05 /Used Step: 9300000\n",
      "Save complete!\n",
      "Ep:  5130 | Mean ep 10 return:  20.22 /Used Time: 42380.33 /Used Step: 9400000\n",
      "Save complete!\n",
      "Ep:  5190 | Mean ep 10 return:  20.22 /Used Time: 42854.44 /Used Step: 9500000\n",
      "Save complete!\n",
      "Ep:  5249 | Mean ep 10 return:  19.67 /Used Time: 43338.02 /Used Step: 9600000\n",
      "Save complete!\n",
      "Ep:  5309 | Mean ep 10 return:  20.11 /Used Time: 43838.64 /Used Step: 9700000\n",
      "Save complete!\n",
      "Ep:  5370 | Mean ep 10 return:  20.33 /Used Time: 44352.07 /Used Step: 9800000\n",
      "Save complete!\n",
      "Ep:  5429 | Mean ep 10 return:  20.0 /Used Time: 44861.39 /Used Step: 9900000\n",
      "Save complete!\n",
      "Ep:  5490 | Mean ep 10 return:  19.56 /Used Time: 45371.26 /Used Step: 10000000\n",
      "Save complete!\n"
     ]
    }
   ],
   "source": [
    "dqn = DQN()\n",
    "\n",
    "# model load with check\n",
    "if LOAD and os.path.isfile(EVAL_PATH) and os.path.isfile(TARGET_PATH):\n",
    "    dqn.load_model()\n",
    "    pkl_file = open(RESULT_PATH,'rb')\n",
    "    result = pickle.load(pkl_file)\n",
    "    pkl_file.close()\n",
    "    print('Load complete!')\n",
    "else:\n",
    "    result = []\n",
    "    print('Initialize results!')\n",
    "\n",
    "print('Collecting experience...')\n",
    "\n",
    "# episode step for accumulate reward \n",
    "# (since we are using EpisodicLifeEnv of OpenAI gym wrapper)\n",
    "epi_step = 0\n",
    "# accumulate return of current episode\n",
    "entire_ep_r = 0.\n",
    "# log for accumulate returns\n",
    "entire_ep_rs = []\n",
    "# check learning time\n",
    "start_time = time.time()\n",
    "\n",
    "while dqn.memory_counter <= STEP_NUM:\n",
    "    # env reset\n",
    "    s = np.array(env.reset())\n",
    "    \n",
    "    # initialize one episode reward\n",
    "    ep_r = 0.\n",
    "\n",
    "    while True:\n",
    "        a = dqn.choose_action(s, EPSILON)\n",
    "\n",
    "        # take action and get next state\n",
    "        s_, r, done, info = env.step(a)\n",
    "        s_ = np.array(s_)\n",
    "        \n",
    "        # accumulate return\n",
    "        ep_r += r\n",
    "        # clip rewards for numerical stability\n",
    "        clip_r = np.sign(r)\n",
    "\n",
    "        # store the transition\n",
    "        dqn.store_transition(s, a, clip_r, s_, float(done))\n",
    "\n",
    "        # annealing the epsilon(exploration strategy), beta(per smoothing)\n",
    "        if dqn.memory_counter <= MEMORY_CAPACITY:\n",
    "            # linear annealing to 0.9 until million step\n",
    "            EPSILON += 0.9/MEMORY_CAPACITY\n",
    "        elif dqn.memory_counter <= STEP_NUM:\n",
    "            # linear annealing to 0.99 until the end\n",
    "            EPSILON += 0.09/(STEP_NUM - MEMORY_CAPACITY)\n",
    "            # linear annealing to 1 until the end\n",
    "            PER_BETA += (1.0 - PER_BETA) /(STEP_NUM - MEMORY_CAPACITY)\n",
    "\n",
    "        # if memory fill 50K and mod 4 = 0(for speed issue), learn pred net\n",
    "        if (5e+4 <= dqn.memory_counter) and (dqn.memory_counter % LEARN_FREQ == 0):\n",
    "            dqn.learn(PER_BETA)\n",
    "            \n",
    "        # print log and save\n",
    "        if dqn.memory_counter % SAVE_FREQ == 0:\n",
    "            # check time interval\n",
    "            time_interval = round(time.time() - start_time, 2)\n",
    "            # calc mean return\n",
    "            mean_10_ep_return = round(np.mean(entire_ep_rs[-10:-1]),2)\n",
    "            result.append(mean_10_ep_return)\n",
    "            # print log\n",
    "            print('Ep: ',epi_step,\n",
    "                  '| Mean ep 10 return: ', mean_10_ep_return,\n",
    "                  '/Used Time:',time_interval,\n",
    "                  '/Used Step:',dqn.memory_counter)\n",
    "            # save model\n",
    "            dqn.save_model()\n",
    "            pkl_file = open(RESULT_PATH, 'wb')\n",
    "            pickle.dump(np.array(result), pkl_file)\n",
    "            pkl_file.close()\n",
    "            print('Save complete!')\n",
    "            \n",
    "        # if agent meets end-of-life, update return, acc return\n",
    "        if done:\n",
    "            entire_ep_r += ep_r\n",
    "            epi_step += 1\n",
    "            if epi_step % 1 == 0:\n",
    "                entire_ep_rs.append(entire_ep_r)\n",
    "                entire_ep_r = 0.\n",
    "            break\n",
    "\n",
    "        s = s_\n",
    "\n",
    "        if RENDERING:\n",
    "            env.render()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
